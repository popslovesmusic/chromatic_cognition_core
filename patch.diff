 (cd "$(git rev-parse --show-toplevel)" && git apply --3way <<'EOF' 
diff --git a/Cargo.lock b/Cargo.lock
index a657839dd9c63d27278367a85d7c13e6ddfb37c2..448f77641f1b0d8007f3a607c058ab572a0ca2a1 100644
--- a/Cargo.lock
+++ b/Cargo.lock
@@ -964,50 +964,51 @@ checksum = "1ad61aed86bc3faea4300c7aee358b4c6d0c8d6ccc36524c96e4c92ccf26e77e"
 dependencies = [
  "num-integer",
  "strength_reduce",
 ]
 
 [[package]]
 name = "ttf-parser"
 version = "0.20.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "17f77d76d837a7830fe1d4f12b7b4ba4192c1888001c7164257e4bc6d21d96b4"
 
 [[package]]
 name = "unicode-ident"
 version = "1.0.20"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "462eeb75aeb73aea900253ce739c8e18a67423fadf006037cd3ff27e82748a06"
 
 [[package]]
 name = "uuid"
 version = "1.18.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "2f87b8aa10b915a06587d0dec516c282ff295b475d94abf425d62b57710070a2"
 dependencies = [
  "getrandom 0.3.4",
  "js-sys",
+ "serde",
  "wasm-bindgen",
 ]
 
 [[package]]
 name = "walkdir"
 version = "2.5.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "29790946404f91d9c5d06f9874efddea1dc06c5efe94541a7d6863108e3a5e4b"
 dependencies = [
  "same-file",
  "winapi-util",
 ]
 
 [[package]]
 name = "wasi"
 version = "0.11.1+wasi-snapshot-preview1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "ccf3ec651a847eb01de73ccad15eb7d99f80485de043efb2f370cd654f4ea44b"
 
 [[package]]
 name = "wasip2"
 version = "1.0.1+wasi-0.2.4"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "0562428422c63773dad2c345a1882263bbf4d65cf3f42e90921f787ef5ad58e7"
 dependencies = [
diff --git a/Cargo.toml b/Cargo.toml
index 9cd57227e8c785872183fb2493fbb0c4078079f6..c3ebcd182b76e2101eae3b89925d8cffc5eb3326 100644
--- a/Cargo.toml
+++ b/Cargo.toml
@@ -1,19 +1,19 @@
 [package]
 name = "chromatic_cognition_core"
 version = "0.1.0"
 edition = "2021"
 
 [dependencies]
 ndarray = { version = "0.15", features = ["serde", "rayon"] }
 rayon = "1.8"
 serde = { version = "1", features = ["derive"] }
 serde_json = "1"
 toml = "0.8"
 plotters = { version = "0.3", features = ["bitmap_backend", "bitmap_encoder"] }
 rand = { version = "0.8", features = ["std_rng"] }
 anyhow = "1.0.100"
 rustfft = "6.2"
-uuid = { version = "1.0", features = ["v4"] }
+uuid = { version = "1.0", features = ["v4", "serde"] }
 
 [dev-dependencies]
 serde_json = "1"
diff --git a/config/engine.toml b/config/engine.toml
index 08d528e53fb4f39780c7ff6e822f626ce98ec6bd..74b12e856b2eb27057bded065d5b296894845bcd 100644
--- a/config/engine.toml
+++ b/config/engine.toml
@@ -1,6 +1,17 @@
 [engine]
 rows = 64
 cols = 64
 layers = 8
 seed = 42
 device = "cpu"
+
+[phase4]
+embed_dim = 64
+similarity = "cosine"
+alpha = 0.65
+beta = 0.20
+gamma = 0.10
+delta = 0.05
+mmr_lambda = 0.7
+refresh_interval_steps = 500
+drift_threshold = 0.08
diff --git a/examples/demo.rs b/examples/demo.rs
index b292b5227309c02809b11fbb3648abc9a55d95e5..77f8923628b36eefcb8865a0c9fcb7feaae944c4 100644
--- a/examples/demo.rs
+++ b/examples/demo.rs
@@ -1,31 +1,31 @@
 use std::path::PathBuf;
 
 use chromatic_cognition_core::config::ConfigError;
 use chromatic_cognition_core::logging;
 use chromatic_cognition_core::{
-    ChromaticTensor, EngineConfig, GradientLayer, complement, filter, mix, mse_loss, saturate,
+    complement, filter, mix, mse_loss, saturate, ChromaticTensor, EngineConfig, GradientLayer,
 };
 
 fn main() -> Result<(), Box<dyn std::error::Error>> {
     let config = load_config()?;
     println!(
         "Loaded config: rows={} cols={} layers={} seed={}",
         config.rows, config.cols, config.layers, config.seed
     );
 
     let primary = ChromaticTensor::from_seed(config.seed, config.rows, config.cols, config.layers);
     let secondary = ChromaticTensor::from_seed(
         config.seed ^ 0xABCD_EF01,
         config.rows,
         config.cols,
         config.layers,
     );
 
     let mixed = mix(&primary, &secondary);
     let filtered = filter(&mixed, &secondary);
     let complemented = complement(&filtered);
     let saturated = saturate(&complemented, 1.25);
 
     let gradient = GradientLayer::from_tensor(&saturated);
     gradient.to_png(PathBuf::from("out/frame_0001.png"))?;
 
diff --git a/examples/dream_validation.rs b/examples/dream_validation.rs
index 1fa2b66dbc473157ec8ff79d4bce086b306efb6c..8cfbe20ab22dd9c943805048d0f54b9bb5a0c873 100644
--- a/examples/dream_validation.rs
+++ b/examples/dream_validation.rs
@@ -70,78 +70,96 @@ fn main() {
         dataset_config,
         seed: 42,
     };
 
     let solver_b = ChromaticNativeSolver::default();
     let mut harness_b = ExperimentHarness::new(config_b, solver_b);
     let result_b = harness_b.run();
 
     println!("  Final Accuracy: {:.4}", result_b.final_accuracy);
     println!("  Convergence Epoch: {:?}", result_b.convergence_epoch);
     println!("  Total Time: {}ms", result_b.total_elapsed_ms);
 
     if let Some(stats) = harness_b.pool_stats() {
         println!("  Pool Size: {}", stats.count);
         println!("  Pool Mean Coherence: {:.4}\n", stats.mean_coherence);
     }
 
     // Compare results
     println!("=== Comparison ===");
     println!("Group A Final Accuracy: {:.4}", result_a.final_accuracy);
     println!("Group B Final Accuracy: {:.4}", result_b.final_accuracy);
 
     let accuracy_improvement = result_b.final_accuracy - result_a.final_accuracy;
     let improvement_pct = (accuracy_improvement / result_a.final_accuracy) * 100.0;
 
-    println!("Accuracy Improvement: {:.4} ({:.2}%)", accuracy_improvement, improvement_pct);
+    println!(
+        "Accuracy Improvement: {:.4} ({:.2}%)",
+        accuracy_improvement, improvement_pct
+    );
 
     if let (Some(conv_a), Some(conv_b)) = (result_a.convergence_epoch, result_b.convergence_epoch) {
         let epoch_reduction = conv_a as i32 - conv_b as i32;
         println!("Group A Convergence: epoch {}", conv_a);
         println!("Group B Convergence: epoch {}", conv_b);
         println!("Epochs Saved: {}\n", epoch_reduction);
     } else {
-        println!("Convergence: A={:?}, B={:?}\n", result_a.convergence_epoch, result_b.convergence_epoch);
+        println!(
+            "Convergence: A={:?}, B={:?}\n",
+            result_a.convergence_epoch, result_b.convergence_epoch
+        );
     }
 
     // Compare mean coherence across epochs
-    let mean_coherence_a: f64 = result_a.epoch_metrics.iter().map(|m| m.mean_coherence).sum::<f64>()
+    let mean_coherence_a: f64 = result_a
+        .epoch_metrics
+        .iter()
+        .map(|m| m.mean_coherence)
+        .sum::<f64>()
         / result_a.epoch_metrics.len() as f64;
-    let mean_coherence_b: f64 = result_b.epoch_metrics.iter().map(|m| m.mean_coherence).sum::<f64>()
+    let mean_coherence_b: f64 = result_b
+        .epoch_metrics
+        .iter()
+        .map(|m| m.mean_coherence)
+        .sum::<f64>()
         / result_b.epoch_metrics.len() as f64;
 
     println!("Mean Coherence (all epochs):");
     println!("  Group A: {:.4}", mean_coherence_a);
     println!("  Group B: {:.4}", mean_coherence_b);
     println!("  Difference: {:.4}\n", mean_coherence_b - mean_coherence_a);
 
     // Save detailed results to JSON
     println!("Saving results to logs/...");
     std::fs::create_dir_all("logs").expect("Failed to create logs directory");
 
     let json_a = serde_json::to_string_pretty(&result_a).expect("Failed to serialize Group A");
     let mut file_a = File::create("logs/experiment_group_a.json").expect("Failed to create file");
-    file_a.write_all(json_a.as_bytes()).expect("Failed to write");
+    file_a
+        .write_all(json_a.as_bytes())
+        .expect("Failed to write");
 
     let json_b = serde_json::to_string_pretty(&result_b).expect("Failed to serialize Group B");
     let mut file_b = File::create("logs/experiment_group_b.json").expect("Failed to create file");
-    file_b.write_all(json_b.as_bytes()).expect("Failed to write");
+    file_b
+        .write_all(json_b.as_bytes())
+        .expect("Failed to write");
 
     println!("Results saved!");
     println!("\n=== Experiment Complete ===");
 
     // Decision gate
     println!("\n=== Decision Gate ===");
     if improvement_pct > 5.0 && accuracy_improvement > 0.01 {
         println!("✅ HYPOTHESIS VALIDATED");
         println!("   Retrieval-based seeding shows meaningful improvement.");
         println!("   Recommendation: Proceed to Phase 2 (Persistence, FFT)");
     } else if improvement_pct > 0.0 {
         println!("⚠️  HYPOTHESIS WEAKLY SUPPORTED");
         println!("   Small improvement observed, but below significance threshold.");
         println!("   Recommendation: Tune parameters or investigate further");
     } else {
         println!("❌ HYPOTHESIS NOT VALIDATED");
         println!("   No improvement from retrieval-based seeding.");
         println!("   Recommendation: Defer Dream Pool implementation");
     }
 }
diff --git a/examples/dream_validation_full.rs b/examples/dream_validation_full.rs
index e2888fb4bcdf10dfc1feadef423c20a5c4eed960..be83d67aa719ba41d45423fe702583b00da94b82 100644
--- a/examples/dream_validation_full.rs
+++ b/examples/dream_validation_full.rs
@@ -24,51 +24,54 @@ fn main() {
     println!("╚════════════════════════════════════════════════════════════════╝\n");
 
     // Shared configuration
     let dataset_config = DatasetConfig {
         tensor_size: (16, 16, 4),
         noise_level: 0.15,
         samples_per_class: 50,
         seed: 42,
     };
 
     let pool_config = PoolConfig {
         max_size: 500,
         coherence_threshold: 0.65,
         retrieval_limit: 3,
     };
 
     let num_epochs = 40;
     let dream_iterations = 8;
 
     println!("Configuration:");
     println!("  Tensor Size: {:?}", dataset_config.tensor_size);
     println!("  Samples per Class: {}", dataset_config.samples_per_class);
     println!("  Total Samples: {}", dataset_config.samples_per_class * 10);
     println!("  Epochs: {}", num_epochs);
     println!("  Dream Iterations: {}", dream_iterations);
-    println!("  Pool Coherence Threshold: {}\n", pool_config.coherence_threshold);
+    println!(
+        "  Pool Coherence Threshold: {}\n",
+        pool_config.coherence_threshold
+    );
 
     // ========================================================================
     // Group A: Control (Random Noise)
     // ========================================================================
     println!("┌─────────────────────────────────────────────────────────────┐");
     println!("│ Group A: Control (Random Noise Seeding)                    │");
     println!("└─────────────────────────────────────────────────────────────┘");
 
     let config_a = ExperimentConfig {
         strategy: SeedingStrategy::RandomNoise,
         num_epochs,
         batch_size: 10,
         dream_iterations,
         pool_config: pool_config.clone(),
         dataset_config: dataset_config.clone(),
         seed: 42,
     };
 
     let solver_a = ChromaticNativeSolver::default();
     let mut harness_a = ExperimentHarness::new(config_a, solver_a);
 
     print!("Running experiment... ");
     std::io::stdout().flush().unwrap();
     let result_a = harness_a.run();
     println!("Done!");
@@ -133,99 +136,122 @@ fn main() {
             / result_b.epoch_metrics.len() as f64
     );
 
     // ========================================================================
     // Statistical Comparison
     // ========================================================================
     println!("┌─────────────────────────────────────────────────────────────┐");
     println!("│ Statistical Analysis                                        │");
     println!("└─────────────────────────────────────────────────────────────┘");
 
     let comparison = compare_experiments(&result_a, &result_b, 0.01);
     let report = generate_report(&comparison);
     println!("{}", report);
 
     // ========================================================================
     // Save Results
     // ========================================================================
     println!("\n┌─────────────────────────────────────────────────────────────┐");
     println!("│ Saving Results                                              │");
     println!("└─────────────────────────────────────────────────────────────┘");
 
     std::fs::create_dir_all("logs").expect("Failed to create logs directory");
 
     // Save raw results
     let json_a = serde_json::to_string_pretty(&result_a).expect("Failed to serialize Group A");
-    let mut file_a =
-        File::create("logs/validation_group_a.json").expect("Failed to create file");
-    file_a.write_all(json_a.as_bytes()).expect("Failed to write");
+    let mut file_a = File::create("logs/validation_group_a.json").expect("Failed to create file");
+    file_a
+        .write_all(json_a.as_bytes())
+        .expect("Failed to write");
     println!("✓ Group A results: logs/validation_group_a.json");
 
     let json_b = serde_json::to_string_pretty(&result_b).expect("Failed to serialize Group B");
-    let mut file_b =
-        File::create("logs/validation_group_b.json").expect("Failed to create file");
-    file_b.write_all(json_b.as_bytes()).expect("Failed to write");
+    let mut file_b = File::create("logs/validation_group_b.json").expect("Failed to create file");
+    file_b
+        .write_all(json_b.as_bytes())
+        .expect("Failed to write");
     println!("✓ Group B results: logs/validation_group_b.json");
 
     // Save comparison
     let json_comp =
         serde_json::to_string_pretty(&comparison).expect("Failed to serialize comparison");
     let mut file_comp =
         File::create("logs/validation_comparison.json").expect("Failed to create file");
     file_comp
         .write_all(json_comp.as_bytes())
         .expect("Failed to write");
     println!("✓ Comparison: logs/validation_comparison.json");
 
     // Save human-readable report
     let mut report_file =
         File::create("logs/validation_report.txt").expect("Failed to create report file");
     report_file
         .write_all(report.as_bytes())
         .expect("Failed to write report");
     println!("✓ Report: logs/validation_report.txt");
 
     // ========================================================================
     // Generate CSV for plotting
     // ========================================================================
-    let mut csv = String::from("epoch,group_a_accuracy,group_b_accuracy,group_a_coherence,group_b_coherence\n");
-    for (a, b) in result_a.epoch_metrics.iter().zip(result_b.epoch_metrics.iter()) {
+    let mut csv = String::from(
+        "epoch,group_a_accuracy,group_b_accuracy,group_a_coherence,group_b_coherence\n",
+    );
+    for (a, b) in result_a
+        .epoch_metrics
+        .iter()
+        .zip(result_b.epoch_metrics.iter())
+    {
         csv.push_str(&format!(
             "{},{},{},{},{}\n",
-            a.epoch, a.validation_accuracy, b.validation_accuracy, a.mean_coherence, b.mean_coherence
+            a.epoch,
+            a.validation_accuracy,
+            b.validation_accuracy,
+            a.mean_coherence,
+            b.mean_coherence
         ));
     }
 
     let mut csv_file =
         File::create("logs/validation_metrics.csv").expect("Failed to create CSV file");
-    csv_file.write_all(csv.as_bytes()).expect("Failed to write CSV");
+    csv_file
+        .write_all(csv.as_bytes())
+        .expect("Failed to write CSV");
     println!("✓ Metrics CSV: logs/validation_metrics.csv");
 
     println!("\n┌─────────────────────────────────────────────────────────────┐");
     println!("│ Experiment Complete                                         │");
     println!("└─────────────────────────────────────────────────────────────┘");
 
     // Final decision
     if comparison.is_significant {
         println!("\n🎉 SUCCESS: Retrieval hypothesis VALIDATED!");
-        println!("   ✓ Improvement: {:.2}%", comparison.accuracy_improvement_pct);
-        println!("   ✓ Statistically significant (p < {})", comparison.significance_level);
+        println!(
+            "   ✓ Improvement: {:.2}%",
+            comparison.accuracy_improvement_pct
+        );
+        println!(
+            "   ✓ Statistically significant (p < {})",
+            comparison.significance_level
+        );
         println!("\n   → RECOMMENDATION: Proceed to Phase 2");
         println!("     • Implement SQLite persistence");
         println!("     • Add FFT spectral analysis");
         println!("     • Develop full Dream Pool specification");
     } else if comparison.accuracy_improvement > 0.0 {
         println!("\n⚠️  MARGINAL: Hypothesis weakly supported");
-        println!("   • Improvement: {:.2}%", comparison.accuracy_improvement_pct);
+        println!(
+            "   • Improvement: {:.2}%",
+            comparison.accuracy_improvement_pct
+        );
         println!("   • Below significance threshold");
         println!("\n   → RECOMMENDATION: Investigate further");
         println!("     • Tune hyperparameters");
         println!("     • Increase dataset size");
         println!("     • Analyze failure modes");
     } else {
         println!("\n❌ NEGATIVE: Hypothesis not validated");
         println!("   • No improvement observed");
         println!("\n   → RECOMMENDATION: Defer Dream Pool");
         println!("     • Focus on core solver optimization");
         println!("     • Consider alternative approaches");
     }
 }
diff --git a/examples/learner_validation.rs b/examples/learner_validation.rs
index 9e5d6c5454ef852346ac7ccb5d6c62554fb7f1a0..f73fc2bbd135e8f582a0f8938af36eee568bcdec 100644
--- a/examples/learner_validation.rs
+++ b/examples/learner_validation.rs
@@ -1,46 +1,47 @@
 //! Learner Validation Experiment
 //!
 //! This example validates the Minimal Viable Learner (MVP) and demonstrates
 //! that real gradient descent training works with the color classification task.
 //!
 //! Validates:
 //! 1. Training can actually learn (reaches >90% accuracy)
 //! 2. Retrieval-based seeding from Dream Pool helps convergence
 //! 3. Proper feedback loop infrastructure for full LEARNER MANIFEST v1.0
 //!
 //! Run with:
 //! ```
 //! cargo run --example learner_validation --release
 //! ```
 
 use chromatic_cognition_core::data::{ColorDataset, DatasetConfig};
 use chromatic_cognition_core::dream::simple_pool::PoolConfig;
 use chromatic_cognition_core::dream::{RetrievalMode, SimpleDreamPool};
+use chromatic_cognition_core::learner::training::SoftRetrievalConfig;
 use chromatic_cognition_core::{
-    ChromaticNativeSolver, ClassifierConfig, MLPClassifier, TrainingConfig,
-    train_baseline, train_with_dreams,
+    train_baseline, train_with_dreams, ChromaticNativeSolver, ClassifierConfig, MLPClassifier,
+    TrainingConfig,
 };
 use std::fs::File;
 use std::io::Write;
 
 fn main() {
     println!("╔══════════════════════════════════════════════════════════════╗");
     println!("║  Learner Validation Experiment - Minimal Viable Learner     ║");
     println!("║  Goal: Prove training works & Dream Pool helps convergence  ║");
     println!("╚══════════════════════════════════════════════════════════════╝\n");
 
     // Configuration
     let dataset_config = DatasetConfig {
         tensor_size: (16, 16, 4),
         noise_level: 0.1,
         samples_per_class: 100, // 1000 total samples
         seed: 42,
     };
 
     let classifier_config = ClassifierConfig {
         input_size: 16 * 16 * 4 * 3,
         hidden_size: 256,
         output_size: 10,
         seed: 42,
     };
 
@@ -57,250 +58,297 @@ fn main() {
     println!("  Pool: 500 max dreams, coherence ≥ 0.7\n");
 
     // Generate dataset
     println!("Generating dataset...");
     let dataset = ColorDataset::generate(dataset_config);
     let (train_data, val_data) = dataset.split(0.8);
     println!("  Train: {} samples", train_data.len());
     println!("  Val: {} samples\n", val_data.len());
 
     // ========================================================================
     // Experiment 1: Baseline Training (No Dream Pool)
     // ========================================================================
     println!("┌──────────────────────────────────────────────────────────────┐");
     println!("│ Experiment 1: Baseline Training (No Dream Pool)             │");
     println!("└──────────────────────────────────────────────────────────────┘");
 
     let baseline_config = TrainingConfig {
         num_epochs: 100,
         batch_size: 32,
         learning_rate: 0.01,
         lr_decay: 0.98,
         use_dream_pool: false,
         num_dreams_retrieve: 0,
         retrieval_mode: RetrievalMode::Hard,
         seed: 42,
+        soft: SoftRetrievalConfig::default(),
     };
 
     let classifier_baseline = MLPClassifier::new(classifier_config.clone());
 
     print!("Training baseline model... ");
     std::io::stdout().flush().unwrap();
 
-    let result_baseline = train_baseline(
-        classifier_baseline,
-        &train_data,
-        &val_data,
-        baseline_config,
-    );
+    let result_baseline =
+        train_baseline(classifier_baseline, &train_data, &val_data, baseline_config);
 
     println!("Done!");
     println!("\nBaseline Results:");
-    println!("  Final Train Accuracy: {:.2}%", result_baseline.final_train_accuracy * 100.0);
-    println!("  Final Val Accuracy: {:.2}%", result_baseline.final_val_accuracy * 100.0);
-    println!("  Converged at Epoch: {:?}", result_baseline.converged_epoch);
+    println!(
+        "  Final Train Accuracy: {:.2}%",
+        result_baseline.final_train_accuracy * 100.0
+    );
+    println!(
+        "  Final Val Accuracy: {:.2}%",
+        result_baseline.final_val_accuracy * 100.0
+    );
+    println!(
+        "  Converged at Epoch: {:?}",
+        result_baseline.converged_epoch
+    );
     println!("  Total Time: {}ms\n", result_baseline.total_elapsed_ms);
 
     // ========================================================================
     // Experiment 2: Training with Dream Pool Retrieval
     // ========================================================================
     println!("┌──────────────────────────────────────────────────────────────┐");
     println!("│ Experiment 2: Training with Dream Pool Retrieval            │");
     println!("└──────────────────────────────────────────────────────────────┘");
 
     let dream_config = TrainingConfig {
         num_epochs: 100,
         batch_size: 32,
         learning_rate: 0.01,
         lr_decay: 0.98,
         use_dream_pool: true,
         num_dreams_retrieve: 3,
         retrieval_mode: RetrievalMode::Hard,
         seed: 42,
+        soft: SoftRetrievalConfig::default(),
     };
 
     let classifier_dream = MLPClassifier::new(classifier_config.clone());
     let mut pool = SimpleDreamPool::new(pool_config);
     let mut solver = ChromaticNativeSolver::default();
 
     print!("Training with Dream Pool... ");
     std::io::stdout().flush().unwrap();
 
     let result_dream = train_with_dreams(
         classifier_dream,
         &train_data,
         &val_data,
         dream_config,
         Some(&mut pool),
         Some(&mut solver),
     );
 
     println!("Done!");
     println!("\nDream Pool Results:");
-    println!("  Final Train Accuracy: {:.2}%", result_dream.final_train_accuracy * 100.0);
-    println!("  Final Val Accuracy: {:.2}%", result_dream.final_val_accuracy * 100.0);
+    println!(
+        "  Final Train Accuracy: {:.2}%",
+        result_dream.final_train_accuracy * 100.0
+    );
+    println!(
+        "  Final Val Accuracy: {:.2}%",
+        result_dream.final_val_accuracy * 100.0
+    );
     println!("  Converged at Epoch: {:?}", result_dream.converged_epoch);
     println!("  Total Time: {}ms", result_dream.total_elapsed_ms);
 
     let pool_stats = pool.stats();
     println!("  Pool Size: {}", pool_stats.count);
     println!("  Pool Mean Coherence: {:.4}\n", pool_stats.mean_coherence);
 
     // ========================================================================
     // Comparison
     // ========================================================================
     println!("┌──────────────────────────────────────────────────────────────┐");
     println!("│ Comparison                                                   │");
     println!("└──────────────────────────────────────────────────────────────┘");
 
     let accuracy_improvement = result_dream.final_val_accuracy - result_baseline.final_val_accuracy;
     let improvement_pct = (accuracy_improvement / result_baseline.final_val_accuracy) * 100.0;
 
     println!("Final Validation Accuracy:");
-    println!("  Baseline: {:.2}%", result_baseline.final_val_accuracy * 100.0);
-    println!("  Dream Pool: {:.2}%", result_dream.final_val_accuracy * 100.0);
-    println!("  Improvement: {:.4} ({:.2}%)\n", accuracy_improvement, improvement_pct);
+    println!(
+        "  Baseline: {:.2}%",
+        result_baseline.final_val_accuracy * 100.0
+    );
+    println!(
+        "  Dream Pool: {:.2}%",
+        result_dream.final_val_accuracy * 100.0
+    );
+    println!(
+        "  Improvement: {:.4} ({:.2}%)\n",
+        accuracy_improvement, improvement_pct
+    );
 
-    let convergence_comparison = match (result_baseline.converged_epoch, result_dream.converged_epoch) {
+    let convergence_comparison = match (
+        result_baseline.converged_epoch,
+        result_dream.converged_epoch,
+    ) {
         (Some(baseline_epoch), Some(dream_epoch)) => {
             println!("Convergence (95% accuracy):");
             println!("  Baseline: epoch {}", baseline_epoch);
             println!("  Dream Pool: epoch {}", dream_epoch);
             if dream_epoch < baseline_epoch {
-                println!("  Dream Pool converged {} epochs FASTER ✓\n", baseline_epoch - dream_epoch);
+                println!(
+                    "  Dream Pool converged {} epochs FASTER ✓\n",
+                    baseline_epoch - dream_epoch
+                );
             } else if dream_epoch > baseline_epoch {
-                println!("  Dream Pool converged {} epochs SLOWER ⚠️\n", dream_epoch - baseline_epoch);
+                println!(
+                    "  Dream Pool converged {} epochs SLOWER ⚠️\n",
+                    dream_epoch - baseline_epoch
+                );
             } else {
                 println!("  Same convergence speed\n");
             }
             Some((baseline_epoch, dream_epoch))
         }
         (Some(baseline_epoch), None) => {
             println!("Convergence:");
             println!("  Baseline: epoch {} ✓", baseline_epoch);
             println!("  Dream Pool: Did not converge ⚠️\n");
             None
         }
         (None, Some(dream_epoch)) => {
             println!("Convergence:");
             println!("  Baseline: Did not converge");
             println!("  Dream Pool: epoch {} ✓\n", dream_epoch);
             None
         }
         (None, None) => {
             println!("Convergence:");
             println!("  Neither model converged to 95% accuracy\n");
             None
         }
     };
 
     // ========================================================================
     // Save Results
     // ========================================================================
     println!("┌──────────────────────────────────────────────────────────────┐");
     println!("│ Saving Results                                               │");
     println!("└──────────────────────────────────────────────────────────────┘");
 
     std::fs::create_dir_all("logs").expect("Failed to create logs directory");
 
     // Save JSON results
-    let json_baseline = serde_json::to_string_pretty(&result_baseline)
-        .expect("Failed to serialize baseline");
-    let mut file_baseline = File::create("logs/learner_baseline.json")
-        .expect("Failed to create file");
-    file_baseline.write_all(json_baseline.as_bytes()).expect("Failed to write");
+    let json_baseline =
+        serde_json::to_string_pretty(&result_baseline).expect("Failed to serialize baseline");
+    let mut file_baseline =
+        File::create("logs/learner_baseline.json").expect("Failed to create file");
+    file_baseline
+        .write_all(json_baseline.as_bytes())
+        .expect("Failed to write");
     println!("✓ Baseline results: logs/learner_baseline.json");
 
-    let json_dream = serde_json::to_string_pretty(&result_dream)
-        .expect("Failed to serialize dream pool");
-    let mut file_dream = File::create("logs/learner_dream_pool.json")
-        .expect("Failed to create file");
-    file_dream.write_all(json_dream.as_bytes()).expect("Failed to write");
+    let json_dream =
+        serde_json::to_string_pretty(&result_dream).expect("Failed to serialize dream pool");
+    let mut file_dream =
+        File::create("logs/learner_dream_pool.json").expect("Failed to create file");
+    file_dream
+        .write_all(json_dream.as_bytes())
+        .expect("Failed to write");
     println!("✓ Dream Pool results: logs/learner_dream_pool.json");
 
     // Generate CSV for plotting
     let mut csv = String::from("epoch,baseline_train_acc,baseline_val_acc,dream_train_acc,dream_val_acc,baseline_loss,dream_loss\n");
-    for (baseline, dream) in result_baseline.epoch_metrics.iter().zip(result_dream.epoch_metrics.iter()) {
+    for (baseline, dream) in result_baseline
+        .epoch_metrics
+        .iter()
+        .zip(result_dream.epoch_metrics.iter())
+    {
         csv.push_str(&format!(
             "{},{},{},{},{},{},{}\n",
             baseline.epoch,
             baseline.train_accuracy,
             baseline.val_accuracy,
             dream.train_accuracy,
             dream.val_accuracy,
             baseline.train_loss,
             dream.train_loss,
         ));
     }
 
-    let mut csv_file = File::create("logs/learner_comparison.csv")
-        .expect("Failed to create CSV file");
-    csv_file.write_all(csv.as_bytes()).expect("Failed to write CSV");
+    let mut csv_file =
+        File::create("logs/learner_comparison.csv").expect("Failed to create CSV file");
+    csv_file
+        .write_all(csv.as_bytes())
+        .expect("Failed to write CSV");
     println!("✓ Metrics CSV: logs/learner_comparison.csv");
 
     // ========================================================================
     // Validation Assessment
     // ========================================================================
     println!("\n┌──────────────────────────────────────────────────────────────┐");
     println!("│ Validation Assessment                                        │");
     println!("└──────────────────────────────────────────────────────────────┘");
 
     let training_works = result_baseline.final_val_accuracy >= 0.90;
     let retrieval_helps = result_dream.final_val_accuracy > result_baseline.final_val_accuracy;
     let converged_faster = convergence_comparison
         .map(|(baseline, dream)| dream < baseline)
         .unwrap_or(false);
 
     println!("\n✓ VALIDATION CHECKLIST:\n");
 
     print!("  [");
     if training_works {
         print!("✓");
     } else {
         print!("✗");
     }
     println!("] Training achieves >90% accuracy");
-    println!("      Baseline: {:.2}%", result_baseline.final_val_accuracy * 100.0);
+    println!(
+        "      Baseline: {:.2}%",
+        result_baseline.final_val_accuracy * 100.0
+    );
 
     print!("  [");
     if retrieval_helps {
         print!("✓");
     } else {
         print!("✗");
     }
     println!("] Dream Pool improves final accuracy");
     println!("      Improvement: {:.2}%", improvement_pct);
 
     print!("  [");
     if converged_faster {
         print!("✓");
     } else {
         print!("✗");
     }
     println!("] Dream Pool accelerates convergence");
     if let Some((baseline, dream)) = convergence_comparison {
-        println!("      Baseline: {} epochs, Dream Pool: {} epochs", baseline, dream);
+        println!(
+            "      Baseline: {} epochs, Dream Pool: {} epochs",
+            baseline, dream
+        );
     }
 
     println!("\n╔══════════════════════════════════════════════════════════════╗");
     if training_works && (retrieval_helps || converged_faster) {
         println!("║  ✓ SUCCESS: Learner validated & Dream Pool hypothesis holds ║");
         println!("╚══════════════════════════════════════════════════════════════╝");
         println!("\n🎉 The Minimal Viable Learner is working!");
         println!("\nNext Steps:");
         println!("  1. ✓ Training algorithm works (gradient descent + cross-entropy)");
         println!("  2. ✓ Dream Pool retrieval integration functional");
         println!("  3. → Ready for full LEARNER MANIFEST v1.0 implementation:");
         println!("       - FFT feature extraction");
         println!("       - Feedback collection (Δloss tracking)");
         println!("       - Bias profile synthesis");
         println!("       - Advanced retrieval modes (euclidean, mixed)");
     } else if training_works && !retrieval_helps {
         println!("║  ⚠️  PARTIAL: Training works, but Dream Pool shows no benefit║");
         println!("╚══════════════════════════════════════════════════════════════╝");
         println!("\nTraining algorithm is functional, but retrieval hypothesis not validated.");
         println!("\nPossible causes:");
         println!("  - Coherence threshold too high (pool too sparse)");
         println!("  - Dataset too simple (training converges quickly anyway)");
         println!("  - Retrieval strategy needs tuning");
         println!("\nRecommendation: Investigate Dream Pool parameters or try harder task.");
     } else {
diff --git a/examples/phase_3b_validation.rs b/examples/phase_3b_validation.rs
index adc5e29715fcff5c6e7f43be520d34198d7cf27d..c0b998efd7220f12f403615e4edd0ce24d8fbee5 100644
--- a/examples/phase_3b_validation.rs
+++ b/examples/phase_3b_validation.rs
@@ -1,72 +1,75 @@
 //! Phase 3B Validation - Test refined Dream Pool improvements
 //!
 //! This experiment validates whether Phase 3B refinements improve training:
 //! - Class-aware retrieval
 //! - Diversity enforcement (MMR)
 //! - Spectral feature extraction
 //! - ΔLoss utility scoring
 //! - Bias profile synthesis
 //!
 //! **3-Way Comparison:**
 //! 1. Baseline: No Dream Pool
 //! 2. Phase 3A: Original Dream Pool (cosine similarity only)
 //! 3. Phase 3B: Refined Dream Pool (all enhancements)
 
 use chromatic_cognition_core::data::{ColorClass, ColorDataset, ColorSample, DatasetConfig};
 use chromatic_cognition_core::dream::simple_pool::PoolConfig;
 use chromatic_cognition_core::dream::{BiasProfile, RetrievalMode, SimpleDreamPool};
 use chromatic_cognition_core::learner::feedback::{FeedbackRecord, UtilityAggregator};
-use chromatic_cognition_core::learner::training::{EpochMetrics, TrainingConfig, TrainingResult, train_with_dreams};
+use chromatic_cognition_core::learner::training::{
+    train_with_dreams, EpochMetrics, SoftRetrievalConfig, TrainingConfig, TrainingResult,
+};
 use chromatic_cognition_core::learner::{ClassifierConfig, MLPClassifier};
-use chromatic_cognition_core::tensor::operations::mix;
 use chromatic_cognition_core::solver::native::ChromaticNativeSolver;
+use chromatic_cognition_core::tensor::operations::mix;
 use chromatic_cognition_core::Solver;
 
 fn main() {
     println!("╔══════════════════════════════════════════════════════════════╗");
     println!("║  Phase 3B Validation - Refined Dream Pool Evaluation       ║");
     println!("║  Testing: Class-aware + Diversity + Utility + Bias         ║");
     println!("╚══════════════════════════════════════════════════════════════╝\n");
 
     // Configuration
     let dataset_config = DatasetConfig {
         samples_per_class: 100,
         tensor_size: (16, 16, 4),
         noise_level: 0.1,
         seed: 42,
     };
 
     let base_training = TrainingConfig {
         num_epochs: 100,
         batch_size: 32,
         learning_rate: 0.01,
         lr_decay: 0.98,
         use_dream_pool: false,
         num_dreams_retrieve: 3,
         retrieval_mode: RetrievalMode::Hard,
         seed: 42,
+        soft: SoftRetrievalConfig::default(),
     };
 
     let classifier_config = ClassifierConfig {
         hidden_size: 256,
         ..Default::default()
     };
 
     let pool_config = PoolConfig {
         max_size: 500,
         coherence_threshold: 0.7,
         retrieval_limit: 3,
     };
 
     println!("Configuration:");
     println!("  Dataset: 1000 samples (100 per class)");
     println!("  Tensor Size: 16×16×4");
     println!("  Model: MLP with 256 hidden units");
     println!("  Pool: 500 max dreams, coherence ≥ 0.7\n");
 
     // Generate dataset
     println!("Generating dataset...");
     let dataset = ColorDataset::generate(dataset_config);
     let (train_samples, val_samples) = dataset.split(0.8);
     println!("  Train: {} samples", train_samples.len());
     println!("  Val: {} samples\n", val_samples.len());
@@ -74,209 +77,233 @@ fn main() {
     // ========================================================================
     // Experiment 1: Baseline (No Dream Pool)
     // ========================================================================
     println!("┌──────────────────────────────────────────────────────────────┐");
     println!("│ Experiment 1: Baseline (No Dream Pool)                     │");
     println!("└──────────────────────────────────────────────────────────────┘");
 
     let baseline_config = TrainingConfig {
         use_dream_pool: false,
         num_dreams_retrieve: 0,
         retrieval_mode: RetrievalMode::Hard,
         ..base_training.clone()
     };
 
     let classifier = MLPClassifier::new(classifier_config.clone());
     let result_baseline = train_with_dreams(
         classifier,
         &train_samples,
         &val_samples,
         baseline_config.clone(),
         None::<&mut SimpleDreamPool>,
         None::<&mut ChromaticNativeSolver>,
     );
 
     println!("\nBaseline Results:");
-    println!("  Final Train Accuracy: {:.2}%", result_baseline.final_train_accuracy * 100.0);
-    println!("  Final Val Accuracy: {:.2}%", result_baseline.final_val_accuracy * 100.0);
-    println!("  Converged at Epoch: {:?}", result_baseline.converged_epoch);
+    println!(
+        "  Final Train Accuracy: {:.2}%",
+        result_baseline.final_train_accuracy * 100.0
+    );
+    println!(
+        "  Final Val Accuracy: {:.2}%",
+        result_baseline.final_val_accuracy * 100.0
+    );
+    println!(
+        "  Converged at Epoch: {:?}",
+        result_baseline.converged_epoch
+    );
     println!("  Total Time: {}ms\n", result_baseline.total_elapsed_ms);
 
     // ========================================================================
     // Experiment 2: Phase 3A (Original Dream Pool)
     // ========================================================================
     println!("┌──────────────────────────────────────────────────────────────┐");
     println!("│ Experiment 2: Phase 3A (Original Dream Pool)               │");
     println!("└──────────────────────────────────────────────────────────────┘");
 
     let mut pool_3a = SimpleDreamPool::new(pool_config.clone());
     let mut solver_3a = ChromaticNativeSolver::default();
 
     println!("Populating Dream Pool...");
     for sample in &train_samples {
         if let Ok(result) = solver_3a.evaluate(&sample.tensor, false) {
             pool_3a.add_if_coherent(sample.tensor.clone(), result);
         }
     }
     println!("  Pool size: {}\n", pool_3a.len());
 
     let phase3a_config = TrainingConfig {
         use_dream_pool: true,
         retrieval_mode: RetrievalMode::Hard,
         ..base_training.clone()
     };
 
     let classifier = MLPClassifier::new(classifier_config.clone());
     let result_3a = train_with_dreams(
         classifier,
         &train_samples,
         &val_samples,
         phase3a_config.clone(),
         Some(&mut pool_3a),
         Some(&mut solver_3a),
     );
 
     println!("\nPhase 3A Results:");
-    println!("  Final Train Accuracy: {:.2}%", result_3a.final_train_accuracy * 100.0);
-    println!("  Final Val Accuracy: {:.2}%", result_3a.final_val_accuracy * 100.0);
+    println!(
+        "  Final Train Accuracy: {:.2}%",
+        result_3a.final_train_accuracy * 100.0
+    );
+    println!(
+        "  Final Val Accuracy: {:.2}%",
+        result_3a.final_val_accuracy * 100.0
+    );
     println!("  Converged at Epoch: {:?}", result_3a.converged_epoch);
     println!("  Total Time: {}ms\n", result_3a.total_elapsed_ms);
 
     // ========================================================================
     // Experiment 3: Phase 3B (Refined Dream Pool)
     // ========================================================================
     println!("┌──────────────────────────────────────────────────────────────┐");
     println!("│ Experiment 3: Phase 3B (Refined Dream Pool)                │");
     println!("│ Features: Class-aware + Diversity + Utility + Bias         │");
     println!("└──────────────────────────────────────────────────────────────┘");
 
     let mut pool_3b = SimpleDreamPool::new(pool_config.clone());
     let mut solver_3b = ChromaticNativeSolver::default();
     let mut aggregator = UtilityAggregator::new();
 
     println!("Populating Dream Pool with class labels...");
     for sample in &train_samples {
         if let Ok(result) = solver_3b.evaluate(&sample.tensor, false) {
             pool_3b.add_with_class(sample.tensor.clone(), result, sample.label);
         }
     }
     println!("  Pool size: {}\n", pool_3b.len());
 
     let phase3b_config = TrainingConfig {
         use_dream_pool: true,
         retrieval_mode: RetrievalMode::Hybrid,
         ..base_training.clone()
     };
 
     let classifier = MLPClassifier::new(classifier_config.clone());
 
     println!("Training with Phase 3B enhancements...");
     let result_3b = train_with_phase_3b(
         classifier,
         &train_samples,
         &val_samples,
         phase3b_config.clone(),
         &mut pool_3b,
         &mut solver_3b,
         &mut aggregator,
     );
 
     println!("\nPhase 3B Results:");
-    println!("  Final Train Accuracy: {:.2}%", result_3b.final_train_accuracy * 100.0);
-    println!("  Final Val Accuracy: {:.2}%", result_3b.final_val_accuracy * 100.0);
+    println!(
+        "  Final Train Accuracy: {:.2}%",
+        result_3b.final_train_accuracy * 100.0
+    );
+    println!(
+        "  Final Val Accuracy: {:.2}%",
+        result_3b.final_val_accuracy * 100.0
+    );
     println!("  Converged at Epoch: {:?}", result_3b.converged_epoch);
     println!("  Total Time: {}ms", result_3b.total_elapsed_ms);
     println!("  Feedback Records: {}", aggregator.len());
     println!("  Mean Utility: {:.3}\n", aggregator.mean_utility());
 
     // ========================================================================
     // Synthesize and Save Bias Profile
     // ========================================================================
     println!("┌──────────────────────────────────────────────────────────────┐");
     println!("│ Bias Profile Synthesis                                      │");
     println!("└──────────────────────────────────────────────────────────────┘");
 
     let bias_profile = BiasProfile::from_aggregator(&aggregator, 0.1);
 
     println!("\nClass Biases:");
     for class in [ColorClass::Red, ColorClass::Green, ColorClass::Blue] {
         if let Some(stats) = aggregator.class_stats(class) {
             println!(
                 "  {:?}: mean_utility={:.3}, helpful={}, harmful={}",
-                class,
-                stats.mean_utility,
-                stats.helpful_count,
-                stats.harmful_count
+                class, stats.mean_utility, stats.helpful_count, stats.harmful_count
             );
         }
     }
 
     println!("\nPreferred Classes:");
     for class_name in bias_profile.preferred_classes() {
         println!("  • {}", class_name);
     }
 
     std::fs::create_dir_all("logs").expect("Failed to create logs directory");
     bias_profile
         .save_to_json("logs/phase_3b_bias_profile.json")
         .expect("Failed to save bias profile");
     println!("\n✓ Bias profile saved to logs/phase_3b_bias_profile.json");
 
     // ========================================================================
     // Final Comparison
     // ========================================================================
     println!("\n┌──────────────────────────────────────────────────────────────┐");
     println!("│ Final Comparison                                             │");
     println!("└──────────────────────────────────────────────────────────────┘\n");
 
     println!("| Metric              | Baseline | Phase 3A | Phase 3B | Winner   |");
     println!("|---------------------|----------|----------|----------|----------|");
 
     let val_accs = [
         result_baseline.final_val_accuracy,
         result_3a.final_val_accuracy,
         result_3b.final_val_accuracy,
     ];
     let best_acc_idx = val_accs
         .iter()
         .enumerate()
         .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
         .map(|(i, _)| i)
         .unwrap();
 
     println!(
         "| Val Accuracy        | {:.2}%   | {:.2}%   | {:.2}%   | {}      |",
         val_accs[0] * 100.0,
         val_accs[1] * 100.0,
         val_accs[2] * 100.0,
         ["Baseline", "Phase 3A", "Phase 3B"][best_acc_idx]
     );
 
     let epochs = [
-        result_baseline.converged_epoch.unwrap_or(base_training.num_epochs),
-        result_3a.converged_epoch.unwrap_or(base_training.num_epochs),
-        result_3b.converged_epoch.unwrap_or(base_training.num_epochs),
+        result_baseline
+            .converged_epoch
+            .unwrap_or(base_training.num_epochs),
+        result_3a
+            .converged_epoch
+            .unwrap_or(base_training.num_epochs),
+        result_3b
+            .converged_epoch
+            .unwrap_or(base_training.num_epochs),
     ];
     let best_epoch_idx = epochs
         .iter()
         .enumerate()
         .min_by(|(_, a), (_, b)| a.cmp(b))
         .map(|(i, _)| i)
         .unwrap();
 
     println!(
         "| Convergence Epoch   | {}       | {}       | {}       | {}      |",
         epochs[0],
         epochs[1],
         epochs[2],
         ["Baseline", "Phase 3A", "Phase 3B"][best_epoch_idx]
     );
 
     println!("\n┌──────────────────────────────────────────────────────────────┐");
     println!("│ Conclusion                                                   │");
     println!("└──────────────────────────────────────────────────────────────┘\n");
 
     if best_acc_idx == 2 && best_epoch_idx == 2 {
         println!("✅ SUCCESS: Phase 3B outperforms both Baseline and Phase 3A!");
         println!("   Refinements (class-aware + diversity + utility) are effective.\n");
     } else if best_acc_idx == 2 || best_epoch_idx == 2 {
         println!("⚠️  PARTIAL: Phase 3B shows improvements in some metrics.");
@@ -304,51 +331,55 @@ fn train_with_phase_3b(
     let mut current_lr = config.learning_rate;
     let start = Instant::now();
 
     let mut epoch_metrics = Vec::new();
     let mut best_val_accuracy = 0.0;
     let mut converged_epoch = None;
     let mut final_train_accuracy = 0.0;
 
     for epoch in 0..config.num_epochs {
         let epoch_start = Instant::now();
         let mut epoch_loss = 0.0f32;
         let mut batch_count = 0usize;
         let mut epoch_feedback = 0usize;
         let mut dreams_used = 0usize;
 
         for batch_start in (0..train_data.len()).step_by(config.batch_size.max(1)) {
             let batch_end = (batch_start + config.batch_size).min(train_data.len());
             let batch = &train_data[batch_start..batch_end];
 
             let mut batch_tensors = Vec::with_capacity(batch.len());
             let mut batch_labels = Vec::with_capacity(batch.len());
 
             for sample in batch {
                 let mut tensor = sample.tensor.clone();
                 let signature = tensor.mean_rgb();
-                let retrieved = pool.retrieve_similar_class(&signature, sample.label, config.num_dreams_retrieve);
+                let retrieved = pool.retrieve_similar_class(
+                    &signature,
+                    sample.label,
+                    config.num_dreams_retrieve,
+                );
 
                 if let Some(entry) = retrieved.first() {
                     tensor = mix(&tensor, &entry.tensor);
                     dreams_used += 1;
                 }
 
                 batch_tensors.push(tensor);
                 batch_labels.push(sample.label);
             }
 
             let loss_before = classifier.compute_loss(&batch_tensors, &batch_labels).0;
             let (loss, gradients) = classifier.compute_loss(&batch_tensors, &batch_labels);
             classifier.update_weights(&gradients, current_lr);
             let loss_after = classifier.compute_loss(&batch_tensors, &batch_labels).0;
 
             epoch_loss += loss;
             batch_count += 1;
 
             if epoch > 0 && batch_start % 256 == 0 {
                 if let Some(sample) = batch.first() {
                     let record = FeedbackRecord::new(
                         sample.tensor.mean_rgb(),
                         Some(sample.label),
                         loss_before,
                         loss_after,
@@ -389,67 +420,64 @@ fn train_with_phase_3b(
             .filter(|sample| classifier.predict(&sample.tensor) == sample.label)
             .count();
         let train_accuracy = if train_data.is_empty() {
             0.0
         } else {
             train_correct as f32 / train_data.len() as f32
         };
 
         final_train_accuracy = train_accuracy;
 
         let avg_train_loss = if batch_count > 0 {
             epoch_loss / batch_count as f32
         } else {
             0.0
         };
 
         epoch_metrics.push(EpochMetrics {
             epoch: epoch + 1,
             train_loss: avg_train_loss,
             train_accuracy,
             val_loss,
             val_accuracy,
             learning_rate: current_lr,
             elapsed_ms: epoch_start.elapsed().as_millis(),
             dreams_used: dreams_used.max(epoch_feedback),
+            unique_dreams: epoch_feedback,
         });
 
         current_lr *= config.lr_decay;
 
         if converged_epoch.is_some() && epoch >= converged_epoch.unwrap() + 5 {
             break;
         }
 
         // Refresh pool with recent tensors for continued retrieval
         if epoch % 5 == 0 {
             for sample in batch_subset(train_data, epoch) {
                 if let Ok(result) = solver.evaluate(&sample.tensor, false) {
                     pool.add_with_class(sample.tensor.clone(), result, sample.label);
                 }
             }
         }
     }
 
     TrainingResult {
         config,
         epoch_metrics,
         final_train_accuracy,
         final_val_accuracy: best_val_accuracy,
         total_elapsed_ms: start.elapsed().as_millis(),
         converged_epoch,
     }
 }
 
 /// Helper: deterministic subset selection for periodic pool refresh.
 fn batch_subset<'a>(samples: &'a [ColorSample], epoch: usize) -> Vec<&'a ColorSample> {
     if samples.is_empty() {
         return Vec::new();
     }
 
     let step = (samples.len() / 16).max(1);
     let offset = epoch % step;
-    samples
-        .iter()
-        .skip(offset)
-        .step_by(step)
-        .collect()
+    samples.iter().skip(offset).step_by(step).collect()
 }
diff --git a/examples/solver_demo.rs b/examples/solver_demo.rs
index 2884da1f2a7039f8da2d38ad2ef0b93d7270751e..20fe15c80bb17cd13fe8e9530bcd78cdd40787a2 100644
--- a/examples/solver_demo.rs
+++ b/examples/solver_demo.rs
@@ -1,108 +1,140 @@
 /// Demonstration of the chromatic field solver
 ///
 /// This example shows how to:
 /// 1. Create a chromatic tensor
 /// 2. Evaluate it with the native solver
 /// 3. Interpret energy, coherence, and violation metrics
 /// 4. Compute gradients for optimization
-
-use chromatic_cognition_core::{ChromaticTensor, ChromaticNativeSolver, Solver};
+use chromatic_cognition_core::{ChromaticNativeSolver, ChromaticTensor, Solver};
 
 fn main() {
     println!("=== Chromatic Field Solver Demo ===\n");
 
     // Create a solver with default parameters
     let mut solver = ChromaticNativeSolver::new();
     println!("Solver: {}", solver.name());
     println!("Parameters:");
     println!("  lambda_tv (total variation): {}", solver.lambda_tv);
     println!("  lambda_sat (saturation penalty): {}", solver.lambda_sat);
     println!("  target_saturation: {}", solver.target_saturation);
-    println!("  discontinuity_threshold: {}\n", solver.discontinuity_threshold);
+    println!(
+        "  discontinuity_threshold: {}\n",
+        solver.discontinuity_threshold
+    );
 
     // Example 1: Smooth random field
     println!("--- Example 1: Smooth Random Field ---");
     let smooth_field = ChromaticTensor::from_seed(42, 8, 8, 2);
     let result = solver.evaluate(&smooth_field, false).expect("eval failed");
-    println!("Energy: {:.4} (total variation + saturation penalty)", result.energy);
-    println!("Coherence: {:.4} (0-1, higher = more harmonious)", result.coherence);
-    println!("Violation: {:.4} (0-1, lower = fewer constraint violations)\n", result.violation);
+    println!(
+        "Energy: {:.4} (total variation + saturation penalty)",
+        result.energy
+    );
+    println!(
+        "Coherence: {:.4} (0-1, higher = more harmonious)",
+        result.coherence
+    );
+    println!(
+        "Violation: {:.4} (0-1, lower = fewer constraint violations)\n",
+        result.violation
+    );
 
     // Example 2: High contrast field (checkerboard pattern)
     println!("--- Example 2: High Contrast Field ---");
     let mut colors = ndarray::Array4::zeros((4, 4, 1, 3));
     for r in 0..4 {
         for c in 0..4 {
             let is_black_square = (r + c) % 2 == 0;
             if is_black_square {
                 colors[[r, c, 0, 0]] = 0.0; // Black
                 colors[[r, c, 0, 1]] = 0.0;
                 colors[[r, c, 0, 2]] = 0.0;
             } else {
                 colors[[r, c, 0, 0]] = 1.0; // White
                 colors[[r, c, 0, 1]] = 1.0;
                 colors[[r, c, 0, 2]] = 1.0;
             }
         }
     }
     let certainty = ndarray::Array3::ones((4, 4, 1));
     let checkerboard = ChromaticTensor::from_arrays(colors, certainty);
 
     let result = solver.evaluate(&checkerboard, false).expect("eval failed");
-    println!("Energy: {:.4} (high due to sharp transitions)", result.energy);
-    println!("Coherence: {:.4} (low due to extreme black/white)", result.coherence);
-    println!("Violation: {:.4} (low, colors in gamut)\n", result.violation);
+    println!(
+        "Energy: {:.4} (high due to sharp transitions)",
+        result.energy
+    );
+    println!(
+        "Coherence: {:.4} (low due to extreme black/white)",
+        result.coherence
+    );
+    println!(
+        "Violation: {:.4} (low, colors in gamut)\n",
+        result.violation
+    );
 
     // Example 3: Pure colors (high saturation)
     println!("--- Example 3: Pure RGB Colors ---");
     let mut pure_colors = ndarray::Array4::zeros((3, 1, 1, 3));
     pure_colors[[0, 0, 0, 0]] = 1.0; // Pure red
     pure_colors[[1, 0, 0, 1]] = 1.0; // Pure green
     pure_colors[[2, 0, 0, 2]] = 1.0; // Pure blue
     let certainty = ndarray::Array3::ones((3, 1, 1));
     let pure_field = ChromaticTensor::from_arrays(pure_colors, certainty);
 
     let result = solver.evaluate(&pure_field, false).expect("eval failed");
-    println!("Energy: {:.4} (high saturation deviation from target)", result.energy);
-    println!("Coherence: {:.4} (complementary colors present)", result.coherence);
-    println!("Violation: {:.4} (high saturation flagged)\n", result.violation);
+    println!(
+        "Energy: {:.4} (high saturation deviation from target)",
+        result.energy
+    );
+    println!(
+        "Coherence: {:.4} (complementary colors present)",
+        result.coherence
+    );
+    println!(
+        "Violation: {:.4} (high saturation flagged)\n",
+        result.violation
+    );
 
     // Example 4: Out-of-gamut colors (constraint violation)
     println!("--- Example 4: Out-of-Gamut Colors ---");
     let mut bad_colors = ndarray::Array4::zeros((2, 2, 1, 3));
     bad_colors[[0, 0, 0, 0]] = 1.5; // Out of gamut (> 1.0)
     bad_colors[[0, 1, 0, 1]] = -0.2; // Out of gamut (< 0.0)
     bad_colors[[1, 0, 0, 2]] = 0.5; // Valid
     bad_colors[[1, 1, 0, 0]] = 0.8; // Valid
     let certainty = ndarray::Array3::ones((2, 2, 1));
     let bad_field = ChromaticTensor::from_arrays(bad_colors, certainty);
 
     let result = solver.evaluate(&bad_field, false).expect("eval failed");
     println!("Energy: {:.4}", result.energy);
     println!("Coherence: {:.4}", result.coherence);
-    println!("Violation: {:.4} (HIGH - out-of-gamut colors detected)\n", result.violation);
+    println!(
+        "Violation: {:.4} (HIGH - out-of-gamut colors detected)\n",
+        result.violation
+    );
 
     // Example 5: Gradient computation
     println!("--- Example 5: Gradient Computation ---");
     let test_field = ChromaticTensor::from_seed(123, 4, 4, 1);
     let result_with_grad = solver.evaluate(&test_field, true).expect("eval failed");
 
     if let Some(grad) = &result_with_grad.grad {
         println!("Gradient computed: {} values (4×4×1×3)", grad.len());
         println!("First 9 gradient components:");
         for i in 0..9.min(grad.len()) {
             println!("  grad[{}] = {:.4}", i, grad[i]);
         }
         println!("\nGradient can be used for optimization (gradient descent)");
     }
 
     println!("\n=== Interpretation Guide ===");
     println!("Energy: Total field \"cost\" (lower is smoother/better)");
     println!("  - Combines spatial smoothness (total variation)");
     println!("  - And deviation from target saturation");
     println!();
     println!("Coherence: Color harmony score (0-1, higher is better)");
     println!("  - Measures complementary balance (red-cyan, green-magenta, etc.)");
     println!("  - Measures hue consistency (similar hues = more coherent)");
     println!();
     println!("Violation: Constraint violation score (0-1, lower is better)");
diff --git a/examples/train_color_classifier.rs b/examples/train_color_classifier.rs
index 92028a383f2ffa2a65bc27b4056fe73e6c5b0833..0c02027bfd0c85ee65d8a103bba9cc7be3434f80 100644
--- a/examples/train_color_classifier.rs
+++ b/examples/train_color_classifier.rs
@@ -1,31 +1,33 @@
 //! Training example for chromatic neural network on color classification.
 //!
 //! This example trains a chromatic neural network to classify patterns
 //! into primary colors (red, green, blue).
 
-use chromatic_cognition_core::data::{generate_primary_color_dataset, shuffle_dataset, split_dataset};
+use chromatic_cognition_core::data::{
+    generate_primary_color_dataset, shuffle_dataset, split_dataset,
+};
 use chromatic_cognition_core::neural::{ChromaticNetwork, SGDOptimizer};
 use chromatic_cognition_core::tensor::gradient::GradientLayer;
 use std::path::PathBuf;
 
 fn main() -> Result<(), Box<dyn std::error::Error>> {
     println!("🎨 Chromatic Neural Network - Color Classification");
     println!("==================================================\n");
 
     // Hyperparameters
     let samples_per_class = 50;
     let tensor_size = (16, 16, 4); // Small for fast training
     let num_classes = 3; // Red, Green, Blue
     let epochs = 20;
     let learning_rate = 0.05;
     let momentum = 0.9;
     let weight_decay = 0.0001;
 
     println!("Configuration:");
     println!("  Samples per class: {}", samples_per_class);
     println!("  Tensor size: {:?}", tensor_size);
     println!("  Epochs: {}", epochs);
     println!("  Learning rate: {}", learning_rate);
     println!("  Momentum: {}", momentum);
     println!("  Weight decay: {}", weight_decay);
     println!();
@@ -84,69 +86,74 @@ fn main() -> Result<(), Box<dyn std::error::Error>> {
             epochs,
             train_loss,
             train_acc * 100.0,
             val_loss,
             val_acc * 100.0
         );
     }
 
     println!("\n✅ Training complete!");
     println!();
 
     // Final evaluation
     println!("📈 Final Evaluation:");
     let val_inputs: Vec<_> = val_data.iter().map(|p| p.tensor.clone()).collect();
     let val_labels: Vec<_> = val_data.iter().map(|p| p.label).collect();
     let (final_loss, final_acc) = network.evaluate(&val_inputs, &val_labels);
 
     println!("  Validation Loss: {:.4}", final_loss);
     println!("  Validation Accuracy: {:.2}%", final_acc * 100.0);
     println!();
 
     // Per-class accuracy
     println!("📊 Per-Class Performance:");
     let class_names = ["Red", "Green", "Blue"];
     for class in 0..num_classes {
-        let class_samples: Vec<_> = val_data
-            .iter()
-            .filter(|p| p.label == class)
-            .collect();
+        let class_samples: Vec<_> = val_data.iter().filter(|p| p.label == class).collect();
 
         if !class_samples.is_empty() {
             let class_inputs: Vec<_> = class_samples.iter().map(|p| p.tensor.clone()).collect();
             let class_labels: Vec<_> = class_samples.iter().map(|p| p.label).collect();
             let (_loss, acc) = network.evaluate(&class_inputs, &class_labels);
 
             println!("  {}: {:.2}%", class_names[class], acc * 100.0);
         }
     }
     println!();
 
     // Visualize some predictions
     println!("🖼️  Visualizing sample predictions...");
     std::fs::create_dir_all("out/predictions")?;
 
     for (i, pattern) in val_data.iter().take(9).enumerate() {
         let output = network.forward(&pattern.tensor);
         let gradient = GradientLayer::from_tensor(&output);
 
         let filename = format!("out/predictions/sample_{}_label_{}.png", i, pattern.label);
         gradient.to_png(PathBuf::from(&filename))?;
 
         let stats = output.statistics();
-        let predicted_class = if stats.mean_rgb[0] > stats.mean_rgb[1] && stats.mean_rgb[0] > stats.mean_rgb[2] {
-            0
-        } else if stats.mean_rgb[1] > stats.mean_rgb[2] {
-            1
+        let predicted_class =
+            if stats.mean_rgb[0] > stats.mean_rgb[1] && stats.mean_rgb[0] > stats.mean_rgb[2] {
+                0
+            } else if stats.mean_rgb[1] > stats.mean_rgb[2] {
+                1
+            } else {
+                2
+            };
+
+        let correct = if predicted_class == pattern.label {
+            "✓"
         } else {
-            2
+            "✗"
         };
-
-        let correct = if predicted_class == pattern.label { "✓" } else { "✗" };
-        println!("  {} Sample {}: True={}, Pred={}", correct, i, class_names[pattern.label], class_names[predicted_class]);
+        println!(
+            "  {} Sample {}: True={}, Pred={}",
+            correct, i, class_names[pattern.label], class_names[predicted_class]
+        );
     }
     println!();
 
     println!("✨ Done! Check out/predictions/ for visualizations.");
 
     Ok(())
 }
diff --git a/src/config.rs b/src/config.rs
index 112a0ea3acb94de3adf988022c9c94253a942662..77fee4e46c1ec2d18835cb49b3dc59e1940207d4 100644
--- a/src/config.rs
+++ b/src/config.rs
@@ -1,60 +1,76 @@
 //! Engine configuration management via TOML files.
 //!
 //! This module provides configuration parsing from TOML format with sensible defaults.
 
+use crate::dream::soft_index::Similarity;
+use serde::Serialize;
 use std::collections::HashMap;
 use std::fs;
 use std::path::Path;
 
-use serde::Serialize;
-
 /// Engine configuration loaded from TOML file.
 ///
 /// # Examples
 ///
 /// ```
 /// use chromatic_cognition_core::EngineConfig;
 ///
 /// // Load from file
 /// let config = EngineConfig::load_from_file("config/engine.toml")
 ///     .unwrap_or_else(|_| EngineConfig::default());
 ///
 /// println!("Tensor dimensions: {}x{}x{}", config.rows, config.cols, config.layers);
 /// ```
 #[derive(Debug, Clone, Serialize)]
 pub struct EngineConfig {
     /// Number of rows in the tensor grid
     pub rows: usize,
     /// Number of columns in the tensor grid
     pub cols: usize,
     /// Number of depth layers in the tensor
     pub layers: usize,
     /// Random seed for deterministic initialization
     pub seed: u64,
     /// Target device ("cpu" or future "cuda"/"metal")
     pub device: String,
+    /// Phase 4 retrieval configuration
+    pub phase4: Phase4Config,
+}
+
+/// Configuration for Phase 4 soft retrieval.
+#[derive(Debug, Clone, Serialize)]
+pub struct Phase4Config {
+    pub embed_dim: usize,
+    pub similarity: Similarity,
+    pub alpha: f32,
+    pub beta: f32,
+    pub gamma: f32,
+    pub delta: f32,
+    pub mmr_lambda: f32,
+    pub refresh_interval_steps: usize,
+    pub drift_threshold: f32,
 }
 
 impl EngineConfig {
     pub fn load_from_file<P: AsRef<Path>>(path: P) -> Result<Self, ConfigError> {
         let contents = fs::read_to_string(&path)?;
         Self::from_str(&contents)
     }
 
     pub fn from_str(toml_str: &str) -> Result<Self, ConfigError> {
         let mut section = String::new();
         let mut values: HashMap<String, String> = HashMap::new();
 
         for line in toml_str.lines() {
             let trimmed = line.trim();
             if trimmed.is_empty() || trimmed.starts_with('#') {
                 continue;
             }
 
             if trimmed.starts_with('[') && trimmed.ends_with(']') {
                 section = trimmed.trim_matches(&['[', ']'][..]).to_string();
                 continue;
             }
 
             let (key, value) = trimmed
                 .split_once('=')
@@ -70,69 +86,155 @@ impl EngineConfig {
             .transpose()
             .map_err(|_| ConfigError::Parse("rows must be an integer".into()))?
             .unwrap_or(64);
         let cols = values
             .remove("engine::cols")
             .map(|v| v.parse())
             .transpose()
             .map_err(|_| ConfigError::Parse("cols must be an integer".into()))?
             .unwrap_or(64);
         let layers = values
             .remove("engine::layers")
             .map(|v| v.parse())
             .transpose()
             .map_err(|_| ConfigError::Parse("layers must be an integer".into()))?
             .unwrap_or(8);
         let seed = values
             .remove("engine::seed")
             .map(|v| v.parse())
             .transpose()
             .map_err(|_| ConfigError::Parse("seed must be an integer".into()))?
             .unwrap_or(42);
         let device = values
             .remove("engine::device")
             .unwrap_or_else(|| "cpu".to_string());
 
+        let phase4 = Phase4Config::from_map(&mut values)?;
+
         Ok(Self {
             rows,
             cols,
             layers,
             seed,
             device,
+            phase4,
         })
     }
 }
 
 impl Default for EngineConfig {
     fn default() -> Self {
         Self {
             rows: 64,
             cols: 64,
             layers: 8,
             seed: 42,
             device: "cpu".to_string(),
+            phase4: Phase4Config::default(),
+        }
+    }
+}
+
+impl Phase4Config {
+    fn from_map(values: &mut HashMap<String, String>) -> Result<Self, ConfigError> {
+        let embed_dim = values
+            .remove("phase4::embed_dim")
+            .map(|v| v.parse())
+            .transpose()
+            .map_err(|_| ConfigError::Parse("embed_dim must be an integer".into()))?
+            .unwrap_or(64);
+
+        let similarity = values
+            .remove("phase4::similarity")
+            .map(|v| parse_similarity(&v))
+            .transpose()?
+            .unwrap_or(Similarity::Cosine);
+
+        let alpha = parse_f32(values, "phase4::alpha", 0.65)?;
+        let beta = parse_f32(values, "phase4::beta", 0.20)?;
+        let gamma = parse_f32(values, "phase4::gamma", 0.10)?;
+        let delta = parse_f32(values, "phase4::delta", 0.05)?;
+        let mmr_lambda = parse_f32(values, "phase4::mmr_lambda", 0.7)?;
+        let refresh_interval_steps = values
+            .remove("phase4::refresh_interval_steps")
+            .map(|v| v.parse())
+            .transpose()
+            .map_err(|_| ConfigError::Parse("refresh_interval_steps must be an integer".into()))?
+            .unwrap_or(500);
+        let drift_threshold = parse_f32(values, "phase4::drift_threshold", 0.08)?;
+
+        Ok(Self {
+            embed_dim,
+            similarity,
+            alpha,
+            beta,
+            gamma,
+            delta,
+            mmr_lambda,
+            refresh_interval_steps,
+            drift_threshold,
+        })
+    }
+}
+
+impl Default for Phase4Config {
+    fn default() -> Self {
+        Self {
+            embed_dim: 64,
+            similarity: Similarity::Cosine,
+            alpha: 0.65,
+            beta: 0.20,
+            gamma: 0.10,
+            delta: 0.05,
+            mmr_lambda: 0.7,
+            refresh_interval_steps: 500,
+            drift_threshold: 0.08,
         }
     }
 }
 
 #[derive(Debug)]
 pub enum ConfigError {
     Io(std::io::Error),
     Parse(String),
 }
 
 impl std::fmt::Display for ConfigError {
     fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
         match self {
             ConfigError::Io(err) => write!(f, "IO error: {}", err),
             ConfigError::Parse(err) => write!(f, "Parse error: {}", err),
         }
     }
 }
 
 impl std::error::Error for ConfigError {}
 
 impl From<std::io::Error> for ConfigError {
     fn from(value: std::io::Error) -> Self {
         ConfigError::Io(value)
     }
 }
+
+fn parse_similarity(value: &str) -> Result<Similarity, ConfigError> {
+    match value.to_ascii_lowercase().as_str() {
+        "cosine" => Ok(Similarity::Cosine),
+        "euclidean" => Ok(Similarity::Euclidean),
+        other => Err(ConfigError::Parse(format!(
+            "invalid similarity '{}', expected cosine or euclidean",
+            other
+        ))),
+    }
+}
+
+fn parse_f32(
+    values: &mut HashMap<String, String>,
+    key: &str,
+    default: f32,
+) -> Result<f32, ConfigError> {
+    Ok(values
+        .remove(key)
+        .map(|v| v.parse())
+        .transpose()
+        .map_err(|_| ConfigError::Parse(format!("{} must be a float", key)))?
+        .unwrap_or(default))
+}
diff --git a/src/data/color_dataset.rs b/src/data/color_dataset.rs
index 72ee878e5cc8fa803bd78a5f49aea05cb7d653b5..a16d8de636c32079e9600fcb17f6f24d5d161ba1 100644
--- a/src/data/color_dataset.rs
+++ b/src/data/color_dataset.rs
@@ -1,38 +1,39 @@
 //! Color classification dataset for validation experiments
 //!
 //! Provides a simple 10-class color classification task with synthetic data.
 //! Each sample is a ChromaticTensor filled with a specific color + noise.
 
 use crate::tensor::ChromaticTensor;
 use ndarray::Array4;
+use rand::rngs::StdRng;
 use rand::Rng;
 use rand::SeedableRng;
-use rand::rngs::StdRng;
+use serde::{Deserialize, Serialize};
 
 /// 10 standard color classes for classification
-#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
+#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
 #[repr(usize)]
 pub enum ColorClass {
     Red = 0,
     Green = 1,
     Blue = 2,
     Yellow = 3,
     Cyan = 4,
     Magenta = 5,
     Orange = 6,
     Purple = 7,
     White = 8,
     Black = 9,
 }
 
 impl ColorClass {
     /// Get the canonical RGB values for this color class
     pub fn rgb(&self) -> [f32; 3] {
         match self {
             ColorClass::Red => [1.0, 0.0, 0.0],
             ColorClass::Green => [0.0, 1.0, 0.0],
             ColorClass::Blue => [0.0, 0.0, 1.0],
             ColorClass::Yellow => [1.0, 1.0, 0.0],
             ColorClass::Cyan => [0.0, 1.0, 1.0],
             ColorClass::Magenta => [1.0, 0.0, 1.0],
             ColorClass::Orange => [1.0, 0.5, 0.0],
@@ -60,51 +61,51 @@ impl ColorClass {
     }
 
     /// Total number of color classes
     pub fn num_classes() -> usize {
         10
     }
 
     /// Get all color classes
     pub fn all() -> [ColorClass; 10] {
         [
             ColorClass::Red,
             ColorClass::Green,
             ColorClass::Blue,
             ColorClass::Yellow,
             ColorClass::Cyan,
             ColorClass::Magenta,
             ColorClass::Orange,
             ColorClass::Purple,
             ColorClass::White,
             ColorClass::Black,
         ]
     }
 }
 
 /// A single training/validation sample
-#[derive(Clone)]
+#[derive(Clone, Serialize, Deserialize)]
 pub struct ColorSample {
     pub tensor: ChromaticTensor,
     pub label: ColorClass,
 }
 
 /// Configuration for dataset generation
 #[derive(Debug, Clone)]
 pub struct DatasetConfig {
     /// Tensor dimensions (rows, cols, layers)
     pub tensor_size: (usize, usize, usize),
     /// Amount of Gaussian noise to add (stddev)
     pub noise_level: f32,
     /// Number of samples per color class
     pub samples_per_class: usize,
     /// Random seed for reproducibility
     pub seed: u64,
 }
 
 impl Default for DatasetConfig {
     fn default() -> Self {
         Self {
             tensor_size: (16, 16, 4),
             noise_level: 0.1,
             samples_per_class: 100,
             seed: 42,
@@ -117,51 +118,52 @@ pub struct ColorDataset {
     pub samples: Vec<ColorSample>,
     pub config: DatasetConfig,
 }
 
 impl ColorDataset {
     /// Generate a new synthetic color dataset
     pub fn generate(config: DatasetConfig) -> Self {
         let mut rng = StdRng::seed_from_u64(config.seed);
         let mut samples = Vec::new();
 
         let (rows, cols, layers) = config.tensor_size;
 
         for color_class in ColorClass::all() {
             let base_rgb = color_class.rgb();
 
             for _ in 0..config.samples_per_class {
                 // Create tensor filled with base color + noise
                 let mut colors = Array4::zeros((rows, cols, layers, 3));
                 let mut certainty = ndarray::Array3::zeros((rows, cols, layers));
 
                 for r in 0..rows {
                     for c in 0..cols {
                         for l in 0..layers {
                             // Base color + Gaussian noise
                             for channel in 0..3 {
-                                let noise = rng.gen::<f32>() * config.noise_level * 2.0 - config.noise_level;
+                                let noise = rng.gen::<f32>() * config.noise_level * 2.0
+                                    - config.noise_level;
                                 let value = (base_rgb[channel] + noise).clamp(0.0, 1.0);
                                 colors[[r, c, l, channel]] = value;
                             }
 
                             // Random certainty
                             certainty[[r, c, l]] = rng.gen::<f32>() * 0.5 + 0.5;
                         }
                     }
                 }
 
                 let tensor = ChromaticTensor::from_arrays(colors, certainty);
                 samples.push(ColorSample {
                     tensor,
                     label: color_class,
                 });
             }
         }
 
         // Shuffle samples
         use rand::seq::SliceRandom;
         samples.shuffle(&mut rng);
 
         Self { samples, config }
     }
 
diff --git a/src/data/mod.rs b/src/data/mod.rs
index fb695a118ba91f9dafa390c13be5777cc0b3b052..8c16a3ef3d826401dfe1cd6bf73066a765643933 100644
--- a/src/data/mod.rs
+++ b/src/data/mod.rs
@@ -1,7 +1,7 @@
 //! Dataset and pattern generation for chromatic neural networks.
 
 pub mod color_dataset;
 pub mod pattern;
 
 pub use color_dataset::{ColorClass, ColorDataset, ColorSample, DatasetConfig};
-pub use pattern::{ColorPattern, generate_primary_color_dataset, shuffle_dataset, split_dataset};
+pub use pattern::{generate_primary_color_dataset, shuffle_dataset, split_dataset, ColorPattern};
diff --git a/src/data/pattern.rs b/src/data/pattern.rs
index ac2661f1088c3e3ce46870f18a519e45a11732a1..a816dc4218963a3451bce536a436ce89ab0609b2 100644
--- a/src/data/pattern.rs
+++ b/src/data/pattern.rs
@@ -35,89 +35,91 @@ pub struct ColorPattern {
 /// ```
 /// use chromatic_cognition_core::data::generate_primary_color_dataset;
 ///
 /// let dataset = generate_primary_color_dataset(100, 16, 16, 4, 42);
 /// assert_eq!(dataset.len(), 300); // 100 per class × 3 classes
 /// ```
 pub fn generate_primary_color_dataset(
     samples_per_class: usize,
     rows: usize,
     cols: usize,
     layers: usize,
     seed: u64,
 ) -> Vec<ColorPattern> {
     let mut dataset = Vec::new();
     let mut rng = rand::rngs::StdRng::seed_from_u64(seed);
 
     let class_names = ["red", "green", "blue"];
     let base_colors = [
         [0.9, 0.1, 0.1], // Red
         [0.1, 0.9, 0.1], // Green
         [0.1, 0.1, 0.9], // Blue
     ];
 
     for class in 0..3 {
         for sample_idx in 0..samples_per_class {
-            let sample_seed = seed
-                .wrapping_add((class * samples_per_class + sample_idx) as u64);
+            let sample_seed = seed.wrapping_add((class * samples_per_class + sample_idx) as u64);
 
             // Start with random tensor
             let mut tensor = ChromaticTensor::from_seed(sample_seed, rows, cols, layers);
 
             // Bias toward class color
             let base_color = base_colors[class];
             let intensity = 0.7 + rng.gen::<f32>() * 0.3; // 0.7 to 1.0
 
             for row in 0..rows {
                 for col in 0..cols {
                     for layer in 0..layers {
                         // Mix with base color
                         let r = tensor.colors[[row, col, layer, 0]];
                         let g = tensor.colors[[row, col, layer, 1]];
                         let b = tensor.colors[[row, col, layer, 2]];
 
                         tensor.colors[[row, col, layer, 0]] =
                             (r * (1.0 - intensity) + base_color[0] * intensity).clamp(0.0, 1.0);
                         tensor.colors[[row, col, layer, 1]] =
                             (g * (1.0 - intensity) + base_color[1] * intensity).clamp(0.0, 1.0);
                         tensor.colors[[row, col, layer, 2]] =
                             (b * (1.0 - intensity) + base_color[2] * intensity).clamp(0.0, 1.0);
 
                         // Add some certainty variation
                         tensor.certainty[[row, col, layer]] = 0.5 + rng.gen::<f32>() * 0.5;
                     }
                 }
             }
 
             // Add noise
             let noise_level = 0.1;
             for row in 0..rows {
                 for col in 0..cols {
                     for layer in 0..layers {
-                        tensor.colors[[row, col, layer, 0]] += (rng.gen::<f32>() - 0.5) * noise_level;
-                        tensor.colors[[row, col, layer, 1]] += (rng.gen::<f32>() - 0.5) * noise_level;
-                        tensor.colors[[row, col, layer, 2]] += (rng.gen::<f32>() - 0.5) * noise_level;
+                        tensor.colors[[row, col, layer, 0]] +=
+                            (rng.gen::<f32>() - 0.5) * noise_level;
+                        tensor.colors[[row, col, layer, 1]] +=
+                            (rng.gen::<f32>() - 0.5) * noise_level;
+                        tensor.colors[[row, col, layer, 2]] +=
+                            (rng.gen::<f32>() - 0.5) * noise_level;
                     }
                 }
             }
 
             // Clamp to valid range
             tensor = tensor.clamp(0.0, 1.0);
 
             dataset.push(ColorPattern {
                 tensor,
                 label: class,
                 description: format!("{} pattern #{}", class_names[class], sample_idx),
             });
         }
     }
 
     dataset
 }
 
 /// Splits a dataset into training and validation sets.
 ///
 /// # Arguments
 ///
 /// * `dataset` - Full dataset
 /// * `train_fraction` - Fraction to use for training (e.g., 0.8)
 ///
@@ -174,28 +176,28 @@ mod tests {
             assert!(stats.mean_rgb[0] > stats.mean_rgb[1]); // R > G
             assert!(stats.mean_rgb[0] > stats.mean_rgb[2]); // R > B
         }
 
         // Green class should have higher green values
         for pattern in dataset.iter().filter(|p| p.label == 1) {
             let stats = pattern.tensor.statistics();
             assert!(stats.mean_rgb[1] > stats.mean_rgb[0]); // G > R
             assert!(stats.mean_rgb[1] > stats.mean_rgb[2]); // G > B
         }
 
         // Blue class should have higher blue values
         for pattern in dataset.iter().filter(|p| p.label == 2) {
             let stats = pattern.tensor.statistics();
             assert!(stats.mean_rgb[2] > stats.mean_rgb[0]); // B > R
             assert!(stats.mean_rgb[2] > stats.mean_rgb[1]); // B > G
         }
     }
 
     #[test]
     fn test_split_dataset() {
         let dataset = generate_primary_color_dataset(10, 4, 4, 2, 42);
         let (train, val) = split_dataset(dataset, 0.8);
 
         assert_eq!(train.len(), 24); // 0.8 * 30
-        assert_eq!(val.len(), 6);    // 0.2 * 30
+        assert_eq!(val.len(), 6); // 0.2 * 30
     }
 }
diff --git a/src/dream/analysis.rs b/src/dream/analysis.rs
index d23da6f83256289ac6fd86d594b920fdd6376d60..980f7e9d3452e36499ff1f1872c689827ac56acb 100644
--- a/src/dream/analysis.rs
+++ b/src/dream/analysis.rs
@@ -1,32 +1,32 @@
 //! Statistical analysis utilities for experiment results
 //!
 //! Provides tools for analyzing and comparing A/B test results,
 //! including statistical significance testing.
 
-use crate::dream::experiment::{ExperimentResult, EpochMetrics};
-use serde::{Serialize, Deserialize};
+use crate::dream::experiment::{EpochMetrics, ExperimentResult};
+use serde::{Deserialize, Serialize};
 
 /// Comparison of two experiment results
 #[derive(Debug, Clone, Serialize, Deserialize)]
 pub struct ExperimentComparison {
     pub control_strategy: String,
     pub test_strategy: String,
     pub accuracy_improvement: f64,
     pub accuracy_improvement_pct: f64,
     pub convergence_epoch_delta: Option<i32>,
     pub mean_coherence_improvement: f64,
     pub mean_energy_improvement: f64,
     pub is_significant: bool,
     pub significance_level: f64,
 }
 
 /// Compute basic statistics for a sequence of values
 #[derive(Debug, Clone, Serialize)]
 pub struct Statistics {
     pub mean: f64,
     pub variance: f64,
     pub std_dev: f64,
     pub min: f64,
     pub max: f64,
     pub count: usize,
 }
@@ -76,102 +76,107 @@ impl Statistics {
 pub fn compare_experiments(
     control: &ExperimentResult,
     test: &ExperimentResult,
     significance_threshold: f64,
 ) -> ExperimentComparison {
     // Accuracy comparison
     let accuracy_improvement = test.final_accuracy - control.final_accuracy;
     let accuracy_improvement_pct = if control.final_accuracy > 0.0 {
         (accuracy_improvement / control.final_accuracy) * 100.0
     } else {
         0.0
     };
 
     // Convergence epoch comparison
     let convergence_epoch_delta = match (control.convergence_epoch, test.convergence_epoch) {
         (Some(a), Some(b)) => Some(a as i32 - b as i32),
         _ => None,
     };
 
     // Mean coherence comparison
     let control_coherence: Vec<f64> = control
         .epoch_metrics
         .iter()
         .map(|m| m.mean_coherence)
         .collect();
-    let test_coherence: Vec<f64> = test.epoch_metrics.iter().map(|m| m.mean_coherence).collect();
+    let test_coherence: Vec<f64> = test
+        .epoch_metrics
+        .iter()
+        .map(|m| m.mean_coherence)
+        .collect();
 
     let mean_coherence_control = Statistics::from_slice(&control_coherence).mean;
     let mean_coherence_test = Statistics::from_slice(&test_coherence).mean;
     let mean_coherence_improvement = mean_coherence_test - mean_coherence_control;
 
     // Mean energy comparison (lower is better)
-    let control_energy: Vec<f64> = control.epoch_metrics.iter().map(|m| m.mean_energy).collect();
+    let control_energy: Vec<f64> = control
+        .epoch_metrics
+        .iter()
+        .map(|m| m.mean_energy)
+        .collect();
     let test_energy: Vec<f64> = test.epoch_metrics.iter().map(|m| m.mean_energy).collect();
 
     let mean_energy_control = Statistics::from_slice(&control_energy).mean;
     let mean_energy_test = Statistics::from_slice(&test_energy).mean;
     let mean_energy_improvement = mean_energy_control - mean_energy_test; // Note: reversed (lower is better)
 
     // Simple significance test: improvement must be > threshold and positive
-    let is_significant = accuracy_improvement > significance_threshold && accuracy_improvement > 0.0;
+    let is_significant =
+        accuracy_improvement > significance_threshold && accuracy_improvement > 0.0;
 
     ExperimentComparison {
         control_strategy: control.strategy.clone(),
         test_strategy: test.strategy.clone(),
         accuracy_improvement,
         accuracy_improvement_pct,
         convergence_epoch_delta,
         mean_coherence_improvement,
         mean_energy_improvement,
         is_significant,
         significance_level: significance_threshold,
     }
 }
 
 /// Compute t-test statistic for two independent samples
 ///
 /// Returns (t_statistic, degrees_of_freedom)
 /// Note: This is Welch's t-test (unequal variances)
 pub fn welch_t_test(sample1: &[f64], sample2: &[f64]) -> (f64, f64) {
     let stats1 = Statistics::from_slice(sample1);
     let stats2 = Statistics::from_slice(sample2);
 
     if stats1.count == 0 || stats2.count == 0 {
         return (0.0, 0.0);
     }
 
     let mean_diff = stats1.mean - stats2.mean;
     let se1 = stats1.variance / stats1.count as f64;
     let se2 = stats2.variance / stats2.count as f64;
     let se = (se1 + se2).sqrt();
 
-    let t_stat = if se > 0.0 {
-        mean_diff / se
-    } else {
-        0.0
-    };
+    let t_stat = if se > 0.0 { mean_diff / se } else { 0.0 };
 
     // Welch-Satterthwaite degrees of freedom
     let df = if se1 > 0.0 && se2 > 0.0 {
         let numerator = (se1 + se2).powi(2);
         let denom1 = se1.powi(2) / (stats1.count - 1) as f64;
         let denom2 = se2.powi(2) / (stats2.count - 1) as f64;
         numerator / (denom1 + denom2)
     } else {
         0.0
     };
 
     (t_stat, df)
 }
 
 /// Generate a summary report comparing two experiments
 pub fn generate_report(comparison: &ExperimentComparison) -> String {
     let mut report = String::new();
 
     report.push_str("=== Experiment Comparison Report ===\n\n");
 
     report.push_str(&format!(
         "Control Strategy: {}\n",
         comparison.control_strategy
     ));
     report.push_str(&format!("Test Strategy: {}\n\n", comparison.test_strategy));
@@ -205,73 +210,72 @@ pub fn generate_report(comparison: &ExperimentComparison) -> String {
     ));
     report.push_str(&format!(
         "Mean Energy Improvement: {:.4}\n",
         comparison.mean_energy_improvement
     ));
 
     report.push_str("\n--- Statistical Significance ---\n");
     report.push_str(&format!(
         "Significance Threshold: {:.4}\n",
         comparison.significance_level
     ));
     report.push_str(&format!(
         "Is Significant: {}\n",
         if comparison.is_significant {
             "YES ✓"
         } else {
             "NO"
         }
     ));
 
     report.push_str("\n--- Conclusion ---\n");
     if comparison.is_significant {
         report.push_str("✅ The test strategy shows statistically significant improvement.\n");
         report.push_str("   Recommendation: PROCEED with Dream Pool implementation.\n");
     } else if comparison.accuracy_improvement > 0.0 {
-        report.push_str("⚠️  The test strategy shows improvement, but below significance threshold.\n");
+        report.push_str(
+            "⚠️  The test strategy shows improvement, but below significance threshold.\n",
+        );
         report.push_str("   Recommendation: INVESTIGATE further or tune parameters.\n");
     } else {
         report.push_str("❌ The test strategy does not show improvement.\n");
         report.push_str("   Recommendation: DEFER Dream Pool implementation.\n");
     }
 
     report
 }
 
 /// Analyze learning curves (accuracy over epochs)
 pub fn analyze_learning_curves(
     control_metrics: &[EpochMetrics],
     test_metrics: &[EpochMetrics],
 ) -> LearningCurveAnalysis {
     let control_accuracies: Vec<f64> = control_metrics
         .iter()
         .map(|m| m.validation_accuracy)
         .collect();
-    let test_accuracies: Vec<f64> = test_metrics
-        .iter()
-        .map(|m| m.validation_accuracy)
-        .collect();
+    let test_accuracies: Vec<f64> = test_metrics.iter().map(|m| m.validation_accuracy).collect();
 
     let control_stats = Statistics::from_slice(&control_accuracies);
     let test_stats = Statistics::from_slice(&test_accuracies);
 
     // Find epoch where test first exceeds control
     let mut first_improvement_epoch = None;
     for (i, (control, test)) in control_metrics.iter().zip(test_metrics.iter()).enumerate() {
         if test.validation_accuracy > control.validation_accuracy {
             first_improvement_epoch = Some(i);
             break;
         }
     }
 
     LearningCurveAnalysis {
         control_stats,
         test_stats,
         first_improvement_epoch,
     }
 }
 
 /// Learning curve analysis result
 #[derive(Debug, Clone, Serialize)]
 pub struct LearningCurveAnalysis {
     pub control_stats: Statistics,
     pub test_stats: Statistics,
diff --git a/src/dream/bias.rs b/src/dream/bias.rs
index a47ba410a4c424af20f4d50d67b907eb3c0dc5e8..714af74fc5d6a01469b37c79c767e32cc3ff53af 100644
--- a/src/dream/bias.rs
+++ b/src/dream/bias.rs
@@ -272,54 +272,51 @@ fn synthesize_spectral_biases(aggregator: &UtilityAggregator) -> SpectralBias {
         .iter()
         .filter_map(|r| r.spectral_features.as_ref().map(|f| f.high_freq_energy))
         .collect();
 
     let low_freq_threshold = if !low_freqs.is_empty() {
         Some(low_freqs.iter().sum::<f32>() / low_freqs.len() as f32)
     } else {
         None
     };
 
     let high_freq_threshold = if !high_freqs.is_empty() {
         Some(high_freqs.iter().sum::<f32>() / high_freqs.len() as f32)
     } else {
         None
     };
 
     SpectralBias {
         entropy_range,
         entropy_utility_correlation,
         low_freq_threshold,
         high_freq_threshold,
     }
 }
 
 /// Synthesize chromatic signature biases from feedback.
-fn synthesize_chroma_biases(
-    aggregator: &UtilityAggregator,
-    utility_threshold: f32,
-) -> ChromaBias {
+fn synthesize_chroma_biases(aggregator: &UtilityAggregator, utility_threshold: f32) -> ChromaBias {
     // Get helpful dreams
     let helpful = aggregator.filter_by_utility(utility_threshold);
 
     if helpful.is_empty() {
         return ChromaBias {
             red_range: None,
             green_range: None,
             blue_range: None,
         };
     }
 
     // Extract RGB ranges from helpful dreams
     let reds: Vec<f32> = helpful.iter().map(|r| r.chroma_signature[0]).collect();
     let greens: Vec<f32> = helpful.iter().map(|r| r.chroma_signature[1]).collect();
     let blues: Vec<f32> = helpful.iter().map(|r| r.chroma_signature[2]).collect();
 
     let red_range = compute_range(&reds);
     let green_range = compute_range(&greens);
     let blue_range = compute_range(&blues);
 
     ChromaBias {
         red_range,
         green_range,
         blue_range,
     }
diff --git a/src/dream/diversity.rs b/src/dream/diversity.rs
index 6a31d3017e6969b2644d95693df88d83f4d7f869..450d3590e3e9ab4eaf21401d8e7fb7f645e20b08 100644
--- a/src/dream/diversity.rs
+++ b/src/dream/diversity.rs
@@ -1,36 +1,35 @@
 /// Diversity enforcement for Dream Pool retrieval.
 ///
 /// This module implements diversity metrics and Maximum Marginal Relevance (MMR)
 /// to ensure retrieved dreams have sufficient chromatic variation.
 ///
 /// **Problem:** Cosine similarity can return near-duplicates, reducing effective
 /// batch diversity and limiting training data variation.
 ///
 /// **Solution:** MMR balances relevance (similarity to query) with diversity
 /// (dissimilarity to already-selected dreams).
-
 use crate::dream::simple_pool::DreamEntry;
 
 /// Compute pairwise chromatic dispersion between dream entries.
 ///
 /// **Dispersion** = mean pairwise Euclidean distance in RGB space.
 ///
 /// # Arguments
 /// * `dreams` - Set of dream entries to analyze
 ///
 /// # Returns
 /// * Mean pairwise L2 distance across all RGB channels
 /// * Returns 0.0 for empty or single-element sets
 ///
 /// # Example
 /// ```
 /// # use chromatic_cognition_core::dream::diversity::chroma_dispersion;
 /// # use chromatic_cognition_core::dream::simple_pool::DreamEntry;
 /// # use chromatic_cognition_core::tensor::ChromaticTensor;
 /// # use chromatic_cognition_core::solver::SolverResult;
 /// # use serde_json::json;
 /// # let tensor = ChromaticTensor::new(2, 2, 4);
 /// # let result = SolverResult { energy: 0.1, coherence: 0.8, violation: 0.0, grad: None, mask: None, meta: json!({}) };
 /// let mut dreams = vec![
 ///     DreamEntry::new(tensor.clone(), result.clone()),
 ///     DreamEntry::new(tensor.clone(), result.clone()),
@@ -206,51 +205,53 @@ pub fn retrieve_diverse_mmr(
     lambda: f32,
     min_dispersion: f32,
 ) -> Vec<DreamEntry> {
     if candidates.is_empty() || k == 0 {
         return Vec::new();
     }
 
     let mut selected = Vec::with_capacity(k);
     let mut remaining: Vec<&DreamEntry> = candidates.iter().collect();
 
     // Greedy selection: pick best MMR score at each step
     for _ in 0..k {
         if remaining.is_empty() {
             break;
         }
 
         // Find candidate with highest MMR score
         let (best_idx, _) = remaining
             .iter()
             .enumerate()
             .map(|(idx, candidate)| {
                 let score = mmr_score(candidate, query_sig, &selected, lambda);
                 (idx, score)
             })
             .max_by(|(_, score_a), (_, score_b)| {
-                score_a.partial_cmp(score_b).unwrap_or(std::cmp::Ordering::Equal)
+                score_a
+                    .partial_cmp(score_b)
+                    .unwrap_or(std::cmp::Ordering::Equal)
             })
             .unwrap();
 
         // Add to selected set
         let chosen = remaining.remove(best_idx).clone();
         selected.push(chosen);
 
         // Check dispersion constraint
         if min_dispersion > 0.0 && selected.len() >= 2 {
             let dispersion = chroma_dispersion(&selected);
             if dispersion < min_dispersion {
                 // Constraint violated, stop early
                 break;
             }
         }
     }
 
     selected
 }
 
 /// Statistics about dream set diversity.
 #[derive(Debug, Clone)]
 pub struct DiversityStats {
     /// Mean pairwise chromatic dispersion
     pub mean_dispersion: f32,
@@ -263,54 +264,52 @@ pub struct DiversityStats {
 }
 
 impl DiversityStats {
     /// Compute diversity statistics for a set of dreams.
     ///
     /// # Arguments
     /// * `dreams` - Set of dream entries to analyze
     ///
     /// # Returns
     /// * Diversity statistics struct
     pub fn compute(dreams: &[DreamEntry]) -> Self {
         if dreams.len() <= 1 {
             return Self {
                 mean_dispersion: 0.0,
                 min_distance: 0.0,
                 max_distance: 0.0,
                 count: dreams.len(),
             };
         }
 
         let n = dreams.len();
         let mut distances = Vec::new();
 
         for i in 0..n {
             for j in (i + 1)..n {
-                let dist = euclidean_distance(
-                    &dreams[i].chroma_signature,
-                    &dreams[j].chroma_signature,
-                );
+                let dist =
+                    euclidean_distance(&dreams[i].chroma_signature, &dreams[j].chroma_signature);
                 distances.push(dist);
             }
         }
 
         let mean = distances.iter().sum::<f32>() / distances.len() as f32;
         let min = distances.iter().cloned().fold(f32::INFINITY, f32::min);
         let max = distances.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
 
         Self {
             mean_dispersion: mean,
             min_distance: min,
             max_distance: max,
             count: dreams.len(),
         }
     }
 }
 
 #[cfg(test)]
 mod tests {
     use super::*;
     use crate::solver::SolverResult;
     use crate::tensor::ChromaticTensor;
     use serde_json::json;
 
     fn make_dream(sig: [f32; 3]) -> DreamEntry {
@@ -392,52 +391,55 @@ mod tests {
         let score_pure_relevance = mmr_score(&candidate, &query, &selected, 1.0);
         assert!(score_pure_relevance > 0.8); // Still high relevance
     }
 
     #[test]
     fn test_retrieve_diverse_mmr() {
         let candidates = vec![
             make_dream([1.0, 0.0, 0.0]),   // Red (most relevant)
             make_dream([0.95, 0.05, 0.0]), // Red-ish (similar to red)
             make_dream([0.0, 1.0, 0.0]),   // Green (diverse but less relevant)
             make_dream([0.0, 0.0, 1.0]),   // Blue (diverse but less relevant)
         ];
         let query = [1.0, 0.0, 0.0]; // Red query
 
         // With lambda=0.5 (equal balance relevance/diversity), should select red + diverse color
         let selected = retrieve_diverse_mmr(&candidates, &query, 2, 0.5, 0.0);
         assert_eq!(selected.len(), 2);
 
         // First should be pure red (most relevant)
         let sig0 = selected[0].chroma_signature;
         assert!((sig0[0] - 1.0).abs() < 0.1);
 
         // Second should be diverse (green or blue, not red-ish)
         let sig1 = selected[1].chroma_signature;
         // Either green (sig1[1] > 0.5) or blue (sig1[2] > 0.5)
-        assert!(sig1[1] > 0.5 || sig1[2] > 0.5,
-                "Second selection should be diverse (green or blue), got {:?}", sig1);
+        assert!(
+            sig1[1] > 0.5 || sig1[2] > 0.5,
+            "Second selection should be diverse (green or blue), got {:?}",
+            sig1
+        );
     }
 
     #[test]
     fn test_retrieve_diverse_mmr_with_min_dispersion() {
         let candidates = vec![
             make_dream([1.0, 0.0, 0.0]),   // Red
             make_dream([0.99, 0.01, 0.0]), // Almost identical red
             make_dream([0.98, 0.02, 0.0]), // Almost identical red
         ];
         let query = [1.0, 0.0, 0.0];
 
         // With high min_dispersion, should stop early due to constraint
         let selected = retrieve_diverse_mmr(&candidates, &query, 3, 0.7, 0.5);
         assert!(selected.len() < 3); // Stopped due to dispersion constraint
     }
 
     #[test]
     fn test_diversity_stats() {
         let dreams = vec![
             make_dream([1.0, 0.0, 0.0]),
             make_dream([0.0, 1.0, 0.0]),
             make_dream([0.0, 0.0, 1.0]),
         ];
 
         let stats = DiversityStats::compute(&dreams);
diff --git a/src/dream/embedding.rs b/src/dream/embedding.rs
index 0bc0a275a901b54f4210d336cd64d1967ac0f333..2765f17e51ef288c448f8583421b22ed54ce240b 100644
--- a/src/dream/embedding.rs
+++ b/src/dream/embedding.rs
@@ -1,411 +1,379 @@
 //! Continuous embedding mapper for semantic dream retrieval.
 //!
-//! This module implements Phase 4: Continuous Embedding / Soft Indexing.
-//! Instead of hard class-based retrieval, dreams are mapped to a continuous
-//! latent space and retrieved by semantic similarity (cosine/euclidean).
-//!
-//! **Key Concept:** Fuse multiple features into a fixed-dimension embedding:
-//! - Chromatic signature (RGB): 3 dims
-//! - Spectral features (entropy, band energies): ~6 dims
-//! - Class one-hot (optional): 10 dims
-//! - Utility priors: 1-2 dims
-//!
-//! Output: Fixed D-dimensional vector (default D=64) for ANN retrieval.
+//! Phase 4 introduces a continuous latent index for the dream pool. Instead of
+//! relying solely on class labels or RGB cosine heuristics, we fuse multiple
+//! signals into a fixed-width embedding that can be searched using cosine or
+//! euclidean distance. The mapping is deterministic and does not depend on
+//! learned weights so that tests remain reproducible.
 
 use crate::data::ColorClass;
 use crate::dream::bias::BiasProfile;
 use crate::dream::simple_pool::DreamEntry;
 use crate::spectral::SpectralFeatures;
+use serde::Serialize;
 
 /// Query signature for embedding-based retrieval.
-#[derive(Debug, Clone)]
+#[derive(Debug, Clone, Serialize)]
 pub struct QuerySignature {
     /// Chromatic RGB signature
     pub chroma: [f32; 3],
-
     /// Optional class hint for class-conditional retrieval
     pub class_hint: Option<ColorClass>,
-
-    /// Optional spectral features
+    /// Optional spectral features for richer queries
     pub spectral: Option<SpectralFeatures>,
-
-    /// Optional utility prior (e.g., mean utility from feedback)
+    /// Optional utility prior (e.g., expected helpfulness)
     pub utility_prior: Option<f32>,
+    /// Optional ΔLoss hint if available from the learner
+    pub delta_loss_hint: Option<f32>,
 }
 
 impl QuerySignature {
     /// Create a simple query from RGB signature.
     pub fn from_chroma(chroma: [f32; 3]) -> Self {
         Self {
             chroma,
             class_hint: None,
             spectral: None,
             utility_prior: None,
+            delta_loss_hint: None,
         }
     }
 
     /// Create query with class hint.
     pub fn with_class(chroma: [f32; 3], class: ColorClass) -> Self {
         Self {
             chroma,
             class_hint: Some(class),
             spectral: None,
             utility_prior: None,
+            delta_loss_hint: None,
         }
     }
 }
 
 /// Embedding mapper that fuses multiple features into fixed-dimension vectors.
 ///
-/// **Architecture:** Linear projection with layer normalization
+/// Architecture: deterministic linear projection followed by layer
+/// normalisation.
+///
 /// ```text
-/// features = [rgb(3), spectral(6), class_onehot(10), utility(1)]
+/// features = [rgb(3), spectral(~9), class_onehot(<=10), util_stats(3)]
 /// z = LayerNorm(features · W + b)
 /// ```
-///
-/// Currently uses a simple deterministic linear mapping (no learned weights).
+#[derive(Debug, Clone, Serialize)]
 pub struct EmbeddingMapper {
     /// Output embedding dimension
-    pub dim: usize,
-
-    /// Whether to include class one-hot encoding
-    pub include_class: bool,
-
-    /// Whether to include spectral features
-    pub include_spectral: bool,
-
-    /// Whether to include utility features
-    pub include_utility: bool,
+    pub d: usize,
+    include_class: bool,
+    include_spectral: bool,
+    include_utility: bool,
 }
 
 impl EmbeddingMapper {
     /// Create a new embedding mapper.
-    ///
-    /// # Arguments
-    /// * `dim` - Output embedding dimension (e.g., 64)
-    ///
-    /// # Returns
-    /// * EmbeddingMapper with default feature configuration
-    pub fn new(dim: usize) -> Self {
+    pub fn new(d: usize) -> Self {
         Self {
-            dim,
+            d,
             include_class: true,
             include_spectral: true,
             include_utility: true,
         }
     }
 
     /// Encode a dream entry into an embedding vector.
-    ///
-    /// # Arguments
-    /// * `entry` - Dream entry to encode
-    /// * `bias` - Optional bias profile for conditioning
-    ///
-    /// # Returns
-    /// * D-dimensional embedding vector
     pub fn encode_entry(&self, entry: &DreamEntry, bias: Option<&BiasProfile>) -> Vec<f32> {
         let mut features = Vec::new();
-
-        // 1. Chromatic signature (3 dims)
         features.extend_from_slice(&entry.chroma_signature);
 
-        // 2. Spectral features (6 dims)
         if self.include_spectral {
-            if let Some(ref spectral) = entry.spectral_features {
-                features.push(spectral.entropy);
-                features.push(spectral.low_freq_energy);
-                features.push(spectral.mid_freq_energy);
-                features.push(spectral.high_freq_energy);
-                features.push(spectral.mean_psd);
-
-                // Dominant frequency (averaged across RGB)
-                let dom_freq_mean = (spectral.dominant_frequencies[0] as f32
-                    + spectral.dominant_frequencies[1] as f32
-                    + spectral.dominant_frequencies[2] as f32) / 3.0;
-                features.push(dom_freq_mean);
-            } else {
-                // Missing spectral features - use zeros
-                features.extend_from_slice(&[0.0; 6]);
-            }
+            features.extend(gather_spectral_features(entry.spectral_features.as_ref()));
         }
 
-        // 3. Class one-hot (10 dims)
         if self.include_class {
-            if let Some(class) = entry.class_label {
-                let onehot = self.class_to_onehot(class);
-                features.extend_from_slice(&onehot);
-            } else {
-                // Unknown class - use uniform distribution
-                features.extend_from_slice(&[0.1; 10]);
-            }
+            features.extend(self.encode_class(entry.class_label));
         }
 
-        // 4. Utility features (2 dims: raw utility + bias weight)
         if self.include_utility {
-            let utility = entry.utility.unwrap_or(0.0);
-            features.push(utility);
-
-            // Bias weight if available
-            if let Some(bias_profile) = bias {
-                if let Some(class) = entry.class_label {
-                    let weight = bias_profile.class_weight(class);
-                    features.push(weight);
-                } else {
-                    features.push(0.5); // Neutral weight
-                }
-            } else {
-                features.push(0.5);
-            }
+            features.extend(self.encode_utility_features(
+                entry.utility,
+                Some(entry.util_mean),
+                None,
+                entry.feedback_count,
+                bias,
+                entry.class_label,
+            ));
         }
 
-        // 5. Project to target dimension and normalize
-        self.project_and_normalize(features)
+        deterministic_project(self.d, &features)
     }
 
     /// Encode a query signature into an embedding vector.
-    ///
-    /// # Arguments
-    /// * `query` - Query signature
-    /// * `bias` - Optional bias profile for conditioning
-    ///
-    /// # Returns
-    /// * D-dimensional embedding vector
     pub fn encode_query(&self, query: &QuerySignature, bias: Option<&BiasProfile>) -> Vec<f32> {
         let mut features = Vec::new();
-
-        // 1. Chromatic signature (3 dims)
         features.extend_from_slice(&query.chroma);
 
-        // 2. Spectral features (6 dims)
         if self.include_spectral {
-            if let Some(ref spectral) = query.spectral {
-                features.push(spectral.entropy);
-                features.push(spectral.low_freq_energy);
-                features.push(spectral.mid_freq_energy);
-                features.push(spectral.high_freq_energy);
-                features.push(spectral.mean_psd);
-
-                let dom_freq_mean = (spectral.dominant_frequencies[0] as f32
-                    + spectral.dominant_frequencies[1] as f32
-                    + spectral.dominant_frequencies[2] as f32) / 3.0;
-                features.push(dom_freq_mean);
-            } else {
-                features.extend_from_slice(&[0.0; 6]);
-            }
+            features.extend(gather_spectral_features(query.spectral.as_ref()));
         }
 
-        // 3. Class one-hot (10 dims)
         if self.include_class {
-            if let Some(class) = query.class_hint {
-                let onehot = self.class_to_onehot(class);
-                features.extend_from_slice(&onehot);
-            } else {
-                // No class hint - use uniform
-                features.extend_from_slice(&[0.1; 10]);
-            }
+            features.extend(self.encode_class(query.class_hint));
         }
 
-        // 4. Utility features (2 dims)
         if self.include_utility {
-            let utility_prior = query.utility_prior.unwrap_or(0.0);
-            features.push(utility_prior);
-
-            // Bias weight
-            if let Some(bias_profile) = bias {
-                if let Some(class) = query.class_hint {
-                    let weight = bias_profile.class_weight(class);
-                    features.push(weight);
-                } else {
-                    features.push(0.5);
-                }
-            } else {
-                features.push(0.5);
-            }
+            features.extend(self.encode_utility_features(
+                query.utility_prior,
+                query.utility_prior,
+                query.delta_loss_hint,
+                0,
+                bias,
+                query.class_hint,
+            ));
         }
 
-        self.project_and_normalize(features)
+        deterministic_project(self.d, &features)
     }
 
-    /// Convert ColorClass to one-hot encoding.
-    fn class_to_onehot(&self, class: ColorClass) -> [f32; 10] {
+    fn encode_class(&self, class: Option<ColorClass>) -> [f32; 10] {
         let mut onehot = [0.0; 10];
-        let idx = class as usize;
-        onehot[idx] = 1.0;
-        onehot
-    }
-
-    /// Project features to target dimension and apply layer normalization.
-    ///
-    /// Uses a simple deterministic projection (no learned weights).
-    fn project_and_normalize(&self, features: Vec<f32>) -> Vec<f32> {
-        let feature_dim = features.len();
-
-        // Simple linear projection: repeat/truncate to target dim
-        let mut projected = Vec::with_capacity(self.dim);
-
-        if self.dim <= feature_dim {
-            // Truncate
-            projected.extend_from_slice(&features[..self.dim]);
+        if let Some(class) = class {
+            onehot[class as usize] = 1.0;
         } else {
-            // Repeat features cyclically to fill dimension
-            for i in 0..self.dim {
-                projected.push(features[i % feature_dim]);
-            }
+            // Neutral prior when class unknown
+            onehot.fill(0.1);
         }
+        onehot
+    }
 
-        // Layer normalization: (x - mean) / std
-        let mean = projected.iter().sum::<f32>() / projected.len() as f32;
-        let variance = projected.iter().map(|&x| (x - mean).powi(2)).sum::<f32>()
-            / projected.len() as f32;
-        let std = (variance + 1e-8).sqrt(); // Add epsilon for numerical stability
-
-        projected.iter().map(|&x| (x - mean) / std).collect()
+    fn encode_utility_features(
+        &self,
+        utility: Option<f32>,
+        mean_utility: Option<f32>,
+        delta_loss: Option<f32>,
+        feedback_count: usize,
+        bias: Option<&BiasProfile>,
+        class: Option<ColorClass>,
+    ) -> [f32; 5] {
+        let util = utility.unwrap_or(0.0);
+        let mean = mean_utility.unwrap_or(util);
+        let delta = delta_loss.unwrap_or(0.0);
+        let count = feedback_count as f32;
+        let bias_weight = class
+            .and_then(|c| bias.map(|b| b.class_weight(c)))
+            .unwrap_or(0.5);
+        [util, mean, delta, count.tanh(), bias_weight]
     }
 
     /// Get expected feature dimension (before projection).
     pub fn feature_dim(&self) -> usize {
         let mut dim = 3; // RGB
 
         if self.include_spectral {
-            dim += 6; // entropy + 3 band energies + mean_psd + dom_freq
+            dim += 9; // entropy + bands + mean_psd + dominant freq one-hot summary
         }
 
         if self.include_class {
             dim += 10; // one-hot
         }
 
         if self.include_utility {
-            dim += 2; // utility + bias_weight
+            dim += 5; // utility statistics
         }
 
         dim
     }
 }
 
+fn gather_spectral_features(features: Option<&SpectralFeatures>) -> [f32; 9] {
+    match features {
+        Some(f) => {
+            let mut out = [0.0; 9];
+            out[0] = f.entropy;
+            out[1] = f.low_freq_energy;
+            out[2] = f.mid_freq_energy;
+            out[3] = f.high_freq_energy;
+            out[4] = f.mean_psd;
+            // Normalise dominant frequency bins into [0,1]
+            let max_freq = f.dominant_frequencies.iter().copied().max().unwrap_or(1) as f32;
+            let denom = max_freq.max(1.0);
+            out[5] = f.dominant_frequencies[0] as f32 / denom;
+            out[6] = f.dominant_frequencies[1] as f32 / denom;
+            out[7] = f.dominant_frequencies[2] as f32 / denom;
+            out[8] = (f.dominant_frequencies[0]
+                + f.dominant_frequencies[1]
+                + f.dominant_frequencies[2]) as f32
+                / (3.0 * denom);
+            out
+        }
+        None => [0.0; 9],
+    }
+}
+
+fn deterministic_project(dim: usize, features: &[f32]) -> Vec<f32> {
+    if features.is_empty() {
+        return vec![0.0; dim];
+    }
+
+    let mut projected = Vec::with_capacity(dim);
+    for row in 0..dim {
+        let mut acc = 0.0;
+        for (col, &value) in features.iter().enumerate() {
+            let weight = hashed_weight(row, col);
+            acc += value * weight;
+        }
+        let bias = hashed_bias(row);
+        projected.push(acc + bias);
+    }
+
+    layer_norm(&projected)
+}
+
+fn hashed_weight(row: usize, col: usize) -> f32 {
+    let seed = ((row as u64) << 32) ^ (col as u64);
+    hash_to_unit(seed) * 2.0 - 1.0
+}
+
+fn hashed_bias(row: usize) -> f32 {
+    let seed = 0x9E37_79B9_7F4A_7C15u64 ^ row as u64;
+    hash_to_unit(seed) * 0.5 - 0.25
+}
+
+fn hash_to_unit(mut x: u64) -> f32 {
+    // SplitMix64 adapted to produce deterministic pseudo-random floats
+    x = x.wrapping_add(0x9E37_79B9_7F4A_7C15);
+    let mut z = x;
+    z = (z ^ (z >> 30)).wrapping_mul(0xBF58_476D_1CE4_E5B9);
+    z = (z ^ (z >> 27)).wrapping_mul(0x94D0_49BB_1331_11EB);
+    let z = z ^ (z >> 31);
+    let mantissa = ((z >> 11) as u32) & 0x007F_FFFF; // keep 23 bits
+    let bits = mantissa | 0x3F80_0000; // exponent 127 -> [1,2)
+    f32::from_bits(bits) - 1.0
+}
+
+fn layer_norm(values: &[f32]) -> Vec<f32> {
+    if values.is_empty() {
+        return Vec::new();
+    }
+
+    let mean = values.iter().sum::<f32>() / values.len() as f32;
+    let variance = values
+        .iter()
+        .map(|v| {
+            let diff = *v - mean;
+            diff * diff
+        })
+        .sum::<f32>()
+        / values.len() as f32;
+    let std = (variance + 1e-6).sqrt();
+
+    if std <= 0.0 {
+        return vec![0.0; values.len()];
+    }
+
+    values.iter().map(|v| (*v - mean) / std).collect()
+}
+
 #[cfg(test)]
 mod tests {
     use super::*;
+    use crate::data::ColorClass;
+    use crate::dream::bias::{BiasProfile, ChromaBias, ClassBias, ProfileMetadata, SpectralBias};
     use crate::solver::SolverResult;
+    use crate::spectral::SpectralFeatures;
     use crate::tensor::ChromaticTensor;
     use serde_json::json;
+    use std::collections::HashMap;
+
+    fn make_spectral() -> SpectralFeatures {
+        SpectralFeatures {
+            entropy: 0.42,
+            dominant_frequencies: [3, 5, 7],
+            low_freq_energy: 0.3,
+            mid_freq_energy: 0.2,
+            high_freq_energy: 0.1,
+            mean_psd: 0.25,
+        }
+    }
 
-    fn make_dream_entry() -> DreamEntry {
+    fn make_entry() -> DreamEntry {
         let tensor = ChromaticTensor::new(8, 8, 4);
         let result = SolverResult {
             energy: 0.1,
             coherence: 0.8,
-            violation: 0.0,
+            violation: 0.05,
             grad: None,
             mask: None,
             meta: json!({}),
         };
 
         let mut entry = DreamEntry::new(tensor, result);
-        entry.chroma_signature = [1.0, 0.5, 0.2];
+        entry.chroma_signature = [0.2, 0.5, 0.7];
         entry.class_label = Some(ColorClass::Red);
-        entry.utility = Some(0.3);
+        entry.utility = Some(0.4);
+        entry.util_mean = 0.35;
+        entry.feedback_count = 3;
+        entry.spectral_features = Some(make_spectral());
         entry
     }
 
     #[test]
-    fn test_embedding_mapper_creation() {
-        let mapper = EmbeddingMapper::new(64);
-        assert_eq!(mapper.dim, 64);
-        assert!(mapper.include_class);
-        assert!(mapper.include_spectral);
-        assert!(mapper.include_utility);
-    }
-
-    #[test]
-    fn test_feature_dim() {
-        let mapper = EmbeddingMapper::new(64);
-        // RGB(3) + spectral(6) + class(10) + utility(2) = 21
-        assert_eq!(mapper.feature_dim(), 21);
-    }
-
-    #[test]
-    fn test_encode_entry_shape() {
-        let mapper = EmbeddingMapper::new(64);
-        let entry = make_dream_entry();
-
-        let embedding = mapper.encode_entry(&entry, None);
-        assert_eq!(embedding.len(), 64);
-    }
-
-    #[test]
-    fn test_encode_entry_deterministic() {
-        let mapper = EmbeddingMapper::new(64);
-        let entry = make_dream_entry();
-
-        let embed1 = mapper.encode_entry(&entry, None);
-        let embed2 = mapper.encode_entry(&entry, None);
-
-        assert_eq!(embed1, embed2);
-    }
-
-    #[test]
-    fn test_encode_query_shape() {
-        let mapper = EmbeddingMapper::new(64);
-        let query = QuerySignature::from_chroma([1.0, 0.0, 0.0]);
-
-        let embedding = mapper.encode_query(&query, None);
-        assert_eq!(embedding.len(), 64);
-    }
-
-    #[test]
-    fn test_class_to_onehot() {
+    fn embedding_deterministic() {
         let mapper = EmbeddingMapper::new(64);
-        let onehot = mapper.class_to_onehot(ColorClass::Red);
-
-        assert_eq!(onehot[0], 1.0); // Red is index 0
-        assert_eq!(onehot[1], 0.0);
-        assert_eq!(onehot.iter().sum::<f32>(), 1.0);
-    }
-
-    #[test]
-    fn test_layer_normalization() {
-        let mapper = EmbeddingMapper::new(64);
-        let entry = make_dream_entry();
-
-        let embedding = mapper.encode_entry(&entry, None);
-
-        // Check layer norm properties: mean ≈ 0, std ≈ 1
-        let mean = embedding.iter().sum::<f32>() / embedding.len() as f32;
-        let variance = embedding.iter().map(|&x| (x - mean).powi(2)).sum::<f32>()
-            / embedding.len() as f32;
-        let std = variance.sqrt();
-
-        assert!(mean.abs() < 0.01); // Mean should be ~0
-        assert!((std - 1.0).abs() < 0.01); // Std should be ~1
+        let entry = make_entry();
+        let embed_a = mapper.encode_entry(&entry, None);
+        let embed_b = mapper.encode_entry(&entry, None);
+        assert_eq!(embed_a, embed_b);
+        assert_eq!(embed_a.len(), 64);
     }
 
     #[test]
-    fn test_different_classes_different_embeddings() {
-        let mapper = EmbeddingMapper::new(64);
-
-        let mut entry1 = make_dream_entry();
-        entry1.class_label = Some(ColorClass::Red);
-
-        let mut entry2 = make_dream_entry();
-        entry2.class_label = Some(ColorClass::Blue);
-
-        let embed1 = mapper.encode_entry(&entry1, None);
-        let embed2 = mapper.encode_entry(&entry2, None);
-
-        assert_ne!(embed1, embed2);
+    fn query_embedding_respects_bias() {
+        let mapper = EmbeddingMapper::new(32);
+        let spectral = make_spectral();
+        let query = QuerySignature {
+            chroma: [0.1, 0.2, 0.3],
+            class_hint: Some(ColorClass::Blue),
+            spectral: Some(spectral.clone()),
+            utility_prior: Some(0.5),
+            delta_loss_hint: Some(-0.2),
+        };
+        let mut class_biases = HashMap::new();
+        class_biases.insert(
+            "Blue".to_string(),
+            ClassBias {
+                mean_utility: 0.4,
+                sample_count: 2,
+                prefer: true,
+                weight: 0.9,
+            },
+        );
+        let profile = BiasProfile {
+            class_biases,
+            spectral_bias: SpectralBias {
+                entropy_range: None,
+                entropy_utility_correlation: None,
+                low_freq_threshold: None,
+                high_freq_threshold: None,
+            },
+            chroma_bias: ChromaBias {
+                red_range: None,
+                green_range: None,
+                blue_range: None,
+            },
+            metadata: ProfileMetadata {
+                total_samples: 2,
+                mean_utility: 0.2,
+                timestamp: 0,
+                utility_threshold: 0.0,
+            },
+        };
+        let embed = mapper.encode_query(&query, Some(&profile));
+        assert_eq!(embed.len(), 32);
     }
 
     #[test]
-    fn test_query_with_class_hint() {
-        let mapper = EmbeddingMapper::new(64);
-
-        let query1 = QuerySignature::from_chroma([1.0, 0.0, 0.0]);
-        let query2 = QuerySignature::with_class([1.0, 0.0, 0.0], ColorClass::Red);
-
-        let embed1 = mapper.encode_query(&query1, None);
-        let embed2 = mapper.encode_query(&query2, None);
-
-        // Different due to class hint
-        assert_ne!(embed1, embed2);
+    fn feature_dim_matches_components() {
+        let mapper = EmbeddingMapper::new(16);
+        assert_eq!(mapper.feature_dim(), 27);
     }
 }
diff --git a/src/dream/experiment.rs b/src/dream/experiment.rs
index 05d22aeac4f8b7a29990d7634d30300edb62a857..7547ca686bda90bc5eb42e2cbc57d501bf3104be 100644
--- a/src/dream/experiment.rs
+++ b/src/dream/experiment.rs
@@ -1,36 +1,36 @@
 //! A/B testing harness for Dream Pool validation experiments
 //!
 //! This module implements the validation experiment specified in
 //! "Validation Experiment Specification: Retrieval Hypothesis"
 
 use crate::data::{ColorDataset, ColorSample, DatasetConfig};
-use crate::dream::SimpleDreamPool;
 use crate::dream::simple_pool::{PoolConfig, PoolStats};
+use crate::dream::SimpleDreamPool;
 use crate::solver::Solver;
-use crate::tensor::{ChromaticTensor, operations::mix};
-use serde::{Serialize, Deserialize};
+use crate::tensor::{operations::mix, ChromaticTensor};
+use serde::{Deserialize, Serialize};
 use std::time::Instant;
 
 /// Seeding strategy for the experiment
 #[derive(Debug, Clone, Copy, PartialEq, Eq)]
 pub enum SeedingStrategy {
     /// Group A: Random noise or zero tensor (control)
     RandomNoise,
     /// Group B: Retrieval-based seeding from dream pool (test)
     RetrievalBased,
 }
 
 /// Configuration for a single experimental run
 #[derive(Debug, Clone)]
 pub struct ExperimentConfig {
     /// Seeding strategy to use
     pub strategy: SeedingStrategy,
     /// Number of training epochs
     pub num_epochs: usize,
     /// Batch size for training
     pub batch_size: usize,
     /// Number of solver iterations per dream cycle
     pub dream_iterations: usize,
     /// Pool configuration (only used for RetrievalBased strategy)
     pub pool_config: PoolConfig,
     /// Dataset configuration
@@ -115,87 +115,91 @@ impl<S: Solver> ExperimentHarness<S> {
 
         // Generate dataset
         let dataset = ColorDataset::generate(self.config.dataset_config.clone());
         let (train_samples, val_samples) = dataset.split(0.8);
 
         let mut step_metrics = Vec::new();
         let mut epoch_metrics = Vec::new();
 
         for epoch in 0..self.config.num_epochs {
             let epoch_start = Instant::now();
             let epoch_stats = self.run_epoch(epoch, &train_samples, &mut step_metrics);
 
             // Compute validation accuracy
             let val_accuracy = self.validate(&val_samples);
 
             epoch_metrics.push(EpochMetrics {
                 epoch,
                 mean_energy: epoch_stats.0,
                 mean_coherence: epoch_stats.1,
                 mean_violation: epoch_stats.2,
                 validation_accuracy: val_accuracy,
                 elapsed_ms: epoch_start.elapsed().as_millis(),
             });
         }
 
-        let final_accuracy = epoch_metrics.last().map(|m| m.validation_accuracy).unwrap_or(0.0);
+        let final_accuracy = epoch_metrics
+            .last()
+            .map(|m| m.validation_accuracy)
+            .unwrap_or(0.0);
         let convergence_epoch = self.find_convergence_epoch(&epoch_metrics, final_accuracy);
 
         ExperimentResult {
             strategy: format!("{:?}", self.config.strategy),
             step_metrics,
             epoch_metrics,
             final_accuracy,
             convergence_epoch,
             total_elapsed_ms: start_time.elapsed().as_millis(),
         }
     }
 
     /// Run a single training epoch
     fn run_epoch(
         &mut self,
         epoch: usize,
         train_samples: &[ColorSample],
         step_metrics: &mut Vec<StepMetrics>,
     ) -> (f64, f64, f64) {
         let mut sum_energy = 0.0;
         let mut sum_coherence = 0.0;
         let mut sum_violation = 0.0;
         let mut step_count = 0;
 
         for (batch_idx, sample) in train_samples.iter().enumerate() {
             let step_start = Instant::now();
 
             // Generate seed tensor based on strategy
             let seed_tensor = self.generate_seed_tensor(&sample.tensor);
 
             // Run dream cycle (multiple solver iterations)
             let mut current_tensor = seed_tensor;
             let mut final_result = None;
 
             for _ in 0..self.config.dream_iterations {
-                let result = self.solver
+                let result = self
+                    .solver
                     .evaluate(&current_tensor, false)
                     .expect("Solver evaluation failed");
 
                 final_result = Some(result.clone());
 
                 // Simple update: mix with input (simulation of gradient descent)
                 current_tensor = mix(&current_tensor, &sample.tensor);
             }
 
             let result = final_result.expect("No solver result");
 
             // Store in pool if using retrieval strategy
             if let Some(ref mut pool) = self.pool {
                 pool.add_if_coherent(current_tensor.clone(), result.clone());
             }
 
             // Record metrics
             sum_energy += result.energy;
             sum_coherence += result.coherence;
             sum_violation += result.violation;
             step_count += 1;
 
             step_metrics.push(StepMetrics {
                 epoch,
                 step: batch_idx,
@@ -251,62 +255,67 @@ impl<S: Solver> ExperimentHarness<S> {
                             layers,
                         );
 
                         for entry in similar {
                             seed = mix(&seed, &entry.tensor);
                         }
 
                         seed
                     }
                 } else {
                     panic!("Pool not initialized for retrieval strategy");
                 }
             }
         }
     }
 
     /// Validate on validation set (dummy implementation - returns coherence as proxy for accuracy)
     fn validate(&mut self, val_samples: &[ColorSample]) -> f64 {
         if val_samples.is_empty() {
             return 0.0;
         }
 
         let mut total_coherence = 0.0;
 
         for sample in val_samples.iter().take(50) {
-            let result = self.solver
+            let result = self
+                .solver
                 .evaluate(&sample.tensor, false)
                 .expect("Validation evaluation failed");
             total_coherence += result.coherence;
         }
 
         let sample_count = val_samples.len().min(50);
         total_coherence / sample_count as f64
     }
 
     /// Find the epoch where convergence occurred (90% of final accuracy)
-    fn find_convergence_epoch(&self, epoch_metrics: &[EpochMetrics], final_accuracy: f64) -> Option<usize> {
+    fn find_convergence_epoch(
+        &self,
+        epoch_metrics: &[EpochMetrics],
+        final_accuracy: f64,
+    ) -> Option<usize> {
         let target = final_accuracy * 0.9;
 
         for metrics in epoch_metrics {
             if metrics.validation_accuracy >= target {
                 return Some(metrics.epoch);
             }
         }
 
         None
     }
 
     /// Get pool statistics (if using retrieval strategy)
     pub fn pool_stats(&self) -> Option<PoolStats> {
         self.pool.as_ref().map(|p| p.stats())
     }
 }
 
 #[cfg(test)]
 mod tests {
     use super::*;
     use crate::ChromaticNativeSolver;
 
     #[test]
     fn test_experiment_control_group() {
         let config = ExperimentConfig {
diff --git a/src/dream/hybrid_scoring.rs b/src/dream/hybrid_scoring.rs
index e763928c896882ebd9c692fd793fdafd7a1c86ad..3e8cd3dd8771931232637e10a781439bd58a2553 100644
--- a/src/dream/hybrid_scoring.rs
+++ b/src/dream/hybrid_scoring.rs
@@ -1,79 +1,86 @@
 //! Hybrid scoring combining similarity, utility, class matching, and diversity.
 //!
 //! Implements multi-objective retrieval scoring for Phase 4:
 //! - α·similarity (from SoftIndex)
 //! - β·utility (from ΔLoss tracking)
 //! - γ·class_match (optional class conditioning)
 //! - δ·MMR_penalty (diversity enforcement)
 
+use crate::data::ColorClass;
 use crate::dream::simple_pool::DreamEntry;
 use crate::dream::soft_index::EntryId;
-use crate::data::ColorClass;
+use serde::{Deserialize, Serialize};
 use std::collections::HashMap;
 
 /// Weights for hybrid retrieval scoring
-#[derive(Debug, Clone, Copy)]
+#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
 pub struct RetrievalWeights {
     /// Similarity weight (default: 0.65)
     pub alpha: f32,
 
     /// Utility weight (default: 0.20)
     pub beta: f32,
 
     /// Class match weight (default: 0.10)
     pub gamma: f32,
 
     /// Duplicate penalty weight (default: 0.05)
     pub delta: f32,
 
     /// MMR lambda for diversity (default: 0.7, higher = more diversity)
     pub lambda: f32,
 }
 
 impl Default for RetrievalWeights {
     fn default() -> Self {
         Self {
             alpha: 0.65,
             beta: 0.20,
             gamma: 0.10,
             delta: 0.05,
             lambda: 0.7,
         }
     }
 }
 
 impl RetrievalWeights {
     /// Create new weights with validation
     pub fn new(alpha: f32, beta: f32, gamma: f32, delta: f32, lambda: f32) -> Self {
         assert!(alpha >= 0.0 && alpha <= 1.0, "alpha must be in [0, 1]");
         assert!(beta >= 0.0 && beta <= 1.0, "beta must be in [0, 1]");
         assert!(gamma >= 0.0 && gamma <= 1.0, "gamma must be in [0, 1]");
         assert!(delta >= 0.0 && delta <= 1.0, "delta must be in [0, 1]");
         assert!(lambda >= 0.0 && lambda <= 1.0, "lambda must be in [0, 1]");
 
-        Self { alpha, beta, gamma, delta, lambda }
+        Self {
+            alpha,
+            beta,
+            gamma,
+            delta,
+            lambda,
+        }
     }
 
     /// Normalize weights so alpha + beta + gamma + delta = 1.0
     pub fn normalize(&mut self) {
         let sum = self.alpha + self.beta + self.gamma + self.delta;
         if sum > 0.0 {
             self.alpha /= sum;
             self.beta /= sum;
             self.gamma /= sum;
             self.delta /= sum;
         }
     }
 }
 
 /// Rerank initial SoftIndex hits using hybrid scoring
 ///
 /// # Arguments
 /// * `hits` - Initial k-NN results from SoftIndex (id, similarity_score)
 /// * `weights` - Hybrid scoring weights
 /// * `entries` - Map from EntryId to DreamEntry for utility/class lookup
 /// * `query_class` - Optional class hint for class matching bonus
 ///
 /// # Returns
 /// * Vec<(EntryId, f32)> sorted by hybrid score (descending)
 pub fn rerank_hybrid(
@@ -92,106 +99,111 @@ pub fn rerank_hybrid(
     let sim_range = sim_max - sim_min;
 
     // Compute base hybrid scores (without diversity penalty)
     let mut scored: Vec<(EntryId, f32, f32)> = hits
         .iter()
         .filter_map(|(id, sim)| {
             entries.get(id).map(|entry| {
                 // Normalize similarity
                 let norm_sim = if sim_range > 1e-6 {
                     (sim - sim_min) / sim_range
                 } else {
                     1.0
                 };
 
                 // Utility score (normalized to [0, 1])
                 let utility = entry.util_mean.clamp(0.0, 1.0);
 
                 // Class match bonus
                 let class_match = match (query_class, entry.class_label) {
                     (Some(q), Some(e)) if q == e => 1.0,
                     (Some(_), Some(_)) => 0.0,
                     _ => 0.5, // Neutral if no class info
                 };
 
                 // Base score (before diversity penalty)
-                let base_score = weights.alpha * norm_sim
-                    + weights.beta * utility
-                    + weights.gamma * class_match;
+                let base_score =
+                    weights.alpha * norm_sim + weights.beta * utility + weights.gamma * class_match;
 
                 (*id, base_score, norm_sim)
             })
         })
         .collect();
 
     // Sort by base score descending
     scored.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));
 
     // Apply MMR diversity penalty
     if weights.lambda > 0.0 {
         scored = apply_mmr_penalty(scored, weights.lambda, weights.delta, entries);
     }
 
     // Return final scores
-    scored.into_iter().map(|(id, score, _)| (id, score)).collect()
+    scored
+        .into_iter()
+        .map(|(id, score, _)| (id, score))
+        .collect()
 }
 
 /// Apply Maximum Marginal Relevance (MMR) penalty for diversity
 ///
 /// Iteratively selects entries, penalizing those too similar to already-selected ones.
 fn apply_mmr_penalty(
     candidates: Vec<(EntryId, f32, f32)>,
     lambda: f32,
     delta: f32,
     entries: &HashMap<EntryId, DreamEntry>,
 ) -> Vec<(EntryId, f32, f32)> {
     if candidates.len() <= 1 {
         return candidates;
     }
 
     let mut selected: Vec<(EntryId, f32, f32)> = Vec::new();
     let mut remaining = candidates;
 
     // Select first entry (highest base score)
     if let Some(first) = remaining.first() {
         selected.push(*first);
         remaining.remove(0);
     }
 
     // Iteratively select remaining entries with MMR penalty
     while !remaining.is_empty() {
         let mut best_idx = 0;
         let mut best_mmr_score = f32::NEG_INFINITY;
 
         for (idx, (id, base_score, _)) in remaining.iter().enumerate() {
             // Compute max similarity to already-selected entries
             let max_sim = selected
                 .iter()
                 .filter_map(|(sel_id, _, _)| {
                     let entry = entries.get(id)?;
                     let sel_entry = entries.get(sel_id)?;
-                    Some(chroma_similarity(&entry.chroma_signature, &sel_entry.chroma_signature))
+                    Some(chroma_similarity(
+                        &entry.chroma_signature,
+                        &sel_entry.chroma_signature,
+                    ))
                 })
                 .fold(0.0f32, |a, b| a.max(b));
 
             // MMR score: balance relevance and diversity
             // Higher lambda = more diversity, lower lambda = more relevance
             let mmr_score = lambda * base_score - delta * max_sim;
 
             if mmr_score > best_mmr_score {
                 best_mmr_score = mmr_score;
                 best_idx = idx;
             }
         }
 
         // Move best to selected
         let best = remaining.remove(best_idx);
         selected.push((best.0, best_mmr_score, best.2));
     }
 
     selected
 }
 
 /// Compute cosine similarity between two chromatic signatures
 #[inline]
 fn chroma_similarity(a: &[f32; 3], b: &[f32; 3]) -> f32 {
     let dot: f32 = a.iter().zip(b.iter()).map(|(x, y)| x * y).sum();
@@ -203,209 +215,254 @@ fn chroma_similarity(a: &[f32; 3], b: &[f32; 3]) -> f32 {
     }
 
     dot / (norm_a * norm_b)
 }
 
 /// Find min and max values in a slice
 fn min_max(values: &[f32]) -> (f32, f32) {
     let mut min = f32::INFINITY;
     let mut max = f32::NEG_INFINITY;
 
     for &v in values {
         if v < min {
             min = v;
         }
         if v > max {
             max = v;
         }
     }
 
     (min, max)
 }
 
 #[cfg(test)]
 mod tests {
     use super::*;
-    use crate::tensor::ChromaticTensor;
     use crate::solver::SolverResult;
+    use crate::tensor::ChromaticTensor;
 
-    fn mock_entry(id: EntryId, chroma: [f32; 3], utility: f32, class: Option<ColorClass>) -> (EntryId, DreamEntry) {
+    fn mock_entry(
+        id: EntryId,
+        chroma: [f32; 3],
+        utility: f32,
+        class: Option<ColorClass>,
+    ) -> (EntryId, DreamEntry) {
         let tensor = ChromaticTensor::new(4, 4, 3);
         let result = SolverResult {
             energy: 0.1,
             coherence: 0.8,
             violation: 0.05,
             grad: None,
             mask: None,
             meta: serde_json::json!({}),
         };
 
-        (id, DreamEntry {
-            tensor,
-            result,
-            chroma_signature: chroma,
-            class_label: class,
-            utility: Some(utility),
-            timestamp: std::time::SystemTime::now(),
-            usage_count: 0,
-            spectral_features: None,
-            embed: None,
-            util_mean: utility,
-        })
+        let mut entry = DreamEntry::new(tensor, result);
+        entry.id = id;
+        entry.chroma_signature = chroma;
+        entry.class_label = class;
+        entry.utility = Some(utility);
+        entry.util_mean = utility;
+        (id, entry)
     }
 
     #[test]
     fn test_weights_default() {
         let w = RetrievalWeights::default();
         assert!((w.alpha - 0.65).abs() < 0.01);
         assert!((w.beta - 0.20).abs() < 0.01);
         assert!((w.gamma - 0.10).abs() < 0.01);
         assert!((w.delta - 0.05).abs() < 0.01);
         assert!((w.lambda - 0.7).abs() < 0.01);
     }
 
     #[test]
     fn test_weights_normalize() {
         let mut w = RetrievalWeights::new(0.5, 0.3, 0.1, 0.1, 0.7);
         w.normalize();
 
         let sum = w.alpha + w.beta + w.gamma + w.delta;
         assert!((sum - 1.0).abs() < 0.01);
     }
 
     #[test]
     fn test_rerank_hybrid_basic() {
         let id1 = EntryId::new_v4();
         let id2 = EntryId::new_v4();
         let id3 = EntryId::new_v4();
 
         let mut entries = HashMap::new();
-        entries.insert(id1, mock_entry(id1, [1.0, 0.0, 0.0], 0.9, Some(ColorClass::Red)).1);
-        entries.insert(id2, mock_entry(id2, [0.0, 1.0, 0.0], 0.5, Some(ColorClass::Green)).1);
-        entries.insert(id3, mock_entry(id3, [0.0, 0.0, 1.0], 0.3, Some(ColorClass::Blue)).1);
+        entries.insert(
+            id1,
+            mock_entry(id1, [1.0, 0.0, 0.0], 0.9, Some(ColorClass::Red)).1,
+        );
+        entries.insert(
+            id2,
+            mock_entry(id2, [0.0, 1.0, 0.0], 0.5, Some(ColorClass::Green)).1,
+        );
+        entries.insert(
+            id3,
+            mock_entry(id3, [0.0, 0.0, 1.0], 0.3, Some(ColorClass::Blue)).1,
+        );
 
         let hits = vec![
             (id1, 0.95), // High similarity
             (id2, 0.80), // Medium similarity
             (id3, 0.60), // Low similarity
         ];
 
         let weights = RetrievalWeights::default();
         let results = rerank_hybrid(&hits, &weights, &entries, Some(ColorClass::Red));
 
         assert_eq!(results.len(), 3);
         assert_eq!(results[0].0, id1); // Best: high sim + high utility + class match
     }
 
+    #[test]
+    fn test_class_match_boosts_score() {
+        let id1 = EntryId::new_v4();
+        let id2 = EntryId::new_v4();
+
+        let mut entries = HashMap::new();
+        entries.insert(
+            id1,
+            mock_entry(id1, [0.9, 0.1, 0.0], 0.4, Some(ColorClass::Red)).1,
+        );
+        entries.insert(
+            id2,
+            mock_entry(id2, [0.9, 0.1, 0.0], 0.4, Some(ColorClass::Blue)).1,
+        );
+
+        let hits = vec![(id1, 0.8), (id2, 0.8)];
+
+        let weights = RetrievalWeights {
+            alpha: 0.0,
+            beta: 0.0,
+            gamma: 1.0,
+            delta: 0.0,
+            lambda: 0.0,
+        };
+
+        let results = rerank_hybrid(&hits, &weights, &entries, Some(ColorClass::Red));
+        assert_eq!(results[0].0, id1);
+        assert!(results.iter().any(|(id, _)| *id == id2));
+    }
+
     #[test]
     fn test_rerank_utility_boost() {
         let id1 = EntryId::new_v4();
         let id2 = EntryId::new_v4();
 
         let mut entries = HashMap::new();
         entries.insert(id1, mock_entry(id1, [1.0, 0.0, 0.0], 0.3, None).1); // Low utility
         entries.insert(id2, mock_entry(id2, [0.9, 0.0, 0.0], 0.9, None).1); // High utility
 
         let hits = vec![
-            (id1, 1.0), // Slightly higher similarity
+            (id1, 1.0),  // Slightly higher similarity
             (id2, 0.95), // Slightly lower similarity
         ];
 
         let weights = RetrievalWeights {
             alpha: 0.3, // Lower similarity weight
             beta: 0.7,  // Much higher utility weight
             gamma: 0.0,
             delta: 0.0,
             lambda: 0.0,
         };
 
         let results = rerank_hybrid(&hits, &weights, &entries, None);
 
         // id2 should win due to much higher utility
         assert_eq!(results[0].0, id2);
     }
 
     #[test]
     fn test_rerank_class_match() {
         let id1 = EntryId::new_v4();
         let id2 = EntryId::new_v4();
 
         let mut entries = HashMap::new();
-        entries.insert(id1, mock_entry(id1, [1.0, 0.0, 0.0], 0.5, Some(ColorClass::Red)).1);
-        entries.insert(id2, mock_entry(id2, [1.0, 0.0, 0.0], 0.5, Some(ColorClass::Blue)).1);
+        entries.insert(
+            id1,
+            mock_entry(id1, [1.0, 0.0, 0.0], 0.5, Some(ColorClass::Red)).1,
+        );
+        entries.insert(
+            id2,
+            mock_entry(id2, [1.0, 0.0, 0.0], 0.5, Some(ColorClass::Blue)).1,
+        );
 
-        let hits = vec![
-            (id1, 0.9),
-            (id2, 0.9),
-        ];
+        let hits = vec![(id1, 0.9), (id2, 0.9)];
 
         let weights = RetrievalWeights {
             alpha: 0.3,
             beta: 0.3,
             gamma: 0.4, // High class match weight
             delta: 0.0,
             lambda: 0.0,
         };
 
         let results = rerank_hybrid(&hits, &weights, &entries, Some(ColorClass::Red));
 
         // id1 should win due to class match
         assert_eq!(results[0].0, id1);
     }
 
     #[test]
     fn test_mmr_diversity() {
         let id1 = EntryId::new_v4();
         let id2 = EntryId::new_v4();
         let id3 = EntryId::new_v4();
 
         let mut entries = HashMap::new();
         entries.insert(id1, mock_entry(id1, [1.0, 0.0, 0.0], 0.8, None).1);
         entries.insert(id2, mock_entry(id2, [0.98, 0.02, 0.0], 0.78, None).1); // Very similar to id1
         entries.insert(id3, mock_entry(id3, [0.0, 1.0, 0.0], 0.76, None).1); // Different
 
         let hits = vec![
             (id1, 0.95),
             (id2, 0.94), // Similar to id1, slightly lower sim
             (id3, 0.93), // Different, slightly lower sim
         ];
 
         let weights = RetrievalWeights {
             alpha: 0.5,
             beta: 0.5,
             gamma: 0.0,
             delta: 0.5,  // Higher penalty for duplicates
             lambda: 0.9, // Very high diversity preference
         };
 
         let results = rerank_hybrid(&hits, &weights, &entries, None);
 
         // With high lambda, id3 should rank higher than id2 despite similar base scores
         assert_eq!(results[0].0, id1); // Best overall
 
         // Check that id3 ranks higher than id2 due to diversity
         let id2_rank = results.iter().position(|(id, _)| *id == id2).unwrap();
         let id3_rank = results.iter().position(|(id, _)| *id == id3).unwrap();
-        assert!(id3_rank < id2_rank, "id3 should rank higher than id2 due to diversity");
+        assert!(
+            id3_rank < id2_rank,
+            "id3 should rank higher than id2 due to diversity"
+        );
     }
 
     #[test]
     fn test_chroma_similarity() {
         let a = [1.0, 0.0, 0.0];
         let b = [1.0, 0.0, 0.0];
         let sim = chroma_similarity(&a, &b);
         assert!((sim - 1.0).abs() < 0.01); // Identical
 
         let c = [0.0, 1.0, 0.0];
         let sim = chroma_similarity(&a, &c);
         assert!((sim - 0.0).abs() < 0.01); // Orthogonal
     }
 
     #[test]
     fn test_empty_hits() {
         let entries = HashMap::new();
         let hits = vec![];
         let weights = RetrievalWeights::default();
         let results = rerank_hybrid(&hits, &weights, &entries, None);
         assert!(results.is_empty());
     }
 }
diff --git a/src/dream/mod.rs b/src/dream/mod.rs
index 986c08ddab598ae1e62df6420b567b72cb5a4bcc..17696d5d82dd38be682c4f7630caabe35031ab79 100644
--- a/src/dream/mod.rs
+++ b/src/dream/mod.rs
@@ -1,25 +1,25 @@
 //! Dream Pool module for long-term semantic memory
 //!
 //! This module implements a memory system for storing and retrieving
 //! high-coherence ChromaticTensor states (dreams) to accelerate solver
 //! convergence through retrieval-based seeding.
 
 pub mod analysis;
 pub mod bias;
 pub mod diversity;
 pub mod embedding;
 pub mod experiment;
 pub mod hybrid_scoring;
 pub mod retrieval_mode;
 pub mod simple_pool;
 pub mod soft_index;
 
 pub use analysis::{compare_experiments, generate_report, ExperimentComparison, Statistics};
-pub use bias::{BiasProfile, ClassBias, SpectralBias, ChromaBias, ProfileMetadata};
+pub use bias::{BiasProfile, ChromaBias, ClassBias, ProfileMetadata, SpectralBias};
 pub use diversity::{chroma_dispersion, mmr_score, retrieve_diverse_mmr, DiversityStats};
 pub use embedding::{EmbeddingMapper, QuerySignature};
 pub use experiment::{ExperimentConfig, ExperimentHarness, ExperimentResult, SeedingStrategy};
 pub use hybrid_scoring::{rerank_hybrid, RetrievalWeights};
 pub use retrieval_mode::RetrievalMode;
 pub use simple_pool::SimpleDreamPool;
-pub use soft_index::{SoftIndex, Similarity, EntryId};
+pub use soft_index::{EntryId, Similarity, SoftIndex};
diff --git a/src/dream/retrieval_mode.rs b/src/dream/retrieval_mode.rs
index 458f2285781f797fd7311ce36fba878967b6acc1..0d1a024efae96c5f4b29f5d6f3c44a1d4f6634af 100644
--- a/src/dream/retrieval_mode.rs
+++ b/src/dream/retrieval_mode.rs
@@ -1,30 +1,30 @@
 //! Retrieval mode configuration for Phase 4 training integration.
 //!
 //! Defines how the dream pool retrieval should operate during training.
 
-use serde::{Serialize, Deserialize};
+use serde::{Deserialize, Serialize};
 
 /// Retrieval mode for dream pool queries
 #[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
 pub enum RetrievalMode {
     /// Hard retrieval (Phase 3B): Class-aware cosine similarity on RGB
     /// Uses only chromatic signatures and optional class filtering
     Hard,
 
     /// Soft retrieval (Phase 4): Continuous embedding with ANN
     /// Uses EmbeddingMapper + SoftIndex + hybrid scoring
     Soft,
 
     /// Hybrid mode: Combine hard and soft retrieval results
     /// Retrieves from both methods and merges with deduplication
     Hybrid,
 }
 
 impl Default for RetrievalMode {
     fn default() -> Self {
         RetrievalMode::Hard
     }
 }
 
 impl RetrievalMode {
     /// Check if this mode requires soft index (Phase 4)
diff --git a/src/dream/simple_pool.rs b/src/dream/simple_pool.rs
index 612fefd9003f15f9c055592e66c277ce940e4224..58f201f0037e31c8740c83dc63598568835898c5 100644
--- a/src/dream/simple_pool.rs
+++ b/src/dream/simple_pool.rs
@@ -1,268 +1,278 @@
 //! SimpleDreamPool - In-memory dream storage with cosine similarity retrieval
 //!
 //! This is a minimal implementation designed for validation experiments.
 //! It stores ChromaticTensor dreams with their evaluation metrics and provides
 //! retrieval based on chromatic signature similarity.
 
 use crate::data::ColorClass;
-use crate::tensor::ChromaticTensor;
-use crate::solver::SolverResult;
-use crate::dream::soft_index::{SoftIndex, EntryId, Similarity};
+use crate::dream::bias::BiasProfile;
 use crate::dream::embedding::{EmbeddingMapper, QuerySignature};
-use crate::dream::hybrid_scoring::{RetrievalWeights, rerank_hybrid};
-use std::collections::{VecDeque, HashMap};
+use crate::dream::hybrid_scoring::{rerank_hybrid, RetrievalWeights};
+use crate::dream::soft_index::{EntryId, Similarity, SoftIndex};
+use crate::solver::SolverResult;
+use crate::tensor::ChromaticTensor;
+use serde::{Deserialize, Serialize};
+use std::collections::{HashMap, VecDeque};
 use std::time::SystemTime;
 
 /// A stored dream entry with tensor and evaluation metrics
 ///
 /// Enhanced for Phase 3B with class awareness, utility tracking, and timestamps
 /// Enhanced for Phase 4 with spectral features and embeddings
-#[derive(Clone)]
+#[derive(Debug, Clone, Serialize)]
 pub struct DreamEntry {
+    /// Unique identifier for the entry (stable across indices)
+    pub id: EntryId,
     pub tensor: ChromaticTensor,
     pub result: SolverResult,
     pub chroma_signature: [f32; 3],
     /// Optional class label for class-aware retrieval (Phase 3B)
     pub class_label: Option<ColorClass>,
     /// Utility score from feedback (Phase 3B)
     pub utility: Option<f32>,
     /// Timestamp for recency tracking (Phase 3B)
     pub timestamp: SystemTime,
     /// Number of times this dream has been retrieved (Phase 3B)
     pub usage_count: usize,
     /// Spectral features for embedding (Phase 4)
     pub spectral_features: Option<crate::spectral::SpectralFeatures>,
     /// Cached embedding vector (Phase 4)
     pub embed: Option<Vec<f32>>,
     /// Aggregated mean utility (Phase 4)
     pub util_mean: f32,
+    /// Number of feedback observations applied to this entry
+    pub feedback_count: usize,
 }
 
 impl DreamEntry {
     /// Create a new dream entry from a tensor and its evaluation result
     pub fn new(tensor: ChromaticTensor, result: SolverResult) -> Self {
         let chroma_signature = tensor.mean_rgb();
         Self {
+            id: EntryId::new_v4(),
             tensor,
             result,
             chroma_signature,
             class_label: None,
             utility: None,
             timestamp: SystemTime::now(),
             usage_count: 0,
             spectral_features: None,
             embed: None,
             util_mean: 0.0,
+            feedback_count: 0,
         }
     }
 
     /// Create a new dream entry with class label (Phase 3B)
     pub fn with_class(
         tensor: ChromaticTensor,
         result: SolverResult,
         class_label: ColorClass,
     ) -> Self {
         let chroma_signature = tensor.mean_rgb();
         Self {
+            id: EntryId::new_v4(),
             tensor,
             result,
             chroma_signature,
             class_label: Some(class_label),
             utility: None,
             timestamp: SystemTime::now(),
             usage_count: 0,
             spectral_features: None,
             embed: None,
             util_mean: 0.0,
+            feedback_count: 0,
         }
     }
 
     /// Update the utility score for this dream (Phase 3B)
     pub fn set_utility(&mut self, utility: f32) {
         self.utility = Some(utility);
+        self.util_mean = utility;
+        self.feedback_count = self.feedback_count.max(1);
     }
 
     /// Increment usage count when retrieved (Phase 3B)
     pub fn increment_usage(&mut self) {
         self.usage_count += 1;
     }
 }
 
 /// Configuration for SimpleDreamPool
-#[derive(Debug, Clone)]
+#[derive(Debug, Clone, Serialize, Deserialize)]
 pub struct PoolConfig {
     /// Maximum number of dreams to store in memory
     pub max_size: usize,
     /// Minimum coherence threshold for persistence (0.0-1.0)
     pub coherence_threshold: f64,
     /// Number of similar dreams to retrieve
     pub retrieval_limit: usize,
 }
 
 impl Default for PoolConfig {
     fn default() -> Self {
         Self {
             max_size: 1000,
             coherence_threshold: 0.75,
             retrieval_limit: 3,
         }
     }
 }
 
 /// In-memory dream pool with cosine similarity retrieval
 ///
 /// Stores high-coherence ChromaticTensor states and retrieves similar dreams
 /// based on chromatic signature (mean RGB) for retrieval-based seeding.
 ///
 /// # Example
 ///
 /// ```rust
 /// use chromatic_cognition_core::dream::simple_pool::PoolConfig;
 /// use chromatic_cognition_core::dream::SimpleDreamPool;
 /// use chromatic_cognition_core::{ChromaticNativeSolver, ChromaticTensor, Solver};
 ///
 /// let config = PoolConfig::default();
 /// let mut pool = SimpleDreamPool::new(config);
 ///
 /// // Evaluate and store a dream
 /// let tensor = ChromaticTensor::from_seed(42, 32, 32, 4);
 /// let mut solver = ChromaticNativeSolver::default();
 /// let result = solver.evaluate(&tensor, false).unwrap();
 ///
 /// pool.add_if_coherent(tensor.clone(), result);
 ///
 /// // Retrieve similar dreams
 /// let query_signature = tensor.mean_rgb();
 /// let similar = pool.retrieve_similar(&query_signature, 3);
 /// ```
+#[derive(Debug, Clone, Serialize)]
 pub struct SimpleDreamPool {
     entries: VecDeque<DreamEntry>,
     config: PoolConfig,
     /// Phase 4: Soft index for semantic retrieval
     soft_index: Option<SoftIndex>,
     /// Phase 4: Mapping from EntryId to DreamEntry for retrieval
     id_to_entry: HashMap<EntryId, DreamEntry>,
-    /// Phase 4: Mapping from index in entries VecDeque to EntryId
-    entry_ids: VecDeque<EntryId>,
+    /// Last mapper used to build the soft index
+    embedding_mapper: Option<EmbeddingMapper>,
+    /// Cached bias profile applied during encoding
+    current_bias: Option<BiasProfile>,
 }
 
 impl SimpleDreamPool {
     /// Create a new dream pool with the given configuration
     pub fn new(config: PoolConfig) -> Self {
         let max_size = config.max_size;
         Self {
             entries: VecDeque::with_capacity(max_size),
             config,
             soft_index: None,
             id_to_entry: HashMap::new(),
-            entry_ids: VecDeque::with_capacity(max_size),
+            embedding_mapper: None,
+            current_bias: None,
         }
     }
 
     /// Add a dream entry if it meets the coherence threshold
     ///
     /// Returns true if the dream was added, false otherwise.
     /// If the pool is at capacity, the oldest dream is removed (FIFO).
     pub fn add_if_coherent(&mut self, tensor: ChromaticTensor, result: SolverResult) -> bool {
         if result.coherence < self.config.coherence_threshold {
             return false;
         }
 
         let entry = DreamEntry::new(tensor, result);
 
         // Remove oldest entry if at capacity
         if self.entries.len() >= self.config.max_size {
-            self.entries.pop_front();
-            if let Some(old_id) = self.entry_ids.pop_front() {
-                self.id_to_entry.remove(&old_id);
+            if let Some(old_entry) = self.entries.pop_front() {
+                self.id_to_entry.remove(&old_entry.id);
             }
         }
 
-        let entry_id = EntryId::new_v4();
-        self.entry_ids.push_back(entry_id);
+        let entry_id = entry.id;
         self.id_to_entry.insert(entry_id, entry.clone());
         self.entries.push_back(entry);
 
         // Invalidate soft index since we added a new entry
         self.soft_index = None;
 
         true
     }
 
     /// Force add a dream entry regardless of coherence threshold
     ///
     /// Useful for testing or when coherence filtering is not desired.
     pub fn add(&mut self, tensor: ChromaticTensor, result: SolverResult) {
         let entry = DreamEntry::new(tensor, result);
 
         if self.entries.len() >= self.config.max_size {
-            self.entries.pop_front();
-            if let Some(old_id) = self.entry_ids.pop_front() {
-                self.id_to_entry.remove(&old_id);
+            if let Some(old_entry) = self.entries.pop_front() {
+                self.id_to_entry.remove(&old_entry.id);
             }
         }
 
-        let entry_id = EntryId::new_v4();
-        self.entry_ids.push_back(entry_id);
+        let entry_id = entry.id;
         self.id_to_entry.insert(entry_id, entry.clone());
         self.entries.push_back(entry);
 
         // Invalidate soft index since we added a new entry
         self.soft_index = None;
     }
 
     /// Add a dream entry with class label (Phase 3B)
     ///
     /// # Arguments
     /// * `tensor` - The chromatic tensor to store
     /// * `result` - The solver evaluation result
     /// * `class_label` - The color class this dream represents
     ///
     /// # Returns
     /// true if the dream was added, false if it didn't meet coherence threshold
     pub fn add_with_class(
         &mut self,
         tensor: ChromaticTensor,
         result: SolverResult,
         class_label: ColorClass,
     ) -> bool {
         if result.coherence < self.config.coherence_threshold {
             return false;
         }
 
         let entry = DreamEntry::with_class(tensor, result, class_label);
 
         if self.entries.len() >= self.config.max_size {
-            self.entries.pop_front();
-            if let Some(old_id) = self.entry_ids.pop_front() {
-                self.id_to_entry.remove(&old_id);
+            if let Some(old_entry) = self.entries.pop_front() {
+                self.id_to_entry.remove(&old_entry.id);
             }
         }
 
-        let entry_id = EntryId::new_v4();
-        self.entry_ids.push_back(entry_id);
+        let entry_id = entry.id;
         self.id_to_entry.insert(entry_id, entry.clone());
         self.entries.push_back(entry);
 
         // Invalidate soft index since we added a new entry
         self.soft_index = None;
 
         true
     }
 
     /// Retrieve K most similar dreams based on cosine similarity of chroma signatures
     ///
     /// # Arguments
     /// * `query_signature` - Target RGB signature to match against [r, g, b]
     /// * `k` - Number of similar dreams to retrieve
     ///
     /// # Returns
     /// Vector of up to K most similar dreams, sorted by similarity (highest first)
     pub fn retrieve_similar(&self, query_signature: &[f32; 3], k: usize) -> Vec<DreamEntry> {
         if self.entries.is_empty() {
             return Vec::new();
         }
 
         // Compute cosine similarity for all entries
         let mut scored: Vec<(f32, &DreamEntry)> = self
             .entries
@@ -358,51 +368,54 @@ impl SimpleDreamPool {
     ///
     /// Only retrieves dreams with utility >= threshold.
     ///
     /// # Arguments
     /// * `query_signature` - Target RGB signature
     /// * `k` - Number of dreams to retrieve
     /// * `utility_threshold` - Minimum utility score
     ///
     /// # Returns
     /// Vector of high-utility dreams sorted by similarity
     pub fn retrieve_by_utility(
         &self,
         query_signature: &[f32; 3],
         k: usize,
         utility_threshold: f32,
     ) -> Vec<DreamEntry> {
         if self.entries.is_empty() {
             return Vec::new();
         }
 
         // Filter by utility, then compute similarity
         let mut scored: Vec<(f32, &DreamEntry)> = self
             .entries
             .iter()
             .filter(|entry| {
-                entry.utility.map(|u| u >= utility_threshold).unwrap_or(false)
+                entry
+                    .utility
+                    .map(|u| u >= utility_threshold)
+                    .unwrap_or(false)
             })
             .map(|entry| {
                 let similarity = cosine_similarity(query_signature, &entry.chroma_signature);
                 (similarity, entry)
             })
             .collect();
 
         // Sort by similarity (descending)
         scored.sort_by(|a, b| b.0.partial_cmp(&a.0).unwrap_or(std::cmp::Ordering::Equal));
 
         // Take top K and clone
         scored
             .into_iter()
             .take(k)
             .map(|(_, entry)| entry.clone())
             .collect()
     }
 
     /// Retrieve diverse dreams using Maximum Marginal Relevance (Phase 3B)
     ///
     /// Balances relevance to query with diversity from already-selected dreams.
     /// Uses MMR algorithm to avoid near-duplicates and ensure chromatic variety.
     ///
     /// # Arguments
     /// * `query_signature` - Target RGB signature
@@ -482,304 +495,367 @@ impl SimpleDreamPool {
     }
 
     /// Get diversity statistics for the pool (Phase 3B)
     ///
     /// Computes chromatic dispersion and distance metrics across all dreams.
     ///
     /// # Returns
     /// * DiversityStats with mean/min/max pairwise distances
     pub fn diversity_stats(&self) -> crate::dream::diversity::DiversityStats {
         use crate::dream::diversity::DiversityStats;
 
         if self.entries.is_empty() {
             return DiversityStats {
                 mean_dispersion: 0.0,
                 min_distance: 0.0,
                 max_distance: 0.0,
                 count: 0,
             };
         }
 
         let entries: Vec<DreamEntry> = self.entries.iter().cloned().collect();
         DiversityStats::compute(&entries)
     }
 
     /// Rebuild the soft index using the provided embedding mapper (Phase 4)
-    ///
-    /// # Arguments
-    /// * `mapper` - EmbeddingMapper to encode entries into fixed-dimensional vectors
-    /// * `bias` - Optional BiasProfile for query-time conditioning
-    ///
-    /// This method encodes all current entries and builds a fresh SoftIndex for
-    /// semantic retrieval. Should be called after adding a batch of entries or
-    /// when BiasProfile changes significantly.
-    pub fn rebuild_soft_index(
-        &mut self,
-        mapper: &EmbeddingMapper,
-        bias: Option<&crate::dream::bias::BiasProfile>,
-    ) {
-        let embed_dim = mapper.dim;
+    pub fn rebuild_soft_index(&mut self, mapper: &EmbeddingMapper) {
+        let embed_dim = mapper.d;
         let mut index = SoftIndex::new(embed_dim);
 
-        // Clear old mappings
+        // Clear old mappings and rebuild
         self.id_to_entry.clear();
-        self.entry_ids.clear();
 
-        // Encode all entries and add to index
-        for entry in &self.entries {
-            let embedding = mapper.encode_entry(entry, bias);
-            let entry_id = EntryId::new_v4();
-
-            index.add(entry_id, embedding);
-            self.id_to_entry.insert(entry_id, entry.clone());
-            self.entry_ids.push_back(entry_id);
+        for entry in self.entries.iter_mut() {
+            let embedding = mapper.encode_entry(entry, self.current_bias.as_ref());
+            entry.embed = Some(embedding.clone());
+            index.add(entry.id, embedding);
+            self.id_to_entry.insert(entry.id, entry.clone());
         }
 
         index.build();
+        self.embedding_mapper = Some(mapper.clone());
         self.soft_index = Some(index);
     }
 
+    /// Update the cached bias profile used for embedding encoding.
+    pub fn set_bias_profile(&mut self, bias: Option<BiasProfile>) {
+        self.current_bias = bias;
+        // Require index rebuild to reflect new bias
+        self.soft_index = None;
+    }
+
     /// Retrieve dreams using soft index with hybrid scoring (Phase 4)
-    ///
-    /// # Arguments
-    /// * `query` - QuerySignature specifying target chromatic features and hints
-    /// * `k` - Number of dreams to retrieve
-    /// * `weights` - RetrievalWeights for hybrid scoring (α·sim + β·util + γ·class + MMR)
-    /// * `mode` - Similarity metric (Cosine or Euclidean)
-    /// * `mapper` - EmbeddingMapper to encode the query
-    /// * `bias` - Optional BiasProfile for query conditioning
-    ///
-    /// # Returns
-    /// Vec<DreamEntry> ordered by hybrid score (descending)
-    ///
-    /// If soft index doesn't exist, returns empty vec. Call `rebuild_soft_index` first.
     pub fn retrieve_soft(
         &self,
         query: &QuerySignature,
         k: usize,
         weights: &RetrievalWeights,
         mode: Similarity,
-        mapper: &EmbeddingMapper,
-        bias: Option<&crate::dream::bias::BiasProfile>,
     ) -> Vec<DreamEntry> {
-        // Check if soft index exists
+        self.retrieve_soft_with_scores(query, k, weights, mode)
+            .into_iter()
+            .map(|(_, entry, _)| entry)
+            .collect()
+    }
+
+    /// Retrieve soft matches with associated EntryId and score.
+    pub fn retrieve_soft_with_scores(
+        &self,
+        query: &QuerySignature,
+        k: usize,
+        weights: &RetrievalWeights,
+        mode: Similarity,
+    ) -> Vec<(EntryId, DreamEntry, f32)> {
+        if k == 0 {
+            return Vec::new();
+        }
         let index = match &self.soft_index {
             Some(idx) => idx,
-            None => return Vec::new(), // No index built yet
+            None => return Vec::new(),
+        };
+        let mapper = match &self.embedding_mapper {
+            Some(m) => m,
+            None => return Vec::new(),
         };
 
-        // Encode query
-        let query_embedding = mapper.encode_query(query, bias);
-
-        // Get initial k-NN from SoftIndex
-        let hits = index.query(&query_embedding, k, mode);
-
-        // Apply hybrid scoring with MMR diversity
+        let query_embedding = mapper.encode_query(query, self.current_bias.as_ref());
+        let hits = index.query(&query_embedding, k.max(1), mode);
         let reranked = rerank_hybrid(&hits, weights, &self.id_to_entry, query.class_hint);
 
-        // Map EntryIds back to DreamEntries
         reranked
             .into_iter()
-            .filter_map(|(id, _score)| self.id_to_entry.get(&id).cloned())
+            .take(k)
+            .filter_map(|(id, score)| {
+                self.id_to_entry
+                    .get(&id)
+                    .cloned()
+                    .map(|entry| (id, entry, score))
+            })
             .collect()
     }
 
+    /// Apply utility feedback to an entry and mark the index for rebuild.
+    pub fn update_entry_feedback(&mut self, id: EntryId, utility: f32) {
+        let mut updated = false;
+
+        for entry in self.entries.iter_mut() {
+            if entry.id == id {
+                entry.feedback_count += 1;
+                entry.utility = Some(utility);
+                let count = entry.feedback_count as f32;
+                let prev_sum = entry.util_mean * (count - 1.0);
+                entry.util_mean = (prev_sum + utility) / count.max(1.0);
+                updated = true;
+                break;
+            }
+        }
+
+        if let Some(entry) = self.id_to_entry.get_mut(&id) {
+            entry.feedback_count += 1;
+            entry.utility = Some(utility);
+            let count = entry.feedback_count as f32;
+            let prev_sum = entry.util_mean * (count - 1.0);
+            entry.util_mean = (prev_sum + utility) / count.max(1.0);
+        }
+
+        if updated {
+            self.soft_index = None;
+        }
+    }
+
     /// Check if soft index is built (Phase 4)
     pub fn has_soft_index(&self) -> bool {
         self.soft_index.is_some()
     }
 
     /// Get number of entries in the soft index (Phase 4)
     pub fn soft_index_size(&self) -> usize {
         self.soft_index.as_ref().map_or(0, |idx| idx.len())
     }
 }
 
 /// Pool statistics for monitoring and analysis
-#[derive(Debug, Clone)]
+#[derive(Debug, Clone, Serialize)]
 pub struct PoolStats {
     pub count: usize,
     pub mean_coherence: f64,
     pub mean_energy: f64,
     pub mean_violation: f64,
 }
 
 /// Compute cosine similarity between two 3D vectors
 ///
 /// Returns a value in [-1, 1] where 1 means identical direction,
 /// 0 means orthogonal, and -1 means opposite direction.
 fn cosine_similarity(a: &[f32; 3], b: &[f32; 3]) -> f32 {
     let dot = a[0] * b[0] + a[1] * b[1] + a[2] * b[2];
     let mag_a = (a[0] * a[0] + a[1] * a[1] + a[2] * a[2]).sqrt();
     let mag_b = (b[0] * b[0] + b[1] * b[1] + b[2] * b[2]).sqrt();
 
     if mag_a == 0.0 || mag_b == 0.0 {
         return 0.0;
     }
 
     dot / (mag_a * mag_b)
 }
 
 #[cfg(test)]
 mod tests {
     use super::*;
 
     #[test]
     fn test_cosine_similarity() {
         // Identical vectors
         assert!((cosine_similarity(&[1.0, 0.0, 0.0], &[1.0, 0.0, 0.0]) - 1.0).abs() < 1e-6);
 
         // Orthogonal vectors
         assert!((cosine_similarity(&[1.0, 0.0, 0.0], &[0.0, 1.0, 0.0]) - 0.0).abs() < 1e-6);
 
         // Opposite vectors
         assert!((cosine_similarity(&[1.0, 0.0, 0.0], &[-1.0, 0.0, 0.0]) - (-1.0)).abs() < 1e-6);
     }
 
     #[test]
     fn test_pool_add_and_retrieve() {
-        use crate::ChromaticTensor;
         use crate::solver::SolverResult;
+        use crate::ChromaticTensor;
         use serde_json::json;
 
         let config = PoolConfig {
             max_size: 5,
             coherence_threshold: 0.5,
             retrieval_limit: 3,
         };
         let mut pool = SimpleDreamPool::new(config);
 
         // Add some dreams with different signatures
         let tensor1 = ChromaticTensor::from_seed(42, 8, 8, 2);
         let result1 = SolverResult {
             energy: 0.1,
             coherence: 0.9,
             violation: 0.05,
             grad: None,
             mask: None,
             meta: json!({}),
         };
 
         assert!(pool.add_if_coherent(tensor1.clone(), result1));
         assert_eq!(pool.len(), 1);
 
         // Retrieve similar to tensor1's signature
         let similar = pool.retrieve_similar(&tensor1.mean_rgb(), 1);
         assert_eq!(similar.len(), 1);
     }
 
     #[test]
-    fn test_pool_capacity() {
+    fn test_update_entry_feedback_invalidates_soft_index() {
+        use crate::solver::SolverResult;
         use crate::ChromaticTensor;
+        use serde_json::json;
+
+        let mut pool = SimpleDreamPool::new(PoolConfig::default());
+
+        let tensor = ChromaticTensor::from_seed(7, 8, 8, 2);
+        let result = SolverResult {
+            energy: 0.1,
+            coherence: 0.9,
+            violation: 0.05,
+            grad: None,
+            mask: None,
+            meta: json!({}),
+        };
+
+        assert!(pool.add_if_coherent(tensor, result));
+        let mapper = EmbeddingMapper::new(16);
+        pool.rebuild_soft_index(&mapper);
+
+        assert!(pool.has_soft_index());
+        let entry_id = pool.entries.front().map(|entry| entry.id).unwrap();
+
+        pool.update_entry_feedback(entry_id, 0.75);
+
+        assert!(!pool.has_soft_index());
+        let entry = pool.entries.front().unwrap();
+        assert_eq!(entry.util_mean, 0.75);
+        assert_eq!(entry.feedback_count, 1);
+    }
+
+    #[test]
+    fn test_pool_capacity() {
         use crate::solver::SolverResult;
+        use crate::ChromaticTensor;
         use serde_json::json;
 
         let config = PoolConfig {
             max_size: 3,
             coherence_threshold: 0.0,
             retrieval_limit: 3,
         };
         let mut pool = SimpleDreamPool::new(config);
 
         // Add 5 dreams to a pool with max_size = 3
         for i in 0..5 {
             let tensor = ChromaticTensor::from_seed(i, 8, 8, 2);
             let result = SolverResult {
                 energy: 0.1,
                 coherence: 0.9,
                 violation: 0.05,
                 grad: None,
                 mask: None,
                 meta: json!({}),
             };
             pool.add(tensor, result);
         }
 
         // Should only have 3 dreams (oldest 2 evicted)
         assert_eq!(pool.len(), 3);
     }
 
     #[test]
     fn test_class_aware_retrieval() {
-        use crate::ChromaticTensor;
         use crate::data::ColorClass;
         use crate::solver::SolverResult;
+        use crate::ChromaticTensor;
         use serde_json::json;
 
         let config = PoolConfig {
             max_size: 20,
             coherence_threshold: 0.5,
             retrieval_limit: 5,
         };
         let mut pool = SimpleDreamPool::new(config);
 
         // Add dreams with different class labels
         for i in 0..5 {
             let tensor = ChromaticTensor::from_seed(i, 8, 8, 2);
             let result = SolverResult {
                 energy: 0.1,
                 coherence: 0.8,
                 violation: 0.05,
                 grad: None,
                 mask: None,
                 meta: json!({}),
             };
             pool.add_with_class(tensor, result, ColorClass::Red);
         }
 
         for i in 5..10 {
             let tensor = ChromaticTensor::from_seed(i, 8, 8, 2);
             let result = SolverResult {
                 energy: 0.1,
                 coherence: 0.8,
                 violation: 0.05,
                 grad: None,
                 mask: None,
                 meta: json!({}),
             };
             pool.add_with_class(tensor, result, ColorClass::Blue);
         }
 
         assert_eq!(pool.len(), 10);
 
         // Retrieve only Red class dreams
         let query = [1.0, 0.0, 0.0];
         let red_dreams = pool.retrieve_similar_class(&query, ColorClass::Red, 3);
         assert_eq!(red_dreams.len(), 3);
-        assert!(red_dreams.iter().all(|d| d.class_label == Some(ColorClass::Red)));
+        assert!(red_dreams
+            .iter()
+            .all(|d| d.class_label == Some(ColorClass::Red)));
 
         // Retrieve only Blue class dreams
         let blue_dreams = pool.retrieve_similar_class(&query, ColorClass::Blue, 3);
         assert_eq!(blue_dreams.len(), 3);
-        assert!(blue_dreams.iter().all(|d| d.class_label == Some(ColorClass::Blue)));
+        assert!(blue_dreams
+            .iter()
+            .all(|d| d.class_label == Some(ColorClass::Blue)));
     }
 
     #[test]
     fn test_balanced_retrieval() {
-        use crate::ChromaticTensor;
         use crate::data::ColorClass;
         use crate::solver::SolverResult;
+        use crate::ChromaticTensor;
         use serde_json::json;
 
         let config = PoolConfig::default();
         let mut pool = SimpleDreamPool::new(config);
 
         // Add dreams from three classes
         for i in 0..5 {
             let tensor = ChromaticTensor::from_seed(i, 8, 8, 2);
             let result = SolverResult {
                 energy: 0.1,
                 coherence: 0.8,
                 violation: 0.05,
                 grad: None,
                 mask: None,
                 meta: json!({}),
             };
             pool.add_with_class(tensor, result, ColorClass::Red);
         }
 
         for i in 5..10 {
             let tensor = ChromaticTensor::from_seed(i, 8, 8, 2);
             let result = SolverResult {
                 energy: 0.1,
                 coherence: 0.8,
                 violation: 0.05,
@@ -789,63 +865,72 @@ mod tests {
             };
             pool.add_with_class(tensor, result, ColorClass::Green);
         }
 
         for i in 10..15 {
             let tensor = ChromaticTensor::from_seed(i, 8, 8, 2);
             let result = SolverResult {
                 energy: 0.1,
                 coherence: 0.8,
                 violation: 0.05,
                 grad: None,
                 mask: None,
                 meta: json!({}),
             };
             pool.add_with_class(tensor, result, ColorClass::Blue);
         }
 
         // Retrieve balanced across 3 classes, 2 per class
         let query = [0.5, 0.5, 0.5];
         let classes = vec![ColorClass::Red, ColorClass::Green, ColorClass::Blue];
         let balanced = pool.retrieve_balanced(&query, &classes, 2);
 
         assert_eq!(balanced.len(), 6); // 2 * 3 classes
 
         // Count per class
-        let red_count = balanced.iter().filter(|d| d.class_label == Some(ColorClass::Red)).count();
-        let green_count = balanced.iter().filter(|d| d.class_label == Some(ColorClass::Green)).count();
-        let blue_count = balanced.iter().filter(|d| d.class_label == Some(ColorClass::Blue)).count();
+        let red_count = balanced
+            .iter()
+            .filter(|d| d.class_label == Some(ColorClass::Red))
+            .count();
+        let green_count = balanced
+            .iter()
+            .filter(|d| d.class_label == Some(ColorClass::Green))
+            .count();
+        let blue_count = balanced
+            .iter()
+            .filter(|d| d.class_label == Some(ColorClass::Blue))
+            .count();
 
         assert_eq!(red_count, 2);
         assert_eq!(green_count, 2);
         assert_eq!(blue_count, 2);
     }
 
     #[test]
     fn test_utility_retrieval() {
-        use crate::ChromaticTensor;
         use crate::solver::SolverResult;
+        use crate::ChromaticTensor;
         use serde_json::json;
 
         let config = PoolConfig {
             max_size: 10,
             coherence_threshold: 0.0,
             retrieval_limit: 5,
         };
         let mut pool = SimpleDreamPool::new(config);
 
         // Add dreams with varying utility
         for i in 0..5 {
             let tensor = ChromaticTensor::from_seed(i, 8, 8, 2);
             let result = SolverResult {
                 energy: 0.1,
                 coherence: 0.8,
                 violation: 0.05,
                 grad: None,
                 mask: None,
                 meta: json!({}),
             };
             let mut entry = DreamEntry::new(tensor, result);
             entry.set_utility(0.9); // High utility
 
             pool.entries.push_back(entry);
         }
diff --git a/src/dream/soft_index.rs b/src/dream/soft_index.rs
index b364de659b6f589b743cb7a48cd147f702a89b64..f7795b288cf90fca829c71661920e353f1921031 100644
--- a/src/dream/soft_index.rs
+++ b/src/dream/soft_index.rs
@@ -1,92 +1,97 @@
 //! Soft index for approximate nearest neighbor search with embeddings.
 //!
 //! In-memory ANN-lite supporting cosine and euclidean similarity.
 
+use serde::{Deserialize, Serialize};
+
 /// Unique identifier for indexed entries
 pub type EntryId = uuid::Uuid;
 
 /// Similarity metric for retrieval
-#[derive(Debug, Clone, Copy, PartialEq)]
+#[derive(Debug, Clone, Copy, PartialEq, Serialize, Deserialize)]
 pub enum Similarity {
     /// Cosine similarity: dot(a,b) / (||a|| * ||b||)
     Cosine,
     /// Euclidean distance: ||a - b||
     Euclidean,
 }
 
 /// In-memory soft index for semantic retrieval
+#[derive(Debug, Clone, Serialize)]
 pub struct SoftIndex {
     /// Embedding dimension
-    dim: usize,
+    pub dim: usize,
 
     /// Entry IDs
-    ids: Vec<EntryId>,
+    pub ids: Vec<EntryId>,
 
     /// Embedding vectors
-    vecs: Vec<Vec<f32>>,
+    pub vecs: Vec<Vec<f32>>,
 
-    /// Pre-computed norms for cosine similarity
+    /// Pre-computed norms for cosine similarity (derived, skip serialization)
+    #[serde(skip_serializing, skip_deserializing)]
     norms: Vec<f32>,
 }
 
 impl SoftIndex {
     /// Create a new soft index
     pub fn new(dim: usize) -> Self {
         Self {
             dim,
             ids: Vec::new(),
             vecs: Vec::new(),
             norms: Vec::new(),
         }
     }
 
     /// Add an entry to the index
     pub fn add(&mut self, id: EntryId, vec: Vec<f32>) {
         assert_eq!(vec.len(), self.dim, "Vector dimension mismatch");
 
         self.ids.push(id);
         self.vecs.push(vec);
         self.norms.push(0.0); // Will be computed in build()
     }
 
     /// Build the index (compute norms)
     pub fn build(&mut self) {
         self.norms = self.vecs.iter().map(|v| l2_norm(v)).collect();
     }
 
     /// Query for K nearest neighbors
     pub fn query(&self, query: &[f32], k: usize, mode: Similarity) -> Vec<(EntryId, f32)> {
         assert_eq!(query.len(), self.dim, "Query dimension mismatch");
 
         if self.ids.is_empty() {
             return Vec::new();
         }
 
         // Compute similarities
         let query_norm = l2_norm(query);
-        let mut scores: Vec<(usize, f32)> = self.vecs
+        let mut scores: Vec<(usize, f32)> = self
+            .vecs
             .iter()
             .enumerate()
             .map(|(idx, vec)| {
                 let score = match mode {
                     Similarity::Cosine => cosine_sim(query, vec, query_norm, self.norms[idx]),
                     Similarity::Euclidean => -euclidean_dist(query, vec), // Negative for sorting
                 };
                 (idx, score)
             })
             .collect();
 
         // Sort by score (descending)
         scores.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));
 
         // Return top-K
         scores
             .into_iter()
             .take(k)
             .map(|(idx, score)| (self.ids[idx], score))
             .collect()
     }
 
     /// Get number of entries
     pub fn len(&self) -> usize {
         self.ids.len()
diff --git a/src/learner/classifier.rs b/src/learner/classifier.rs
index 8d8980aa64c32c0cc8a3241c69315dffdcfaa2a6..c97cf1e0ba23cb8e91619e0110b3e782cc51b546 100644
--- a/src/learner/classifier.rs
+++ b/src/learner/classifier.rs
@@ -1,35 +1,35 @@
 //! Color classification model
 //!
 //! Implements a simple MLP (Multi-Layer Perceptron) for classifying
 //! ChromaticTensor inputs into 10 color classes.
 
-use crate::tensor::ChromaticTensor;
 use crate::data::ColorClass;
+use crate::tensor::ChromaticTensor;
 use ndarray::{Array1, Array2};
 use rand::{Rng, SeedableRng};
-use serde::{Serialize, Deserialize};
+use serde::{Deserialize, Serialize};
 
 /// Configuration for the classifier
 #[derive(Debug, Clone, Serialize, Deserialize)]
 pub struct ClassifierConfig {
     /// Input size (flattened tensor dimensions)
     pub input_size: usize,
     /// Hidden layer size
     pub hidden_size: usize,
     /// Output size (number of classes)
     pub output_size: usize,
     /// Random seed for weight initialization
     pub seed: u64,
 }
 
 impl Default for ClassifierConfig {
     fn default() -> Self {
         Self {
             input_size: 16 * 16 * 4 * 3, // 16x16x4 tensor with RGB = 3072
             hidden_size: 128,
             output_size: 10, // 10 color classes
             seed: 42,
         }
     }
 }
 
@@ -68,62 +68,60 @@ pub struct Weights {
 pub struct Gradients {
     pub dw1: Array2<f32>,
     pub db1: Array1<f32>,
     pub dw2: Array2<f32>,
     pub db2: Array1<f32>,
 }
 
 /// Simple MLP classifier: Input → Hidden (ReLU) → Output (Softmax)
 pub struct MLPClassifier {
     config: ClassifierConfig,
     // Layer 1: input → hidden
     w1: Array2<f32>, // [hidden_size, input_size]
     b1: Array1<f32>, // [hidden_size]
     // Layer 2: hidden → output
     w2: Array2<f32>, // [output_size, hidden_size]
     b2: Array1<f32>, // [output_size]
 }
 
 impl MLPClassifier {
     /// Create a new MLP classifier with random initialization
     pub fn new(config: ClassifierConfig) -> Self {
         let mut rng = rand::rngs::StdRng::seed_from_u64(config.seed);
 
         // Xavier initialization for weights
         let w1_scale = (2.0 / config.input_size as f32).sqrt();
-        let w1 = Array2::from_shape_fn(
-            (config.hidden_size, config.input_size),
-            |_| (rng.gen::<f32>() - 0.5) * 2.0 * w1_scale,
-        );
+        let w1 = Array2::from_shape_fn((config.hidden_size, config.input_size), |_| {
+            (rng.gen::<f32>() - 0.5) * 2.0 * w1_scale
+        });
 
         let b1 = Array1::zeros(config.hidden_size);
 
         let w2_scale = (2.0 / config.hidden_size as f32).sqrt();
-        let w2 = Array2::from_shape_fn(
-            (config.output_size, config.hidden_size),
-            |_| (rng.gen::<f32>() - 0.5) * 2.0 * w2_scale,
-        );
+        let w2 = Array2::from_shape_fn((config.output_size, config.hidden_size), |_| {
+            (rng.gen::<f32>() - 0.5) * 2.0 * w2_scale
+        });
 
         let b2 = Array1::zeros(config.output_size);
 
         Self {
             config,
             w1,
             b1,
             w2,
             b2,
         }
     }
 
     /// Flatten a ChromaticTensor into a 1D feature vector
     fn flatten_tensor(&self, tensor: &ChromaticTensor) -> Array1<f32> {
         let (rows, cols, layers) = tensor.dims();
         let mut features = Array1::zeros(rows * cols * layers * 3);
 
         let mut idx = 0;
         for r in 0..rows {
             for c in 0..cols {
                 for l in 0..layers {
                     let rgb = tensor.get_rgb(r, c, l);
                     features[idx] = rgb[0];
                     features[idx + 1] = rgb[1];
                     features[idx + 2] = rgb[2];
@@ -243,147 +241,172 @@ impl ColorClassifier for MLPClassifier {
 
         (avg_loss, Gradients { dw1, db1, dw2, db2 })
     }
 
     fn update_weights(&mut self, gradients: &Gradients, learning_rate: f32) {
         // Gradient descent: W = W - lr * dW
         self.w1 = &self.w1 - &(&gradients.dw1 * learning_rate);
         self.b1 = &self.b1 - &(&gradients.db1 * learning_rate);
         self.w2 = &self.w2 - &(&gradients.dw2 * learning_rate);
         self.b2 = &self.b2 - &(&gradients.db2 * learning_rate);
     }
 
     fn get_weights(&self) -> Weights {
         Weights {
             w1: self.w1.iter().cloned().collect(),
             b1: self.b1.iter().cloned().collect(),
             w2: self.w2.iter().cloned().collect(),
             b2: self.b2.iter().cloned().collect(),
         }
     }
 
     fn set_weights(&mut self, weights: Weights) {
         self.w1 = Array2::from_shape_vec(
             (self.config.hidden_size, self.config.input_size),
             weights.w1,
-        ).expect("Invalid w1 shape");
+        )
+        .expect("Invalid w1 shape");
 
         self.b1 = Array1::from_vec(weights.b1);
 
         self.w2 = Array2::from_shape_vec(
             (self.config.output_size, self.config.hidden_size),
             weights.w2,
-        ).expect("Invalid w2 shape");
+        )
+        .expect("Invalid w2 shape");
 
         self.b2 = Array1::from_vec(weights.b2);
     }
 }
 
 #[cfg(test)]
 mod tests {
     use super::*;
     use crate::data::{ColorDataset, DatasetConfig};
 
     #[test]
     fn test_mlp_creation() {
         let config = ClassifierConfig::default();
         let classifier = MLPClassifier::new(config);
 
         assert_eq!(classifier.w1.dim(), (128, 3072));
         assert_eq!(classifier.b1.dim(), 128);
         assert_eq!(classifier.w2.dim(), (10, 128));
         assert_eq!(classifier.b2.dim(), 10);
     }
 
     #[test]
     fn test_forward_pass() {
         let config = ClassifierConfig::default();
         let classifier = MLPClassifier::new(config);
 
         let tensor = ChromaticTensor::from_seed(42, 16, 16, 4);
         let output = classifier.forward(&tensor);
 
         assert_eq!(output.len(), 10);
 
         // Check softmax: probabilities sum to 1
         let sum: f32 = output.iter().sum();
         assert!((sum - 1.0).abs() < 1e-5);
 
         // All probabilities should be positive
         assert!(output.iter().all(|&p| p >= 0.0 && p <= 1.0));
     }
 
     #[test]
     fn test_predict() {
         let config = ClassifierConfig::default();
         let classifier = MLPClassifier::new(config);
 
         let tensor = ChromaticTensor::from_seed(42, 16, 16, 4);
         let prediction = classifier.predict(&tensor);
 
         // Should return a valid color class
-        assert!(matches!(prediction, ColorClass::Red | ColorClass::Green |
-                        ColorClass::Blue | ColorClass::Yellow | ColorClass::Cyan |
-                        ColorClass::Magenta | ColorClass::Orange | ColorClass::Purple |
-                        ColorClass::White | ColorClass::Black));
+        assert!(matches!(
+            prediction,
+            ColorClass::Red
+                | ColorClass::Green
+                | ColorClass::Blue
+                | ColorClass::Yellow
+                | ColorClass::Cyan
+                | ColorClass::Magenta
+                | ColorClass::Orange
+                | ColorClass::Purple
+                | ColorClass::White
+                | ColorClass::Black
+        ));
     }
 
     #[test]
     fn test_compute_loss_and_gradients() {
         let config = ClassifierConfig::default();
         let classifier = MLPClassifier::new(config);
 
         // Create small batch
         let dataset_config = DatasetConfig {
             tensor_size: (16, 16, 4),
             samples_per_class: 1,
             ..Default::default()
         };
         let dataset = ColorDataset::generate(dataset_config);
 
-        let tensors: Vec<_> = dataset.samples.iter().take(5).map(|s| s.tensor.clone()).collect();
+        let tensors: Vec<_> = dataset
+            .samples
+            .iter()
+            .take(5)
+            .map(|s| s.tensor.clone())
+            .collect();
         let labels: Vec<_> = dataset.samples.iter().take(5).map(|s| s.label).collect();
 
         let (loss, gradients) = classifier.compute_loss(&tensors, &labels);
 
         // Loss should be positive
         assert!(loss > 0.0);
         assert!(loss.is_finite());
 
         // Gradients should have correct shapes
         assert_eq!(gradients.dw1.dim(), classifier.w1.dim());
         assert_eq!(gradients.db1.dim(), classifier.b1.dim());
         assert_eq!(gradients.dw2.dim(), classifier.w2.dim());
         assert_eq!(gradients.db2.dim(), classifier.b2.dim());
 
         // Gradients should be finite
         assert!(gradients.dw1.iter().all(|&v| v.is_finite()));
         assert!(gradients.db1.iter().all(|&v| v.is_finite()));
     }
 
     #[test]
     fn test_weight_update() {
         let config = ClassifierConfig::default();
         let mut classifier = MLPClassifier::new(config);
 
         let initial_w1 = classifier.w1.clone();
 
         // Create dummy gradients
         let dataset_config = DatasetConfig {
             tensor_size: (16, 16, 4),
             samples_per_class: 1,
             ..Default::default()
         };
         let dataset = ColorDataset::generate(dataset_config);
 
-        let tensors: Vec<_> = dataset.samples.iter().take(2).map(|s| s.tensor.clone()).collect();
+        let tensors: Vec<_> = dataset
+            .samples
+            .iter()
+            .take(2)
+            .map(|s| s.tensor.clone())
+            .collect();
         let labels: Vec<_> = dataset.samples.iter().take(2).map(|s| s.label).collect();
 
         let (_, gradients) = classifier.compute_loss(&tensors, &labels);
 
         // Update weights
         classifier.update_weights(&gradients, 0.01);
 
         // Weights should have changed
-        assert!(classifier.w1.iter().zip(initial_w1.iter()).any(|(a, b)| (a - b).abs() > 1e-6));
+        assert!(classifier
+            .w1
+            .iter()
+            .zip(initial_w1.iter())
+            .any(|(a, b)| (a - b).abs() > 1e-6));
     }
 }
diff --git a/src/learner/feedback.rs b/src/learner/feedback.rs
index 6c59ec9a89ae82263920ac5fa7c63fccee539a12..c3bc36b80394e08c1e772dc51e1ff38271f7b251 100644
--- a/src/learner/feedback.rs
+++ b/src/learner/feedback.rs
@@ -223,91 +223,87 @@ impl UtilityAggregator {
     }
 
     /// Filter records by minimum utility threshold.
     pub fn filter_by_utility(&self, min_utility: f32) -> Vec<&FeedbackRecord> {
         self.records
             .iter()
             .filter(|r| r.utility >= min_utility)
             .collect()
     }
 
     /// Get records from a specific class.
     pub fn filter_by_class(&self, class: ColorClass) -> Vec<&FeedbackRecord> {
         self.records
             .iter()
             .filter(|r| r.class_label == Some(class))
             .collect()
     }
 
     /// Compute correlation between spectral entropy and utility.
     ///
     /// Returns None if insufficient data with spectral features.
     pub fn entropy_utility_correlation(&self) -> Option<f32> {
         let pairs: Vec<(f32, f32)> = self
             .records
             .iter()
-            .filter_map(|r| {
-                r.spectral_features
-                    .as_ref()
-                    .map(|f| (f.entropy, r.utility))
-            })
+            .filter_map(|r| r.spectral_features.as_ref().map(|f| (f.entropy, r.utility)))
             .collect();
 
         if pairs.len() < 2 {
             return None;
         }
 
         Some(compute_correlation(&pairs))
     }
 
     /// Clear all records.
     pub fn clear(&mut self) {
         self.records.clear();
         self.class_stats.clear();
     }
 
     /// Recompute class statistics from records.
     fn recompute_stats(&mut self) {
         // Reset all stats
         for stats in self.class_stats.values_mut() {
             stats.mean_utility = 0.0;
             stats.mean_entropy = None;
         }
 
         // Group records by class
         let mut class_records: HashMap<ColorClass, Vec<&FeedbackRecord>> = HashMap::new();
         for record in &self.records {
             if let Some(class) = record.class_label {
                 class_records.entry(class).or_default().push(record);
             }
         }
 
         // Compute mean utility and entropy per class
         for (class, records) in class_records {
             if let Some(stats) = self.class_stats.get_mut(&class) {
-                let mean_utility: f32 = records.iter().map(|r| r.utility).sum::<f32>()
-                    / records.len() as f32;
+                let mean_utility: f32 =
+                    records.iter().map(|r| r.utility).sum::<f32>() / records.len() as f32;
                 stats.mean_utility = mean_utility;
 
                 // Compute mean entropy if available
                 let entropies: Vec<f32> = records
                     .iter()
                     .filter_map(|r| r.spectral_features.as_ref().map(|f| f.entropy))
                     .collect();
 
                 if !entropies.is_empty() {
                     let mean_entropy = entropies.iter().sum::<f32>() / entropies.len() as f32;
                     stats.mean_entropy = Some(mean_entropy);
                 }
             }
         }
     }
 }
 
 impl Default for UtilityAggregator {
     fn default() -> Self {
         Self::new()
     }
 }
 
 /// Compute Pearson correlation coefficient between two variables.
 ///
@@ -328,122 +324,128 @@ fn compute_correlation(pairs: &[(f32, f32)]) -> f32 {
     let mut cov = 0.0;
     let mut var_x = 0.0;
     let mut var_y = 0.0;
 
     for (x, y) in pairs {
         let dx = x - mean_x;
         let dy = y - mean_y;
         cov += dx * dy;
         var_x += dx * dx;
         var_y += dy * dy;
     }
 
     if var_x < 1e-10 || var_y < 1e-10 {
         return 0.0; // No variance
     }
 
     cov / (var_x * var_y).sqrt()
 }
 
 #[cfg(test)]
 mod tests {
     use super::*;
 
     #[test]
     fn test_feedback_record_creation() {
-        let record = FeedbackRecord::new(
-            [1.0, 0.0, 0.0],
-            Some(ColorClass::Red),
-            0.5,
-            0.3,
-            10,
-        );
+        let record = FeedbackRecord::new([1.0, 0.0, 0.0], Some(ColorClass::Red), 0.5, 0.3, 10);
 
         assert_eq!(record.chroma_signature, [1.0, 0.0, 0.0]);
         assert_eq!(record.class_label, Some(ColorClass::Red));
         assert_eq!(record.loss_before, 0.5);
         assert_eq!(record.loss_after, 0.3);
         assert!((record.delta_loss + 0.2).abs() < 0.01); // Loss decreased (floating point)
         assert!(record.utility > 0.0); // Positive utility (helpful)
         assert!(record.was_helpful());
         assert!(!record.was_harmful());
     }
 
     #[test]
     fn test_feedback_record_harmful() {
-        let record = FeedbackRecord::new(
-            [0.0, 1.0, 0.0],
-            Some(ColorClass::Green),
-            0.3,
-            0.5,
-            10,
-        );
+        let record = FeedbackRecord::new([0.0, 1.0, 0.0], Some(ColorClass::Green), 0.3, 0.5, 10);
 
         assert!((record.delta_loss - 0.2).abs() < 0.01); // Loss increased (floating point)
         assert!(record.utility < 0.0); // Negative utility (harmful)
         assert!(!record.was_helpful());
         assert!(record.was_harmful());
     }
 
     #[test]
     fn test_utility_aggregator_add() {
         let mut agg = UtilityAggregator::new();
         assert!(agg.is_empty());
 
         let record1 = FeedbackRecord::new([1.0, 0.0, 0.0], Some(ColorClass::Red), 0.5, 0.3, 1);
         let record2 = FeedbackRecord::new([0.0, 1.0, 0.0], Some(ColorClass::Green), 0.4, 0.2, 2);
 
         agg.add_record(record1);
         agg.add_record(record2);
 
         assert_eq!(agg.len(), 2);
         assert!(!agg.is_empty());
     }
 
     #[test]
     fn test_utility_aggregator_mean() {
         let mut agg = UtilityAggregator::new();
 
         // Helpful dream (utility = 0.2)
         agg.add_record(FeedbackRecord::new([1.0, 0.0, 0.0], None, 0.5, 0.3, 1));
         // Harmful dream (utility = -0.2)
         agg.add_record(FeedbackRecord::new([0.0, 1.0, 0.0], None, 0.3, 0.5, 2));
 
         let mean = agg.mean_utility();
         assert!((mean - 0.0).abs() < 0.01); // Should be ~0 (balanced)
     }
 
     #[test]
     fn test_utility_aggregator_class_stats() {
         let mut agg = UtilityAggregator::new();
 
         // Two helpful red dreams
-        agg.add_record(FeedbackRecord::new([1.0, 0.0, 0.0], Some(ColorClass::Red), 0.5, 0.3, 1));
-        agg.add_record(FeedbackRecord::new([0.9, 0.1, 0.0], Some(ColorClass::Red), 0.4, 0.2, 2));
+        agg.add_record(FeedbackRecord::new(
+            [1.0, 0.0, 0.0],
+            Some(ColorClass::Red),
+            0.5,
+            0.3,
+            1,
+        ));
+        agg.add_record(FeedbackRecord::new(
+            [0.9, 0.1, 0.0],
+            Some(ColorClass::Red),
+            0.4,
+            0.2,
+            2,
+        ));
 
         // One harmful green dream
-        agg.add_record(FeedbackRecord::new([0.0, 1.0, 0.0], Some(ColorClass::Green), 0.3, 0.5, 3));
+        agg.add_record(FeedbackRecord::new(
+            [0.0, 1.0, 0.0],
+            Some(ColorClass::Green),
+            0.3,
+            0.5,
+            3,
+        ));
 
         let red_stats = agg.class_stats(ColorClass::Red).unwrap();
         assert_eq!(red_stats.count, 2);
         assert_eq!(red_stats.helpful_count, 2);
         assert_eq!(red_stats.harmful_count, 0);
         assert!(red_stats.mean_utility > 0.0);
 
         let green_stats = agg.class_stats(ColorClass::Green).unwrap();
         assert_eq!(green_stats.count, 1);
         assert_eq!(green_stats.helpful_count, 0);
         assert_eq!(green_stats.harmful_count, 1);
         assert!(green_stats.mean_utility < 0.0);
     }
 
     #[test]
     fn test_top_k_helpful() {
         let mut agg = UtilityAggregator::new();
 
         agg.add_record(FeedbackRecord::new([1.0, 0.0, 0.0], None, 0.5, 0.1, 1)); // utility = 0.4
         agg.add_record(FeedbackRecord::new([0.0, 1.0, 0.0], None, 0.5, 0.3, 2)); // utility = 0.2
         agg.add_record(FeedbackRecord::new([0.0, 0.0, 1.0], None, 0.5, 0.4, 3)); // utility = 0.1
 
         let top2 = agg.top_k_helpful(2);
         assert_eq!(top2.len(), 2);
         assert!(top2[0].utility > top2[1].utility);
diff --git a/src/learner/mod.rs b/src/learner/mod.rs
index ef4432f40a37076438394ab05f41ce58f893421a..1c914c3764522a3a98c4ad7532109bd1786edc4d 100644
--- a/src/learner/mod.rs
+++ b/src/learner/mod.rs
@@ -1,20 +1,20 @@
 //! Learner module - analytical half of the Dreamer-Learner system
 //!
 //! The Learner extracts structure from Dream Pool entries, evaluates utility,
 //! and provides feedback to bias the Dreamer via the retrieval mechanism.
 //!
 //! This is the Minimal Viable Learner (MVP) that implements:
 //! - Color classification via MLP
 //! - Gradient descent training
 //! - Dream Pool integration for retrieval-based seeding
 //! - Basic feedback collection (Δloss tracking)
 //!
 //! Future expansion path to full LEARNER MANIFEST v1.0 features.
 
 pub mod classifier;
 pub mod feedback;
 pub mod training;
 
-pub use classifier::{ColorClassifier, MLPClassifier, ClassifierConfig};
-pub use feedback::{FeedbackRecord, UtilityAggregator, ClassUtilityStats};
-pub use training::{TrainingConfig, TrainingResult, train_with_dreams};
+pub use classifier::{ClassifierConfig, ColorClassifier, MLPClassifier};
+pub use feedback::{ClassUtilityStats, FeedbackRecord, UtilityAggregator};
+pub use training::{train_with_dreams, TrainingConfig, TrainingResult};
diff --git a/src/learner/training.rs b/src/learner/training.rs
index d5d62daad95d689c86e42b686d3e6c0746105413..fca080d35427100921c28ff88353b192a543a77c 100644
--- a/src/learner/training.rs
+++ b/src/learner/training.rs
@@ -1,282 +1,512 @@
 //! Training loop with Dream Pool integration
 //!
 //! Implements gradient descent training with optional retrieval-based seeding
 //! from the Dream Pool to accelerate convergence.
 
-use crate::data::ColorSample;
-use crate::dream::{RetrievalMode, SimpleDreamPool};
+use crate::data::{ColorClass, ColorSample};
+use crate::dream::soft_index::{EntryId, Similarity};
+use crate::dream::{
+    BiasProfile, EmbeddingMapper, QuerySignature, RetrievalMode, RetrievalWeights, SimpleDreamPool,
+};
 use crate::learner::classifier::{ColorClassifier, MLPClassifier};
+use crate::learner::feedback::{FeedbackRecord, UtilityAggregator};
 use crate::solver::Solver;
-use crate::tensor::ChromaticTensor;
+use crate::spectral::{extract_spectral_features, WindowFunction};
 use crate::tensor::operations::mix;
-use serde::{Serialize, Deserialize};
+use crate::tensor::ChromaticTensor;
+use serde::{Deserialize, Serialize};
+use std::collections::{HashMap, HashSet};
 use std::time::Instant;
 
 /// Training configuration
 #[derive(Debug, Clone, Serialize, Deserialize)]
 pub struct TrainingConfig {
     /// Number of training epochs
     pub num_epochs: usize,
     /// Batch size for training
     pub batch_size: usize,
     /// Learning rate
     pub learning_rate: f32,
     /// Learning rate decay per epoch
     pub lr_decay: f32,
     /// Whether to use Dream Pool retrieval
     pub use_dream_pool: bool,
     /// Number of dreams to retrieve per sample
     pub num_dreams_retrieve: usize,
     /// Retrieval mode: Hard (Phase 3B), Soft (Phase 4), or Hybrid
     pub retrieval_mode: RetrievalMode,
     /// Seed for reproducibility
     pub seed: u64,
+    /// Soft retrieval configuration (Phase 4)
+    pub soft: SoftRetrievalConfig,
+}
+
+/// Configuration specific to soft indexing retrieval.
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub struct SoftRetrievalConfig {
+    pub embed_dim: usize,
+    pub similarity: Similarity,
+    pub weights: RetrievalWeights,
+    pub refresh_interval_steps: usize,
+    pub drift_threshold: f32,
 }
 
 impl Default for TrainingConfig {
     fn default() -> Self {
         Self {
             num_epochs: 50,
             batch_size: 32,
             learning_rate: 0.01,
             lr_decay: 0.95,
             use_dream_pool: false,
             num_dreams_retrieve: 3,
             retrieval_mode: RetrievalMode::default(),
             seed: 42,
+            soft: SoftRetrievalConfig::default(),
+        }
+    }
+}
+
+impl Default for SoftRetrievalConfig {
+    fn default() -> Self {
+        Self {
+            embed_dim: 64,
+            similarity: Similarity::Cosine,
+            weights: RetrievalWeights::default(),
+            refresh_interval_steps: 500,
+            drift_threshold: 0.08,
         }
     }
 }
 
 /// Training metrics for a single epoch
 #[derive(Debug, Clone, Serialize, Deserialize)]
 pub struct EpochMetrics {
     pub epoch: usize,
     pub train_loss: f32,
     pub train_accuracy: f32,
     pub val_loss: f32,
     pub val_accuracy: f32,
     pub learning_rate: f32,
     pub elapsed_ms: u128,
     pub dreams_used: usize,
+    pub unique_dreams: usize,
 }
 
 /// Complete training result
 #[derive(Debug, Clone, Serialize)]
 pub struct TrainingResult {
     pub config: TrainingConfig,
     pub epoch_metrics: Vec<EpochMetrics>,
     pub final_train_accuracy: f32,
     pub final_val_accuracy: f32,
     pub total_elapsed_ms: u128,
     pub converged_epoch: Option<usize>,
 }
 
 /// Compute accuracy on a dataset
-fn compute_accuracy<C: ColorClassifier>(
-    classifier: &C,
-    samples: &[ColorSample],
-) -> f32 {
+fn compute_accuracy<C: ColorClassifier>(classifier: &C, samples: &[ColorSample]) -> f32 {
     if samples.is_empty() {
         return 0.0;
     }
 
     let correct = samples
         .iter()
         .filter(|sample| {
             let predicted = classifier.predict(&sample.tensor);
             predicted == sample.label
         })
         .count();
 
     correct as f32 / samples.len() as f32
 }
 
 /// Augment a tensor with retrieved dreams from the pool
 fn augment_with_dreams(
     tensor: &ChromaticTensor,
+    label: ColorClass,
     pool: &SimpleDreamPool,
     num_dreams: usize,
-) -> ChromaticTensor {
-    if pool.is_empty() {
-        return tensor.clone();
+    retrieval_mode: RetrievalMode,
+    mapper: Option<&EmbeddingMapper>,
+    weights: &RetrievalWeights,
+    similarity: Similarity,
+) -> (ChromaticTensor, Vec<crate::dream::simple_pool::DreamEntry>) {
+    if pool.is_empty() || num_dreams == 0 {
+        return (tensor.clone(), Vec::new());
+    }
+
+    let spectral = extract_spectral_features(tensor, WindowFunction::Hann);
+    let query = QuerySignature {
+        chroma: tensor.mean_rgb(),
+        class_hint: Some(label),
+        spectral: Some(spectral),
+        utility_prior: None,
+        delta_loss_hint: None,
+    };
+
+    let mut dreams: Vec<crate::dream::simple_pool::DreamEntry> = Vec::new();
+
+    match retrieval_mode {
+        RetrievalMode::Hard => {
+            let mut hard = pool.retrieve_similar_class(&query.chroma, label, num_dreams);
+            if hard.is_empty() {
+                hard = pool.retrieve_similar(&query.chroma, num_dreams);
+            }
+            dreams.extend(hard.into_iter());
+        }
+        RetrievalMode::Soft => {
+            if mapper.is_some() {
+                dreams.extend(
+                    pool.retrieve_soft_with_scores(&query, num_dreams, weights, similarity)
+                        .into_iter()
+                        .map(|(_, entry, _)| entry),
+                );
+            }
+        }
+        RetrievalMode::Hybrid => {
+            let mut combined: HashMap<EntryId, (crate::dream::simple_pool::DreamEntry, f32)> =
+                HashMap::new();
+
+            if mapper.is_some() {
+                for (id, entry, score) in
+                    pool.retrieve_soft_with_scores(&query, num_dreams * 2, weights, similarity)
+                {
+                    combined.insert(id, (entry, score));
+                }
+            }
+
+            let hard_primary = pool.retrieve_similar_class(&query.chroma, label, num_dreams);
+            let hard_secondary = pool.retrieve_similar(&query.chroma, num_dreams);
+
+            for entry in hard_primary.into_iter().chain(hard_secondary.into_iter()) {
+                let score = rgb_cosine(&query.chroma, &entry.chroma_signature);
+                combined
+                    .entry(entry.id)
+                    .and_modify(|existing| {
+                        if score > existing.1 {
+                            existing.0 = entry.clone();
+                            existing.1 = score;
+                        }
+                    })
+                    .or_insert((entry, score));
+            }
+
+            let mut merged: Vec<(crate::dream::simple_pool::DreamEntry, f32)> =
+                combined.into_iter().map(|(_, value)| value).collect();
+
+            merged.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));
+            dreams.extend(merged.into_iter().take(num_dreams).map(|(entry, _)| entry));
+        }
     }
 
-    let query_signature = tensor.mean_rgb();
-    let retrieved = pool.retrieve_similar(&query_signature, num_dreams);
+    dreams.truncate(num_dreams);
 
-    if retrieved.is_empty() {
-        return tensor.clone();
+    let mut augmented = tensor.clone();
+    for dream in &dreams {
+        augmented = mix(&augmented, &dream.tensor);
     }
 
-    // Mix original tensor with retrieved dreams
-    let mut result = tensor.clone();
-    for entry in retrieved {
-        result = mix(&result, &entry.tensor);
+    (augmented, dreams)
+}
+
+fn rgb_cosine(a: &[f32; 3], b: &[f32; 3]) -> f32 {
+    let dot = a[0] * b[0] + a[1] * b[1] + a[2] * b[2];
+    let norm_a = (a[0] * a[0] + a[1] * a[1] + a[2] * a[2]).sqrt();
+    let norm_b = (b[0] * b[0] + b[1] * b[1] + b[2] * b[2]).sqrt();
+
+    if norm_a <= 1e-6 || norm_b <= 1e-6 {
+        0.0
+    } else {
+        (dot / (norm_a * norm_b)).clamp(-1.0, 1.0)
     }
+}
+
+fn bias_drift(a: &BiasProfile, b: &BiasProfile) -> f32 {
+    let mut classes: HashSet<String> = a.class_biases.keys().cloned().collect();
+    classes.extend(b.class_biases.keys().cloned());
 
-    result
+    classes
+        .into_iter()
+        .map(|class| {
+            let aw = a.class_biases.get(&class).map(|c| c.weight).unwrap_or(0.0);
+            let bw = b.class_biases.get(&class).map(|c| c.weight).unwrap_or(0.0);
+            (aw - bw).abs()
+        })
+        .fold(0.0, f32::max)
 }
 
 /// Train a classifier with optional Dream Pool augmentation
 pub fn train_with_dreams<S: Solver>(
     mut classifier: MLPClassifier,
     train_data: &[ColorSample],
     val_data: &[ColorSample],
     config: TrainingConfig,
     mut pool: Option<&mut SimpleDreamPool>,
     mut solver: Option<&mut S>,
 ) -> TrainingResult {
     let start_time = Instant::now();
     let mut epoch_metrics = Vec::new();
 
     let mut current_lr = config.learning_rate;
     let mut converged_epoch = None;
 
+    let mut aggregator = UtilityAggregator::new();
+    let mut bias_profile: Option<BiasProfile> = None;
+    let mut steps_since_refresh = 0usize;
+    let mapper = if config.use_dream_pool && config.retrieval_mode.uses_soft_retrieval() {
+        Some(EmbeddingMapper::new(config.soft.embed_dim))
+    } else {
+        None
+    };
+
+    if config.use_dream_pool && config.retrieval_mode.uses_soft_retrieval() {
+        if let (Some(pool_ref), Some(mapper_ref)) = (pool.as_mut(), mapper.as_ref()) {
+            pool_ref.rebuild_soft_index(mapper_ref);
+        }
+    }
+
     for epoch in 0..config.num_epochs {
         let epoch_start = Instant::now();
         let mut epoch_loss = 0.0;
         let mut num_batches = 0;
-        let mut dreams_used = 0;
+        let mut dreams_used = 0usize;
+        let mut unique_ids_epoch: HashSet<EntryId> = HashSet::new();
 
-        // Shuffle training data (simple deterministic shuffle based on epoch)
         let mut indices: Vec<usize> = (0..train_data.len()).collect();
         indices.sort_by_key(|&i| (i + epoch * 997) % train_data.len());
 
-        // Process batches
         for batch_start in (0..train_data.len()).step_by(config.batch_size) {
             let batch_end = (batch_start + config.batch_size).min(train_data.len());
             let batch_indices = &indices[batch_start..batch_end];
 
-            let mut batch_tensors = Vec::new();
+            if config.use_dream_pool && config.retrieval_mode.uses_soft_retrieval() {
+                if let (Some(pool_mut), Some(mapper_ref)) = (pool.as_mut(), mapper.as_ref()) {
+                    if !pool_mut.has_soft_index() {
+                        pool_mut.rebuild_soft_index(mapper_ref);
+                    }
+                }
+            }
+
+            let mut original_tensors = Vec::new();
+            let mut augmented_tensors = Vec::new();
             let mut batch_labels = Vec::new();
+            let mut batch_dreams: Vec<Vec<crate::dream::simple_pool::DreamEntry>> = Vec::new();
 
             for &idx in batch_indices {
                 let sample = &train_data[idx];
+                original_tensors.push(sample.tensor.clone());
+                batch_labels.push(sample.label);
 
-                // Augment with dreams if enabled
-                let tensor = if config.use_dream_pool {
-                    if let Some(pool) = pool.as_ref() {
-                        dreams_used += 1;
-                        augment_with_dreams(&sample.tensor, pool, config.num_dreams_retrieve)
+                if config.use_dream_pool {
+                    if let Some(pool_ref) = pool.as_ref() {
+                        let pool_view: &SimpleDreamPool = &*(*pool_ref);
+                        let (augmented, dreams) = augment_with_dreams(
+                            &sample.tensor,
+                            sample.label,
+                            pool_view,
+                            config.num_dreams_retrieve,
+                            config.retrieval_mode,
+                            mapper.as_ref(),
+                            &config.soft.weights,
+                            config.soft.similarity,
+                        );
+                        dreams_used += dreams.len();
+                        for dream in &dreams {
+                            unique_ids_epoch.insert(dream.id);
+                        }
+                        augmented_tensors.push(augmented);
+                        batch_dreams.push(dreams);
                     } else {
-                        sample.tensor.clone()
+                        augmented_tensors.push(sample.tensor.clone());
+                        batch_dreams.push(Vec::new());
                     }
                 } else {
-                    sample.tensor.clone()
-                };
-
-                batch_tensors.push(tensor);
-                batch_labels.push(sample.label);
+                    augmented_tensors.push(sample.tensor.clone());
+                    batch_dreams.push(Vec::new());
+                }
             }
 
-            // Compute loss and gradients
-            let (loss, gradients) = classifier.compute_loss(&batch_tensors, &batch_labels);
+            let baseline_loss =
+                if config.use_dream_pool && batch_dreams.iter().any(|dreams| !dreams.is_empty()) {
+                    Some(classifier.compute_loss(&original_tensors, &batch_labels).0)
+                } else {
+                    None
+                };
+
+            let (loss, gradients) = classifier.compute_loss(&augmented_tensors, &batch_labels);
             epoch_loss += loss;
             num_batches += 1;
 
-            // Update weights
             classifier.update_weights(&gradients, current_lr);
 
-            // Store augmented tensors in pool if using solver evaluation
             if config.use_dream_pool {
-                if let (Some(pool), Some(solver)) = (pool.as_mut(), solver.as_mut()) {
-                    for tensor in batch_tensors {
-                        if let Ok(result) = solver.evaluate(&tensor, false) {
-                            pool.add_if_coherent(tensor, result);
+                if let Some(loss_before) = baseline_loss {
+                    if let Some(pool_mut) = pool.as_mut() {
+                        for dreams in &batch_dreams {
+                            for dream in dreams {
+                                let mut record = FeedbackRecord::new(
+                                    dream.chroma_signature,
+                                    dream.class_label,
+                                    loss_before,
+                                    loss,
+                                    epoch,
+                                );
+                                if let Some(features) = &dream.spectral_features {
+                                    record = record.with_spectral_features(features.clone());
+                                }
+                                let utility = record.utility;
+                                aggregator.add_record(record);
+                                pool_mut.update_entry_feedback(dream.id, utility);
+                            }
+                        }
+                    } else {
+                        for dreams in &batch_dreams {
+                            for dream in dreams {
+                                let mut record = FeedbackRecord::new(
+                                    dream.chroma_signature,
+                                    dream.class_label,
+                                    loss_before,
+                                    loss,
+                                    epoch,
+                                );
+                                if let Some(features) = &dream.spectral_features {
+                                    record = record.with_spectral_features(features.clone());
+                                }
+                                aggregator.add_record(record);
+                            }
+                        }
+                    }
+                }
+
+                if let (Some(pool_mut), Some(solver_mut)) = (pool.as_mut(), solver.as_mut()) {
+                    for tensor in augmented_tensors.drain(..) {
+                        if let Ok(result) = solver_mut.evaluate(&tensor, false) {
+                            pool_mut.add_if_coherent(tensor, result);
+                        }
+                    }
+                }
+
+                if config.retrieval_mode.uses_soft_retrieval() {
+                    steps_since_refresh += 1;
+                    if steps_since_refresh >= config.soft.refresh_interval_steps
+                        && !aggregator.is_empty()
+                    {
+                        let new_bias = BiasProfile::from_aggregator(&aggregator, 0.0);
+                        let drift = bias_profile
+                            .as_ref()
+                            .map(|existing| bias_drift(existing, &new_bias))
+                            .unwrap_or(f32::INFINITY);
+
+                        if drift > config.soft.drift_threshold {
+                            bias_profile = Some(new_bias.clone());
+                            if let Some(pool_mut) = pool.as_mut() {
+                                pool_mut.set_bias_profile(Some(new_bias.clone()));
+                                if let Some(mapper_ref) = mapper.as_ref() {
+                                    pool_mut.rebuild_soft_index(mapper_ref);
+                                }
+                            }
                         }
+
+                        aggregator.clear();
+                        steps_since_refresh = 0;
                     }
                 }
+            } else {
+                augmented_tensors.clear();
             }
         }
 
-        // Compute epoch metrics
         let avg_train_loss = if num_batches > 0 {
             epoch_loss / num_batches as f32
         } else {
             0.0
         };
 
         let train_accuracy = compute_accuracy(&classifier, train_data);
         let val_accuracy = compute_accuracy(&classifier, val_data);
 
-        // Compute validation loss
         let val_tensors: Vec<_> = val_data.iter().map(|s| s.tensor.clone()).collect();
         let val_labels: Vec<_> = val_data.iter().map(|s| s.label).collect();
         let (val_loss, _) = classifier.compute_loss(&val_tensors, &val_labels);
 
         epoch_metrics.push(EpochMetrics {
             epoch,
             train_loss: avg_train_loss,
             train_accuracy,
             val_loss,
             val_accuracy,
             learning_rate: current_lr,
             elapsed_ms: epoch_start.elapsed().as_millis(),
             dreams_used,
+            unique_dreams: unique_ids_epoch.len(),
         });
 
-        // Check for convergence (95% val accuracy)
         if converged_epoch.is_none() && val_accuracy >= 0.95 {
             converged_epoch = Some(epoch);
         }
 
-        // Learning rate decay
         current_lr *= config.lr_decay;
     }
 
-    let final_train_accuracy = epoch_metrics.last().map(|m| m.train_accuracy).unwrap_or(0.0);
+    let final_train_accuracy = epoch_metrics
+        .last()
+        .map(|m| m.train_accuracy)
+        .unwrap_or(0.0);
     let final_val_accuracy = epoch_metrics.last().map(|m| m.val_accuracy).unwrap_or(0.0);
 
+    if config.use_dream_pool && config.retrieval_mode.uses_soft_retrieval() {
+        if let (Some(pool_mut), Some(mapper_ref)) = (pool.as_mut(), mapper.as_ref()) {
+            if !pool_mut.has_soft_index() {
+                pool_mut.rebuild_soft_index(mapper_ref);
+            }
+        }
+    }
+
     TrainingResult {
         config,
         epoch_metrics,
         final_train_accuracy,
         final_val_accuracy,
         total_elapsed_ms: start_time.elapsed().as_millis(),
         converged_epoch,
     }
 }
 
 /// Simple training without Dream Pool (for baseline comparison)
 pub fn train_baseline(
     classifier: MLPClassifier,
     train_data: &[ColorSample],
     val_data: &[ColorSample],
     config: TrainingConfig,
 ) -> TrainingResult {
     let mut config = config;
     config.use_dream_pool = false;
 
     train_with_dreams::<crate::ChromaticNativeSolver>(
-        classifier,
-        train_data,
-        val_data,
-        config,
-        None,
-        None,
+        classifier, train_data, val_data, config, None, None,
     )
 }
 
 #[cfg(test)]
 mod tests {
     use super::*;
     use crate::data::{ColorDataset, DatasetConfig};
     use crate::dream::simple_pool::PoolConfig;
     use crate::learner::classifier::ClassifierConfig;
     use crate::ChromaticNativeSolver;
 
     #[test]
     fn test_compute_accuracy() {
         let config = ClassifierConfig::default();
         let classifier = MLPClassifier::new(config);
 
         let dataset_config = DatasetConfig {
             tensor_size: (16, 16, 4),
             samples_per_class: 10,
             ..Default::default()
         };
         let dataset = ColorDataset::generate(dataset_config);
 
         let accuracy = compute_accuracy(&classifier, &dataset.samples);
 
@@ -370,31 +600,128 @@ mod tests {
         assert!(result.final_val_accuracy >= 0.0);
 
         // Pool should have accumulated some dreams
         assert!(pool.len() > 0);
     }
 
     #[test]
     fn test_augment_with_dreams() {
         let pool_config = PoolConfig {
             coherence_threshold: 0.0,
             max_size: 10,
             ..Default::default()
         };
         let mut pool = SimpleDreamPool::new(pool_config);
 
         // Add some dreams to pool
         let mut solver = ChromaticNativeSolver::default();
         for i in 0..5 {
             let tensor = ChromaticTensor::from_seed(i, 16, 16, 4);
             let result = solver.evaluate(&tensor, false).unwrap();
             pool.add(tensor, result);
         }
 
         // Augment a tensor
         let query_tensor = ChromaticTensor::from_seed(100, 16, 16, 4);
-        let augmented = augment_with_dreams(&query_tensor, &pool, 3);
+        let (augmented, dreams) = augment_with_dreams(
+            &query_tensor,
+            ColorClass::Red,
+            &pool,
+            3,
+            RetrievalMode::Hard,
+            None,
+            &RetrievalWeights::default(),
+            Similarity::Cosine,
+        );
 
         // Augmented tensor should have different mean RGB
         assert_ne!(query_tensor.mean_rgb(), augmented.mean_rgb());
+        assert!(!dreams.is_empty());
+    }
+
+    #[test]
+    fn test_soft_retrieval_rebuilds_index_after_feedback() {
+        use crate::dream::simple_pool::SimpleDreamPool;
+        use crate::solver::SolverResult;
+        use crate::tensor::ChromaticTensor;
+        use serde_json::json;
+
+        let dataset_config = DatasetConfig {
+            tensor_size: (8, 8, 2),
+            samples_per_class: 2,
+            noise_level: 0.02,
+            seed: 7,
+        };
+        let dataset = ColorDataset::generate(dataset_config);
+        let train_data = dataset.samples.clone();
+        let val_data = dataset.samples.clone();
+
+        let classifier_config = ClassifierConfig {
+            input_size: 8 * 8 * 2 * 3,
+            hidden_size: 16,
+            output_size: 10,
+            seed: 11,
+        };
+        let classifier = MLPClassifier::new(classifier_config);
+
+        let soft_config = SoftRetrievalConfig {
+            embed_dim: 32,
+            similarity: Similarity::Cosine,
+            weights: RetrievalWeights::default(),
+            refresh_interval_steps: 1,
+            drift_threshold: 0.01,
+        };
+
+        let training_config = TrainingConfig {
+            num_epochs: 1,
+            batch_size: 2,
+            learning_rate: 0.01,
+            lr_decay: 1.0,
+            use_dream_pool: true,
+            num_dreams_retrieve: 1,
+            retrieval_mode: RetrievalMode::Soft,
+            seed: 5,
+            soft: soft_config,
+        };
+
+        let mut pool = SimpleDreamPool::new(PoolConfig {
+            max_size: 16,
+            coherence_threshold: 0.0,
+            retrieval_limit: 3,
+        });
+
+        let solver_result = SolverResult {
+            energy: 0.1,
+            coherence: 0.9,
+            violation: 0.05,
+            grad: None,
+            mask: None,
+            meta: json!({}),
+        };
+
+        for seed in 0..3 {
+            let tensor = ChromaticTensor::from_seed(seed as u64 + 13, 8, 8, 2);
+            let class = ColorClass::from_index(seed).unwrap_or(ColorClass::Red);
+            pool.add_with_class(tensor, solver_result.clone(), class);
+        }
+
+        let mapper = EmbeddingMapper::new(training_config.soft.embed_dim);
+        pool.rebuild_soft_index(&mapper);
+        assert!(pool.has_soft_index());
+
+        let mut solver = ChromaticNativeSolver::default();
+        let _ = train_with_dreams(
+            classifier,
+            &train_data,
+            &val_data,
+            training_config,
+            Some(&mut pool),
+            Some(&mut solver),
+        );
+
+        assert!(pool.has_soft_index());
+        assert_eq!(pool.soft_index_size(), pool.len());
+
+        let retrieved = pool.retrieve_similar(&train_data[0].tensor.mean_rgb(), pool.len());
+        assert!(retrieved.iter().any(|entry| entry.feedback_count > 0));
     }
 }
diff --git a/src/lib.rs b/src/lib.rs
index 7a4731428e2fca4c52628e704ee24c34123a9e21..26c8b51cdc05b1ff855cd264c1b2083966b53aa9 100644
--- a/src/lib.rs
+++ b/src/lib.rs
@@ -20,37 +20,39 @@
 //! let saturated = saturate(&complemented, 1.25);
 //!
 //! // Get statistics
 //! let stats = saturated.statistics();
 //! println!("Mean RGB: {:?}", stats.mean_rgb);
 //! ```
 //!
 //! ## Core Modules
 //!
 //! - [`config`] - Engine configuration via TOML
 //! - [`tensor`] - Chromatic tensor types and operations
 //! - [`logging`] - JSON line-delimited logging
 //! - [`training`] - Loss functions and training metrics
 
 pub mod config;
 pub mod data;
 pub mod dream;
 pub mod learner;
 pub mod logging;
 pub mod neural;
 pub mod solver;
 pub mod spectral;
 pub mod tensor;
 pub mod training;
 
-pub use config::EngineConfig;
-pub use dream::{SimpleDreamPool, BiasProfile, ClassBias, SpectralBias, ChromaBias};
-pub use learner::{ColorClassifier, MLPClassifier, ClassifierConfig};
-pub use learner::feedback::{FeedbackRecord, UtilityAggregator, ClassUtilityStats};
-pub use learner::training::{TrainingConfig, TrainingResult, train_with_dreams, train_baseline};
-pub use solver::{Solver, SolverResult};
+pub use config::{EngineConfig, Phase4Config};
+pub use dream::{BiasProfile, ChromaBias, ClassBias, SimpleDreamPool, SpectralBias};
+pub use learner::feedback::{ClassUtilityStats, FeedbackRecord, UtilityAggregator};
+pub use learner::training::{train_baseline, train_with_dreams, TrainingConfig, TrainingResult};
+pub use learner::{ClassifierConfig, ColorClassifier, MLPClassifier};
 pub use solver::native::ChromaticNativeSolver;
-pub use spectral::{extract_spectral_features, compute_spectral_entropy, SpectralFeatures, WindowFunction};
-pub use tensor::ChromaticTensor;
+pub use solver::{Solver, SolverResult};
+pub use spectral::{
+    compute_spectral_entropy, extract_spectral_features, SpectralFeatures, WindowFunction,
+};
 pub use tensor::gradient::GradientLayer;
 pub use tensor::operations::{complement, filter, mix, saturate};
-pub use training::{TrainingMetrics, mse_loss};
+pub use tensor::ChromaticTensor;
+pub use training::{mse_loss, TrainingMetrics};
diff --git a/src/neural/layer.rs b/src/neural/layer.rs
index 6e89dd18fef9ad405f6ec1689028223f294d7024..23bb8b03472b2d1b252d3bd395c57f2c9ab7f47a 100644
--- a/src/neural/layer.rs
+++ b/src/neural/layer.rs
@@ -1,28 +1,30 @@
 //! Neural network layer abstractions for chromatic tensors.
 
-use crate::neural::gradient::{backward_complement, backward_filter, backward_mix, backward_saturate};
+use crate::neural::gradient::{
+    backward_complement, backward_filter, backward_mix, backward_saturate,
+};
 use crate::tensor::ChromaticTensor;
 
 /// Type of chromatic operation performed by a layer.
 #[derive(Debug, Clone, Copy, PartialEq)]
 pub enum ChromaticOp {
     /// Additive mix operation
     Mix,
     /// Subtractive filter operation
     Filter,
     /// Hue complement operation
     Complement,
     /// Saturation adjustment operation
     Saturate,
 }
 
 /// A chromatic neural network layer.
 ///
 /// Performs learned transformations on chromatic tensors using color-space operations.
 ///
 /// # Architecture
 ///
 /// ```text
 /// input → mix(input, weights) → operation → add(bias) → output
 /// ```
 pub struct ChromaticLayer {
@@ -36,57 +38,51 @@ pub struct ChromaticLayer {
     pub param: f32,
     /// Cached input for backward pass
     cached_input: Option<ChromaticTensor>,
     /// Cached pre-operation output for backward pass
     cached_pre_op: Option<ChromaticTensor>,
 }
 
 impl ChromaticLayer {
     /// Creates a new chromatic layer with random initialization.
     ///
     /// # Arguments
     ///
     /// * `rows` - Tensor height
     /// * `cols` - Tensor width
     /// * `layers` - Tensor depth
     /// * `operation` - Type of chromatic operation
     /// * `seed` - Random seed for initialization
     ///
     /// # Examples
     ///
     /// ```
     /// use chromatic_cognition_core::neural::{ChromaticLayer, ChromaticOp};
     ///
     /// let layer = ChromaticLayer::new(32, 32, 8, ChromaticOp::Mix, 42);
     /// ```
-    pub fn new(
-        rows: usize,
-        cols: usize,
-        layers: usize,
-        operation: ChromaticOp,
-        seed: u64,
-    ) -> Self {
+    pub fn new(rows: usize, cols: usize, layers: usize, operation: ChromaticOp, seed: u64) -> Self {
         Self {
             weights: ChromaticTensor::from_seed(seed, rows, cols, layers),
             bias: ChromaticTensor::from_seed(seed.wrapping_add(1000), rows, cols, layers),
             operation,
             param: 1.0, // Default parameter
             cached_input: None,
             cached_pre_op: None,
         }
     }
 
     /// Sets the operation parameter (e.g., saturation alpha).
     pub fn with_param(mut self, param: f32) -> Self {
         self.param = param;
         self
     }
 
     /// Forward pass through the layer.
     ///
     /// Computes: operation(mix(input, weights) + bias)
     ///
     /// # Arguments
     ///
     /// * `input` - Input chromatic tensor
     ///
     /// # Returns
diff --git a/src/neural/loss.rs b/src/neural/loss.rs
index f2f057f42dfbbf6eb8708f578708090aa82860a8..e60332556f12d66ca97307ac60227392e80132d3 100644
--- a/src/neural/loss.rs
+++ b/src/neural/loss.rs
@@ -34,74 +34,88 @@ pub fn mse_loss_with_gradients(
 /// Takes the mean color of the final tensor as logits for each class.
 ///
 /// # Arguments
 ///
 /// * `predicted` - Predicted chromatic tensor (final layer)
 /// * `label` - Target class label
 /// * `num_classes` - Total number of classes
 ///
 /// # Returns
 ///
 /// Tuple of (loss value, gradient w.r.t. predicted colors)
 pub fn cross_entropy_loss(
     predicted: &ChromaticTensor,
     label: usize,
     num_classes: usize,
 ) -> (f32, ChromaticTensor) {
     // Extract logits from mean RGB values
     let stats = predicted.statistics();
     let logits = stats.mean_rgb;
 
     // For 3-class problem, use RGB channels as logits
     // For other cases, we'd need to extend this
     assert!(num_classes <= 3, "Currently only supports up to 3 classes");
 
     // Softmax
-    let max_logit = logits[..num_classes].iter().cloned().fold(f32::NEG_INFINITY, f32::max);
-    let exp_sum: f32 = logits[..num_classes].iter().map(|&x| (x - max_logit).exp()).sum();
-    let probs: Vec<f32> = logits[..num_classes].iter().map(|&x| (x - max_logit).exp() / exp_sum).collect();
+    let max_logit = logits[..num_classes]
+        .iter()
+        .cloned()
+        .fold(f32::NEG_INFINITY, f32::max);
+    let exp_sum: f32 = logits[..num_classes]
+        .iter()
+        .map(|&x| (x - max_logit).exp())
+        .sum();
+    let probs: Vec<f32> = logits[..num_classes]
+        .iter()
+        .map(|&x| (x - max_logit).exp() / exp_sum)
+        .collect();
 
     // Cross-entropy loss
     let loss = -probs[label].ln();
 
     // Gradient: softmax gradient
     let mut grad_logits = [0.0f32; 3];
     for i in 0..num_classes {
         if i == label {
             grad_logits[i] = probs[i] - 1.0;
         } else {
             grad_logits[i] = probs[i];
         }
     }
 
     // Scale gradient back to tensor
     // Gradient is uniform across all spatial locations and layers
     let (rows, cols, layers, _) = predicted.shape();
     let n = (rows * cols * layers) as f32;
 
     let mut gradient = predicted.clone();
-    for val in gradient.colors.as_slice_mut().expect("contiguous").chunks_exact_mut(3) {
+    for val in gradient
+        .colors
+        .as_slice_mut()
+        .expect("contiguous")
+        .chunks_exact_mut(3)
+    {
         val[0] = grad_logits[0] / n;
         val[1] = grad_logits[1] / n;
         val[2] = grad_logits[2] / n;
     }
 
     (loss, gradient)
 }
 
 /// Computes accuracy for classification.
 ///
 /// # Arguments
 ///
 /// * `predicted` - Predicted chromatic tensor
 /// * `label` - True label
 /// * `num_classes` - Number of classes
 ///
 /// # Returns
 ///
 /// 1.0 if prediction is correct, 0.0 otherwise
 pub fn accuracy(predicted: &ChromaticTensor, label: usize, num_classes: usize) -> f32 {
     let stats = predicted.statistics();
     let logits = stats.mean_rgb;
 
     // Find argmax
     let mut max_idx = 0;
@@ -130,38 +144,43 @@ mod tests {
         let target = ChromaticTensor::from_seed(100, 4, 4, 2);
 
         let (loss, gradient) = mse_loss_with_gradients(&predicted, &target);
 
         assert!(loss > 0.0);
         assert_eq!(gradient.shape(), predicted.shape());
     }
 
     #[test]
     fn test_cross_entropy() {
         let predicted = ChromaticTensor::from_seed(42, 4, 4, 2);
         let label = 0;
         let num_classes = 3;
 
         let (loss, gradient) = cross_entropy_loss(&predicted, label, num_classes);
 
         assert!(loss > 0.0);
         assert_eq!(gradient.shape(), predicted.shape());
     }
 
     #[test]
     fn test_accuracy() {
         let mut tensor = ChromaticTensor::from_seed(42, 4, 4, 2);
 
         // Manually set colors to favor class 0 (red)
-        for val in tensor.colors.as_slice_mut().expect("contiguous").chunks_exact_mut(3) {
+        for val in tensor
+            .colors
+            .as_slice_mut()
+            .expect("contiguous")
+            .chunks_exact_mut(3)
+        {
             val[0] = 1.0; // Red
             val[1] = 0.0; // Green
             val[2] = 0.0; // Blue
         }
 
         let acc = accuracy(&tensor, 0, 3);
         assert_eq!(acc, 1.0);
 
         let acc_wrong = accuracy(&tensor, 1, 3);
         assert_eq!(acc_wrong, 0.0);
     }
 }
diff --git a/src/neural/network.rs b/src/neural/network.rs
index 485a1bb49e94b7ddcb7493c6ddc99ff276967022..b76cff949206bbe50ddc76025efec29bfc394e41 100644
--- a/src/neural/network.rs
+++ b/src/neural/network.rs
@@ -1,63 +1,66 @@
 //! Chromatic neural network architecture.
 
 use crate::neural::layer::{ChromaticLayer, ChromaticOp};
 use crate::neural::loss::{accuracy, cross_entropy_loss};
 use crate::neural::optimizer::SGDOptimizer;
 use crate::tensor::ChromaticTensor;
 
 /// A chromatic neural network for classification.
 ///
 /// Stacks multiple chromatic layers to learn complex color patterns.
 pub struct ChromaticNetwork {
     layers: Vec<ChromaticLayer>,
     num_classes: usize,
 }
 
 impl ChromaticNetwork {
     /// Creates a new chromatic network.
     ///
     /// # Arguments
     ///
     /// * `layers` - Vector of chromatic layers
     /// * `num_classes` - Number of output classes
     pub fn new(layers: Vec<ChromaticLayer>, num_classes: usize) -> Self {
-        Self { layers, num_classes }
+        Self {
+            layers,
+            num_classes,
+        }
     }
 
     /// Creates a simple 2-layer network for experiments.
     ///
     /// # Arguments
     ///
     /// * `input_size` - Size of input tensors (rows, cols, layers)
     /// * `num_classes` - Number of output classes
     /// * `seed` - Random seed
     pub fn simple(input_size: (usize, usize, usize), num_classes: usize, seed: u64) -> Self {
         let (rows, cols, layers) = input_size;
 
-        let layer1 = ChromaticLayer::new(rows, cols, layers, ChromaticOp::Saturate, seed)
-            .with_param(1.2);
+        let layer1 =
+            ChromaticLayer::new(rows, cols, layers, ChromaticOp::Saturate, seed).with_param(1.2);
         let layer2 = ChromaticLayer::new(rows, cols, layers, ChromaticOp::Mix, seed + 1);
 
         Self::new(vec![layer1, layer2], num_classes)
     }
 
     /// Forward pass through the network.
     ///
     /// # Arguments
     ///
     /// * `input` - Input chromatic tensor
     ///
     /// # Returns
     ///
     /// Output chromatic tensor
     pub fn forward(&mut self, input: &ChromaticTensor) -> ChromaticTensor {
         let mut activation = input.clone();
 
         for layer in &mut self.layers {
             activation = layer.forward(&activation);
         }
 
         activation
     }
 
     /// Computes loss and gradients for a single sample.
diff --git a/src/neural/optimizer.rs b/src/neural/optimizer.rs
index eb3ce79340d18f3a3aae84e7b7d8c1d49193c741..0f4ba651486aab4bd8664b167187a082cf295f1e 100644
--- a/src/neural/optimizer.rs
+++ b/src/neural/optimizer.rs
@@ -32,61 +32,62 @@ impl SGDOptimizer {
     /// * `weight_decay` - L2 regularization strength (typically 0.0001)
     ///
     /// # Examples
     ///
     /// ```
     /// use chromatic_cognition_core::neural::SGDOptimizer;
     ///
     /// let optimizer = SGDOptimizer::new(0.01, 0.9, 0.0001);
     /// ```
     pub fn new(learning_rate: f32, momentum: f32, weight_decay: f32) -> Self {
         Self {
             learning_rate,
             momentum,
             weight_decay,
             velocities: HashMap::new(),
         }
     }
 
     /// Updates a parameter using accumulated gradients.
     ///
     /// # Arguments
     ///
     /// * `param_name` - Unique identifier for this parameter
     /// * `param` - Parameter tensor to update (modified in-place)
     /// * `gradient` - Gradient tensor
-    pub fn step(&mut self, param_name: &str, param: &mut ChromaticTensor, gradient: &ChromaticTensor) {
+    pub fn step(
+        &mut self,
+        param_name: &str,
+        param: &mut ChromaticTensor,
+        gradient: &ChromaticTensor,
+    ) {
         // Get or initialize velocity for this parameter
         let velocity = self
             .velocities
             .entry(param_name.to_string())
             .or_insert_with(|| {
-                ChromaticTensor::new(
-                    param.shape().0,
-                    param.shape().1,
-                    param.shape().2,
-                )
+                ChromaticTensor::new(param.shape().0, param.shape().1, param.shape().2)
             });
 
         // Apply weight decay (L2 regularization)
         let mut grad_with_decay = gradient.clone();
         if self.weight_decay > 0.0 {
             grad_with_decay = grad_with_decay + param.clone() * self.weight_decay;
         }
 
         // Update velocity: v = momentum * v + lr * gradient
         *velocity = velocity.clone() * self.momentum + grad_with_decay * self.learning_rate;
 
         // Update parameter: param = param - velocity
         *param = param.clone() - velocity.clone();
     }
 
     /// Resets all accumulated velocities.
     pub fn zero_grad(&mut self) {
         self.velocities.clear();
     }
 }
 
 /// Adam optimizer (Adaptive Moment Estimation).
 ///
 /// Implements adaptive learning rates for each parameter.
 pub struct AdamOptimizer {
@@ -102,88 +103,95 @@ pub struct AdamOptimizer {
     pub weight_decay: f32,
     /// First moment estimates (mean of gradients)
     first_moments: HashMap<String, ChromaticTensor>,
     /// Second moment estimates (variance of gradients)
     second_moments: HashMap<String, ChromaticTensor>,
     /// Time step counter
     t: usize,
 }
 
 impl AdamOptimizer {
     /// Creates a new Adam optimizer.
     pub fn new(learning_rate: f32, weight_decay: f32) -> Self {
         Self {
             learning_rate,
             beta1: 0.9,
             beta2: 0.999,
             epsilon: 1e-8,
             weight_decay,
             first_moments: HashMap::new(),
             second_moments: HashMap::new(),
             t: 0,
         }
     }
 
     /// Updates a parameter using Adam algorithm.
-    pub fn step(&mut self, param_name: &str, param: &mut ChromaticTensor, gradient: &ChromaticTensor) {
+    pub fn step(
+        &mut self,
+        param_name: &str,
+        param: &mut ChromaticTensor,
+        gradient: &ChromaticTensor,
+    ) {
         self.t += 1;
 
         // Get or initialize moments
         let m = self
             .first_moments
             .entry(param_name.to_string())
             .or_insert_with(|| {
                 ChromaticTensor::new(param.shape().0, param.shape().1, param.shape().2)
             });
 
         let v = self
             .second_moments
             .entry(param_name.to_string())
             .or_insert_with(|| {
                 ChromaticTensor::new(param.shape().0, param.shape().1, param.shape().2)
             });
 
         // Apply weight decay
         let mut grad_with_decay = gradient.clone();
         if self.weight_decay > 0.0 {
             grad_with_decay = grad_with_decay + param.clone() * self.weight_decay;
         }
 
         // Update biased first moment: m = beta1 * m + (1 - beta1) * gradient
         *m = m.clone() * self.beta1 + grad_with_decay.clone() * (1.0 - self.beta1);
 
         // Update biased second moment: v = beta2 * v + (1 - beta2) * gradient^2
         let grad_squared = element_wise_square(&grad_with_decay);
         *v = v.clone() * self.beta2 + grad_squared * (1.0 - self.beta2);
 
         // Bias correction
         let m_hat = m.clone() * (1.0 / (1.0 - self.beta1.powi(self.t as i32)));
         let v_hat = v.clone() * (1.0 / (1.0 - self.beta2.powi(self.t as i32)));
 
         // Update parameter: param = param - lr * m_hat / (sqrt(v_hat) + epsilon)
         let v_sqrt = element_wise_sqrt(&v_hat);
-        let denominator = v_sqrt + ChromaticTensor::new(param.shape().0, param.shape().1, param.shape().2) * self.epsilon;
+        let denominator = v_sqrt
+            + ChromaticTensor::new(param.shape().0, param.shape().1, param.shape().2)
+                * self.epsilon;
         let update = element_wise_divide(&m_hat, &denominator) * self.learning_rate;
 
         *param = param.clone() - update;
     }
 
     /// Resets all accumulated moments.
     pub fn zero_grad(&mut self) {
         self.first_moments.clear();
         self.second_moments.clear();
         self.t = 0;
     }
 }
 
 // Helper functions for element-wise operations
 fn element_wise_square(tensor: &ChromaticTensor) -> ChromaticTensor {
     let mut result = tensor.clone();
     for val in result.colors.as_slice_mut().expect("contiguous") {
         *val = *val * *val;
     }
     result
 }
 
 fn element_wise_sqrt(tensor: &ChromaticTensor) -> ChromaticTensor {
     let mut result = tensor.clone();
     for val in result.colors.as_slice_mut().expect("contiguous") {
diff --git a/src/solver/mod.rs b/src/solver/mod.rs
index e860c03072e7f6bf83a9e2aaaab304f096437eb9..a7ebfe92a194ece74f1101afc67469501e46f651 100644
--- a/src/solver/mod.rs
+++ b/src/solver/mod.rs
@@ -1,38 +1,38 @@
 /// Solver trait and implementations for chromatic field evaluation
 ///
 /// This module provides a trait-based interface for evaluating chromatic tensor fields
 /// and computing metrics like energy, coherence, and constraint violations.
-
 pub mod native;
 
 use crate::tensor::ChromaticTensor;
-use serde_json::Value;
 use anyhow::Result;
+use serde::Serialize;
+use serde_json::Value;
 
 /// Result of evaluating a chromatic field
-#[derive(Debug, Clone)]
+#[derive(Debug, Clone, Serialize)]
 pub struct SolverResult {
     /// Total field energy (lower is better)
     /// Combines smoothness (total variation) and saturation penalties
     pub energy: f64,
 
     /// Field coherence score (0-1, higher is better)
     /// Measures color harmony and consistency
     pub coherence: f64,
 
     /// Constraint violation score (0-1, lower is better)
     /// Measures out-of-gamut colors, extreme saturation, discontinuities
     pub violation: f64,
 
     /// Gradient with respect to RGB values (optional)
     /// Length: rows * cols * layers * 3
     pub grad: Option<Vec<f32>>,
 
     /// Per-cell penalty/attention mask (optional)
     /// Length: rows * cols * layers
     pub mask: Option<Vec<f32>>,
 
     /// Additional metadata (timings, diagnostics, etc.)
     pub meta: Value,
 }
 
diff --git a/src/solver/native.rs b/src/solver/native.rs
index ccfed136ed90cfaacda9701d6ff0f3286ea62908..8d64fa2eb6471664bdc0791c9a5406f1b472d439 100644
--- a/src/solver/native.rs
+++ b/src/solver/native.rs
@@ -1,76 +1,85 @@
+use crate::solver::{Solver, SolverResult};
 /// Native Rust implementation of chromatic field solver
 ///
 /// This solver computes color-theory-informed metrics directly without
 /// external dependencies. It provides:
 ///
 /// - **Energy**: Total variation (smoothness) + saturation penalty
 /// - **Coherence**: Color harmony based on complementary balance and hue consistency
 /// - **Violation**: Gamut clipping, extreme saturation, local discontinuities
 ///
 /// All metrics have analytical gradients for efficient training.
-
 use crate::tensor::ChromaticTensor;
-use crate::solver::{Solver, SolverResult};
 use anyhow::Result;
 use serde_json::json;
 
 /// Native Rust solver with color-space metrics
 pub struct ChromaticNativeSolver {
     /// Weight for total variation term in energy
     pub lambda_tv: f32,
 
     /// Weight for saturation penalty term in energy
     pub lambda_sat: f32,
 
     /// Target saturation (0.5 = neutral, deviations penalized)
     pub target_saturation: f32,
 
     /// Threshold for local discontinuity detection (ΔE)
     pub discontinuity_threshold: f32,
 }
 
 impl Default for ChromaticNativeSolver {
     fn default() -> Self {
         Self {
             lambda_tv: 1.0,
             lambda_sat: 0.1,
             target_saturation: 0.5,
             discontinuity_threshold: 0.3,
         }
     }
 }
 
 impl ChromaticNativeSolver {
     /// Create a new native solver with default parameters
     pub fn new() -> Self {
         Self::default()
     }
 
     /// Create a solver with custom parameters
-    pub fn with_params(lambda_tv: f32, lambda_sat: f32, target_saturation: f32, discontinuity_threshold: f32) -> Self {
-        Self { lambda_tv, lambda_sat, target_saturation, discontinuity_threshold }
+    pub fn with_params(
+        lambda_tv: f32,
+        lambda_sat: f32,
+        target_saturation: f32,
+        discontinuity_threshold: f32,
+    ) -> Self {
+        Self {
+            lambda_tv,
+            lambda_sat,
+            target_saturation,
+            discontinuity_threshold,
+        }
     }
 
     /// Compute total variation (spatial smoothness penalty)
     ///
     /// TV(F) = Σ ‖F[i,j,l] - F[i+1,j,l]‖ + ‖F[i,j,l] - F[i,j+1,l]‖
     fn compute_total_variation(&self, field: &ChromaticTensor) -> f32 {
         let mut tv = 0.0;
 
         for l in 0..field.layers() {
             for r in 0..field.rows() {
                 for c in 0..field.cols() {
                     let curr = field.get_rgb(r, c, l);
 
                     // Right neighbor
                     if r + 1 < field.rows() {
                         let right = field.get_rgb(r + 1, c, l);
                         tv += rgb_distance(&curr, &right);
                     }
 
                     // Down neighbor
                     if c + 1 < field.cols() {
                         let down = field.get_rgb(r, c + 1, l);
                         tv += rgb_distance(&curr, &down);
                     }
                 }
@@ -87,85 +96,85 @@ impl ChromaticNativeSolver {
         let mut penalty = 0.0;
         let total_cells = (field.rows() * field.cols() * field.layers()) as f32;
 
         for l in 0..field.layers() {
             for r in 0..field.rows() {
                 for c in 0..field.cols() {
                     let rgb = field.get_rgb(r, c, l);
                     let saturation = rgb_saturation(&rgb);
                     let diff = saturation - self.target_saturation;
                     penalty += diff * diff;
                 }
             }
         }
 
         penalty / total_cells
     }
 
     /// Compute color harmony score (0-1, higher is better)
     ///
     /// Measures:
     /// - Complementary balance (red-cyan, green-magenta, yellow-blue)
     /// - Hue consistency (lower std dev = more coherent)
     fn compute_color_harmony(&self, field: &ChromaticTensor) -> f32 {
         // 1. Complementary balance
         let mean_rgb = field.mean_rgb();
-        let complementary_balance = 1.0 - (
-            (mean_rgb[0] - 0.5).abs() +
-            (mean_rgb[1] - 0.5).abs() +
-            (mean_rgb[2] - 0.5).abs()
-        ) / 1.5; // Normalize to [0,1]
+        let complementary_balance = 1.0
+            - ((mean_rgb[0] - 0.5).abs() + (mean_rgb[1] - 0.5).abs() + (mean_rgb[2] - 0.5).abs())
+                / 1.5; // Normalize to [0,1]
 
         // 2. Hue consistency
         let hue_std = self.compute_hue_std_dev(field);
         let hue_consistency = 1.0 - (hue_std / 180.0).min(1.0);
 
         // Combine metrics
         0.6 * complementary_balance + 0.4 * hue_consistency
     }
 
     /// Compute standard deviation of hue angles
     fn compute_hue_std_dev(&self, field: &ChromaticTensor) -> f32 {
         let mut hues = Vec::new();
 
         for l in 0..field.layers() {
             for r in 0..field.rows() {
                 for c in 0..field.cols() {
                     let rgb = field.get_rgb(r, c, l);
                     let (_, _, hue) = rgb_to_hsv(&rgb);
                     hues.push(hue);
                 }
             }
         }
 
         let mean_hue: f32 = hues.iter().sum::<f32>() / hues.len() as f32;
-        let variance: f32 = hues.iter()
+        let variance: f32 = hues
+            .iter()
             .map(|&h| {
                 let diff = angle_difference(h, mean_hue);
                 diff * diff
             })
-            .sum::<f32>() / hues.len() as f32;
+            .sum::<f32>()
+            / hues.len() as f32;
 
         variance.sqrt()
     }
 
     /// Compute constraint violation score (0-1, lower is better)
     ///
     /// Measures:
     /// - Out-of-gamut pixels (RGB outside [0,1])
     /// - Extreme saturation (> 0.95)
     /// - Local discontinuities (sharp color jumps)
     fn compute_constraint_violation(&self, field: &ChromaticTensor) -> f32 {
         let total_cells = (field.rows() * field.cols() * field.layers()) as f32;
         let mut violation_count = 0.0;
 
         for l in 0..field.layers() {
             for r in 0..field.rows() {
                 for c in 0..field.cols() {
                     let rgb = field.get_rgb(r, c, l);
 
                     // 1. Out-of-gamut check
                     if rgb.iter().any(|&v| v < 0.0 || v > 1.0) {
                         violation_count += 1.0;
                     }
 
                     // 2. Extreme saturation check
diff --git a/src/spectral/fft.rs b/src/spectral/fft.rs
index b40bbbecda3ecb7e7e2848cd55fb5000a15b5125..4650b19258c262f5f417e5b9981fec404ffe2539 100644
--- a/src/spectral/fft.rs
+++ b/src/spectral/fft.rs
@@ -1,78 +1,79 @@
 //! FFT-based feature extraction for chromatic tensors.
 //!
 //! This module implements frequency-domain analysis using Fast Fourier Transform (FFT)
 //! to extract spectral features from chromatic fields. These features capture patterns
 //! that may not be apparent in the spatial domain.
 //!
 //! **Use Case (Phase 3B):** The Learner can use spectral entropy and frequency features
 //! to bias the Dreamer toward dreams with specific frequency characteristics that
 //! historically helped training convergence.
 
 use crate::tensor::ChromaticTensor;
 use ndarray::Array2;
 use rustfft::{num_complex::Complex, FftPlanner};
+use serde::Serialize;
 use std::f32::consts::PI;
 
 /// Window function for FFT preprocessing.
 ///
 /// Windowing reduces spectral leakage by smoothly tapering the signal at edges.
 #[derive(Debug, Clone, Copy, PartialEq)]
 pub enum WindowFunction {
     /// No windowing (rectangular)
     None,
     /// Hann window: 0.5 - 0.5*cos(2π*n/N)
     Hann,
     /// Hamming window: 0.54 - 0.46*cos(2π*n/N)
     Hamming,
 }
 
 impl WindowFunction {
     /// Generate window coefficients for a given length.
     ///
     /// # Arguments
     /// * `n` - Window length
     ///
     /// # Returns
     /// * Array of window coefficients [0, 1]
     pub fn generate(&self, n: usize) -> Vec<f32> {
         match self {
             WindowFunction::None => vec![1.0; n],
             WindowFunction::Hann => (0..n)
                 .map(|i| 0.5 - 0.5 * (2.0 * PI * i as f32 / n as f32).cos())
                 .collect(),
             WindowFunction::Hamming => (0..n)
                 .map(|i| 0.54 - 0.46 * (2.0 * PI * i as f32 / n as f32).cos())
                 .collect(),
         }
     }
 }
 
 /// Spectral features extracted from a chromatic tensor.
 ///
 /// Contains frequency-domain statistics computed via FFT.
-#[derive(Debug, Clone)]
+#[derive(Debug, Clone, Serialize)]
 pub struct SpectralFeatures {
     /// Spectral entropy (0-1, higher = more complex frequency distribution)
     ///
     /// Measures the "disorder" in the frequency spectrum. High entropy indicates
     /// a broad, flat spectrum (complex patterns), while low entropy indicates
     /// a peaky spectrum (simple, periodic patterns).
     pub entropy: f32,
 
     /// Dominant frequency bin for each RGB channel
     ///
     /// Identifies the strongest frequency component in each color channel.
     /// Format: [red_freq_bin, green_freq_bin, blue_freq_bin]
     pub dominant_frequencies: [usize; 3],
 
     /// Energy in low frequency band (0-25% of Nyquist)
     ///
     /// Captures large-scale, smooth color variations.
     pub low_freq_energy: f32,
 
     /// Energy in mid frequency band (25-75% of Nyquist)
     ///
     /// Captures medium-scale patterns and textures.
     pub mid_freq_energy: f32,
 
     /// Energy in high frequency band (75-100% of Nyquist)
@@ -326,51 +327,51 @@ fn compute_band_energies(psd: &[f32]) -> (f32, f32, f32) {
     let high_energy: f32 = psd[high_cutoff..].iter().sum();
 
     (low_energy, mid_energy, high_energy)
 }
 
 #[cfg(test)]
 mod tests {
     use super::*;
 
     #[test]
     fn test_window_function_none() {
         let window = WindowFunction::None.generate(5);
         assert_eq!(window, vec![1.0, 1.0, 1.0, 1.0, 1.0]);
     }
 
     #[test]
     fn test_window_function_hann() {
         let window = WindowFunction::Hann.generate(5);
         assert_eq!(window.len(), 5);
         // Hann window: 0.5 - 0.5*cos(2π*n/N)
         // At n=0: 0.5 - 0.5*cos(0) = 0.5 - 0.5 = 0.0
         // At n=4: 0.5 - 0.5*cos(8π/5) ≈ 0.095 (not exactly 0 for N=5)
         // The window is symmetric and peaks in the middle
         assert!(window[0] < 0.01); // Near zero at start
         assert!(window[2] > 0.9); // Peak near 1 in middle
-        // Don't check endpoint for odd-length windows
+                                  // Don't check endpoint for odd-length windows
     }
 
     #[test]
     fn test_spectral_entropy_uniform() {
         // Uniform distribution should have high entropy
         let psd = vec![1.0, 1.0, 1.0, 1.0];
         let entropy = compute_spectral_entropy(&psd);
         assert!((entropy - 1.0).abs() < 0.01); // Should be ~1.0
     }
 
     #[test]
     fn test_spectral_entropy_peaked() {
         // Single peak should have low entropy
         let psd = vec![0.0, 0.0, 1.0, 0.0, 0.0];
         let entropy = compute_spectral_entropy(&psd);
         assert!(entropy < 0.1); // Should be ~0.0
     }
 
     #[test]
     fn test_spectral_entropy_empty() {
         let psd = vec![];
         let entropy = compute_spectral_entropy(&psd);
         assert_eq!(entropy, 0.0);
     }
 
diff --git a/src/tensor/chromatic_tensor.rs b/src/tensor/chromatic_tensor.rs
index 8d99cb28d863cb1869103ec5c4a6700c5e23d940..7cf4d835ec3ce3b9ff64d7241745160f350d933b 100644
--- a/src/tensor/chromatic_tensor.rs
+++ b/src/tensor/chromatic_tensor.rs
@@ -1,52 +1,52 @@
 use std::fmt::{self, Display};
 use std::ops::{Add, Sub};
 
 use ndarray::{Array3, Array4, Axis};
 use rayon::prelude::*;
-use serde::Serialize;
+use serde::{Deserialize, Serialize};
 
 /// A 4-dimensional chromatic tensor representing an RGB color field with certainty weights.
 ///
 /// The tensor structure is `[rows, cols, layers, 3]` where each cell contains:
 /// - RGB color values (3 channels) in range [0.0, 1.0]
 /// - A certainty/confidence weight ρ in range [0.0, 1.0]
 ///
 /// # Examples
 ///
 /// ```
 /// use chromatic_cognition_core::ChromaticTensor;
 ///
 /// // Create a deterministic random tensor
 /// let tensor = ChromaticTensor::from_seed(42, 64, 64, 8);
 ///
 /// // Get statistics
 /// let stats = tensor.statistics();
 /// println!("Mean RGB: {:?}", stats.mean_rgb);
 /// println!("Variance: {}", stats.variance);
 /// ```
-#[derive(Clone, Debug, Serialize)]
+#[derive(Clone, Debug, Serialize, Deserialize)]
 pub struct ChromaticTensor {
     /// RGB color values as a 4D array: [rows, cols, layers, 3]
     pub colors: Array4<f32>,
     /// Certainty weights as a 3D array: [rows, cols, layers]
     pub certainty: Array3<f32>,
 }
 
 impl ChromaticTensor {
     /// Creates a new chromatic tensor initialized with zeros.
     ///
     /// # Arguments
     ///
     /// * `rows` - Number of rows in the tensor
     /// * `cols` - Number of columns in the tensor
     /// * `layers` - Number of depth layers in the tensor
     ///
     /// # Examples
     ///
     /// ```
     /// use chromatic_cognition_core::ChromaticTensor;
     ///
     /// let tensor = ChromaticTensor::new(64, 64, 8);
     /// assert_eq!(tensor.shape(), (64, 64, 8, 3));
     /// ```
     pub fn new(rows: usize, cols: usize, layers: usize) -> Self {
diff --git a/src/tensor/gradient.rs b/src/tensor/gradient.rs
index 744750746fd3e7f6b520e04d09965214edc76738..5686d22e1ea50efd0e34836cba49d45168cf93f2 100644
--- a/src/tensor/gradient.rs
+++ b/src/tensor/gradient.rs
@@ -1,29 +1,29 @@
 use std::io;
 use std::path::Path;
 
-use ndarray::{Array3, s};
+use ndarray::{s, Array3};
 use plotters::prelude::*;
 use rayon::prelude::*;
 
 use super::ChromaticTensor;
 
 #[derive(Debug, Clone)]
 pub struct GradientLayer {
     pub image: Array3<f32>,
 }
 
 impl GradientLayer {
     pub fn from_tensor(tensor: &ChromaticTensor) -> Self {
         let (rows, cols, layers, _) = tensor.colors.dim();
         let mut image = Array3::zeros((rows, cols, 3));
 
         image
             .indexed_iter_mut()
             .par_bridge()
             .for_each(|((row, col, channel), value)| {
                 let mut numerator = 0.0f32;
                 let mut denominator = 0.0f32;
                 for layer in 0..layers {
                     let weight = tensor.certainty[[row, col, layer]].max(0.0);
                     let color = tensor.colors[[row, col, layer, channel]];
                     numerator += color * weight;
diff --git a/tests/operations.rs b/tests/operations.rs
index 043f6838fd0711cd3289aeb06abe61df4fa540f2..c95a2014dcabbd52b31444454c52f07a4408f964 100644
--- a/tests/operations.rs
+++ b/tests/operations.rs
@@ -1,27 +1,27 @@
 use chromatic_cognition_core::{
-    ChromaticTensor, GradientLayer, complement, filter, mix, mse_loss, saturate,
+    complement, filter, mix, mse_loss, saturate, ChromaticTensor, GradientLayer,
 };
 use ndarray::{Array3, Array4};
 
 fn sample_tensor(values: [[f32; 3]; 2]) -> ChromaticTensor {
     let colors = Array4::from_shape_vec((1, 1, 2, 3), values.into_iter().flatten().collect())
         .expect("shape matches");
     let certainty = Array3::from_elem((1, 1, 2), 1.0);
     ChromaticTensor::from_arrays(colors, certainty)
 }
 
 #[test]
 fn mix_adds_and_normalizes() {
     let a = sample_tensor([[0.2, 0.4, 0.6], [0.1, 0.2, 0.3]]);
     let b = sample_tensor([[0.3, 0.3, 0.3], [0.2, 0.5, 0.7]]);
     let mixed = mix(&a, &b);
     let data = mixed.colors;
     assert!((data[[0, 0, 0, 0]] - 0.5).abs() < 1e-6);
     assert!((data[[0, 0, 0, 1]] - 0.7).abs() < 1e-6);
     assert!((data[[0, 0, 1, 2]] - 1.0).abs() < 1e-6);
 }
 
 #[test]
 fn filter_subtracts_and_clamps() {
     let a = sample_tensor([[0.4, 0.5, 0.6], [0.8, 0.1, 0.3]]);
     let b = sample_tensor([[0.2, 0.7, 0.1], [0.4, 0.5, 0.9]]);
@@ -33,42 +33,47 @@ fn filter_subtracts_and_clamps() {
 }
 
 #[test]
 fn complement_inverts_green_and_blue() {
     let tensor = sample_tensor([[0.1, 0.2, 0.3], [0.5, 0.6, 0.7]]);
     let complemented = complement(&tensor);
     let data = complemented.colors;
     assert!((data[[0, 0, 0, 0]] - 0.1).abs() < 1e-6);
     assert!((data[[0, 0, 0, 1]] - 0.8).abs() < 1e-6);
     assert!((data[[0, 0, 1, 2]] - 0.3).abs() < 1e-6);
 }
 
 #[test]
 fn saturate_stretches_chroma() {
     let tensor = sample_tensor([[0.3, 0.4, 0.5], [0.2, 0.4, 0.6]]);
     let saturated = saturate(&tensor, 1.5);
     let data = saturated.colors;
     assert!(data[[0, 0, 0, 0]] < tensor.colors[[0, 0, 0, 0]]);
     assert!(data[[0, 0, 0, 2]] > tensor.colors[[0, 0, 0, 2]]);
 }
 
 #[test]
 fn gradient_layer_averages_across_layers() {
     let tensor = sample_tensor([[0.0, 0.5, 1.0], [1.0, 0.5, 0.0]]);
     let gradient = GradientLayer::from_tensor(&tensor);
-    println!("R: {}, G: {}, B: {}", gradient.image[[0, 0, 0]], gradient.image[[0, 0, 1]], gradient.image[[0, 0, 2]]);
+    println!(
+        "R: {}, G: {}, B: {}",
+        gradient.image[[0, 0, 0]],
+        gradient.image[[0, 0, 1]],
+        gradient.image[[0, 0, 2]]
+    );
     println!("Expected: R: 0.5, G: 0.5, B: 0.5");
     // The test expects to average [0.0, 0.5, 1.0] and [1.0, 0.5, 0.0] across 2 layers
     // Result should be [(0.0+1.0)/2, (0.5+0.5)/2, (1.0+0.0)/2] = [0.5, 0.5, 0.5]
     assert!((gradient.image[[0, 0, 0]] - 0.5).abs() < 1e-6);
     assert!((gradient.image[[0, 0, 1]] - 0.5).abs() < 1e-6);
     assert!((gradient.image[[0, 0, 2]] - 0.5).abs() < 1e-6);
 }
 
 #[test]
 fn mse_loss_computes_mean_squared_error() {
     let a = sample_tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]);
     let b = sample_tensor([[0.1, 0.1, 0.1], [0.2, 0.2, 0.2]]);
     let metrics = mse_loss(&a, &b);
     assert!(metrics.loss > 0.0);
     assert!(metrics.mean_rgb[0] > 0.0);
 }
 
EOF
)